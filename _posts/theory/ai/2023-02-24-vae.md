---
title: "Variational Autoencoders"
categories: [Theory, AI]
tags: [Autoencoder, Variational Autoencoder]
img_path: /assets/img/posts/theory/ai/vae/
author: gshan
math: true
---

# Ⅰ. Generative Model

## ⅰ. Latnet Variable Model

Autoencoder의 목적은 고차원의 data를 저차원의 latent vector로 표현하는 것이다.
즉, encoder를 얻기 위해 decoder를 활용했다.
Variational autoencoder(VAE)는 generative model로 encoder와 다르게 decoder를 얻기 위해 encoder를 활용한다.

![](1.jpg){: w="500"}
_Figure 1: Latent variable generative model._

- $z\sim p(z)$: random variable
- $g_\theta(\cdot)$: deterministic function parameterized by $\theta$
- $x = g_\theta(z)$: random variable

Latent variable $z$는 target data $x$를 control하는 parameter로 볼 수 있다.
예를 들면, 인물 사진을 생성하는 model에서 $z_i$를 바꾸면 성별이 달라질 수 있을 것이다.

Loss function을 MLE 관점으로 보면, $z$는 주어진 prior distribution에서 sampling된 값으로, $g_\theta$는 주어진 확률 분포 $p$의 parameter(평균, 표준편차, 등)로 해석할 수 있다.
그러면 $p(x\|g_\theta(z)) = p_\theta(x\|z)$는 $x$의 likelihood가 된다.

직관적으로 표현하자면, 인물의 특징들인 $z$를 반영한 고차원 data가 모여있는 곳으로 확률 분포 $p$를 이동시키는 parameter $g_\theta$를 찾는 것이다.

따라서 training set에 있는 모든 $x$의 확률을 최대화 하는 parameter를 출력하는 $\theta$를 찾는 게 목표가 된다.

> Image마다 특징이 다르기 때문에 prior distribution에서 sampling된 $z$도 image마다 다르다.
> 이를 얻기 위해 encoder가 필요하다.

$$
\underset{\theta}{\text{argmax}}\int p(x|g_\theta(z))p(z)dz 
 = \underset{\theta}{\text{argmax}}\int p_\theta(x|z)p(z)dz 
 = \underset{\theta}{\text{argmax}}\int p_\theta(x, z)dz 
 = \underset{\theta}{\text{argmax }} \ p_\theta(x)
$$

## ⅱ. Prior distribution

$z$로 control를 하기 위해 $z$를 원하는 대로 다룰 수 있어야 한다.
즉, prior distribution은 다루기 쉬운 분포(Gaussian, Uniform 등)를 사용한다.

$z$는 prior distribution에서 sampling된 값이라고 했다.
그러면 당연히 prior distribution은 $z$의 manifold를 잘 반영한 distribution이어야 할 것이다.
그렇다면, 간단한 prior 분포에서 복잡한 manifold에 있는 $z$를 잘 sampling할 수 있을까?

![](2.jpg)
_Figure 2: Given a random variable z with one distribution, we can create another random variable $X = g(z)$ with a completely different distribution. Left: samples from a gaussian distribution. Right: those same samples mapped through the function $g(z) = z/10 + z/\|\|z\|\|$ to form a ring. This is the strategy that VAEs use to create arbitrary distributions: the deterministic function g is learned from data._

[Tutorial on Variational Autoencoders][3]에 따르면, generator $g_\theta$가 여러 개의 layer를 사용할 경우, 처음 몇 개의 layers를 통해 복잡할 수 있지만 딱 맞는 latnet space로의 mapping이 수행되고 나머지 layers를 통해 latent vector에 맞는 image를 생성할 수 있다고 한다.

Figure 2에서 왼쪽은 Gaussian 분포를 나타낸다.
그리고 오른쪽은 Gaussian에서 sampling된 $z$ 값을 간단히 변형시켜 원의 테두리를 나타내는 분포로 바꾼 것이다.
이처럼 $g_\theta$에서 앞단의 한 두개 layer가 간단한 분포를 manifold를 잘 표현하는 복잡한 분포로 바꾸도록 학습될 것이다.

## ⅲ. MLE directly?

간단한 분포도 generator에 의해 latent space를 잘 표현하는 복잡한 분포로 mapping된다고 했다.
그러면 encoder 필요 없이 normalize prior distribution $z \sim \mathcal{N}(0, 1)$과 MSE loss를 사용해 학습이 가능하지 않을까?
미리 말하자면 불가능하다.

![](3.jpg)
_Figure 3: It’s hard to measure the likelihood of images under a model using only sampling. Given an image $X$ (a), the middle sample (b) is much closer in Euclidean distance than the one on the right (c). Because pixel distance is so different from perceptual distance, a sample needs to be extremely close in pixel distance to a datapoint $X$ before it can be considered evidence that $X$ is likely under the model._

Fig. 3에서 (b)는 (a)에서 pixel 몇 개를 지운 것이고 (c)는 (a)를 오른쪽으로 1 pixel 이동시킨 것이다.
MSE 관점에서 (b)의 값이 더 적어 generator $g_\theta$는 (c)보다 (b)를 생성하도록 훈련될 것이다.
따라서 간단한 분포를 사용하더라도 생성하고자 하는 image인 숫자 '2'의 특징을 반영한 prior distribution에서 $z$를 sampling해야 한다. 이때 variational inference를 사용한다.

# Ⅱ. Variational inference

![](4.jpg)
_Figure 4: Variational inference._

Target data $x$와 의미적으로 같은 것을 생성하는 latent variable $z$의 prior distribution은 알 수 없다.
그렇기 때문에 prior distribution을 추정해야 하는데 이때 variational inference를 사용한다.

Target data $x$와 의미적으로 같은 것을 생성해내는 $z$의 true posterior $p(z\|x)$가 있다고 했을 때, variational inference는 $x$를 evidence로 주고 approximation class(Gaussian, Uniform 등)에서 true posterior와 가장 유사한 분포 $q_\phi(z\|x)$을 찾는 과정이다.

## ⅰ. ELBO: Evidence LowerBOund

목적은 $p(x)$를 최대화 하는 것이다.
이를 위해 임의의 prior distribution $p(z)$가 아니라 $x$를 잘 생성해낼 수 있도록 하는 posterior distribution $p(z\|x)$가 필요하다.
그렇지만 이를 알 수 없기 때문에 variational inference를 통해 추정 분포 $q_\phi(z\|x)$를 구한다.

이 네 개의 분포 $p(x), p(z), p(z\|x), q_\phi(z\|x)$간의 관계는 ELBO로 표현된다.

![](5.jpg)
_Figure 5: ELBO with Jensen's inequality._

Jensen's inequality를 활용해 ELBO를 구하는 방법으로 직관적이지 않다.

![](6.jpg)
_Figure 6: ELBO with marginalization._

위의 식에서 ELBO($\phi$)는 Fig. 5의 ELBO 식과 같다.
그리고 $KL\big(q_\phi(z\|x) \|\| p(z\|x)\big)$를 살펴보면 추정 분포인 $q_\phi(z\|x)$와 이상적인 sampling 분포인 $p(z\|x)$ 사이의 유사도를 나타낸 것이다.
해당 KL term이 작을수록 $q$가 $p$와 유사하다는 것을 의미한다.
KL은 항상 0보다 크거나 같으므로 KL trem을 왼쪽으로 넘기면 Fig. 5와 같은 의미를 가지게 된다.

$$
\begin{split}
\text{ELBO} &= \text{log }p(x) - KL\\
\text{ELBO} &\le \text{log }p(x)
\end{split}
$$

![](7.jpg)
_Figure 7: Maximize ELBO._

따라서 $g_\theta$가 고정되어 있다고 생각하면 (즉, $\text{log }p(x)$의 값이 고정되어 있다고 생각하면), ELBO를 최대화해 KL term을 최소화함으로써 $q_\phi$가 $p$와 유사해지도록 한다.

ELBO를 풀어쓰면 KL trem이 또 등장하는데, 앞서 살펴봤던 KL term과 인자가 다르다.

# Ⅲ. Loss function

![](8.jpg)
_Figure 8: Overall Optimization._

따라서 VAE는 지금까지 살펴봤던 ELBO를 최대화해 sampling 분포 $q_\phi$를 학습하고, $z$를 sampling해서 $x$를 generate해 MLE를 최대화해 generator를 학습해야 한다.
1. $\phi$: Maximize ELBO
2. $\theta$: Maximize MLE

그런데 Fig. 7의 ELBO 식을 자세히 보면 MLE에 관한 term이 포함되어 있다.

$$
\mathbb{E}_{q_\phi(z|x)}[\text{log }p(x|z)]
$$

따라서 $\phi$에 대해 ELBO를 최대화하는 것이 variational inference가 되고, $\theta$에 대해 ELBO를 최대화하는 것은 MLE가 된다.

## ⅰ. Explanation

![](9.jpg)
_Figure 9: Detail about ELBO._

ELBO를 조금 더 살펴보자.
Reconstruction error는 $x$를 넣었을 때 $x$가 잘 나오는지에 대한 term이다.
$p$가 Gaussian이라고 하면 loss는 MSE가 되고, Bernoulli라고 하면 cross entropy가 된다.
그리고 regularization은 같은 reconstruction error를 갖는 $q_\phi$가 여러개가 있다면, 이왕이면 다루기 쉬운 sampling인 prior $p(z)$와 유사한 모양이 되라는 의미를 담고 있다.

<span class="text-color-bold"> **Regularization** </span>

![](10.jpg)
_Figure 10: Regularization_

$q_\phi(z\|x_i) \sim \mathcal{N}(\mu_i, \sigma_i^2I)$, $p(z) \sim \mathcal{N}(0, I)$라고 가정하자.

> 즉 $x$를 잘 생성하는 $q_\phi$가 multivariate normal distribution과 유사하도록 만든다.

두 Gaussian의 KL term을 풀어쓰면 Fig. 11의 수식과 같다.
이는 Fig. 11의 제일 아래에서 볼 수 있듯이 수학적으로 이미 증명되어 있다.

<span class="text-color-bold"> **Reconstruction Error** </span>

![](11.jpg)
_Figure 11: Reconstruction error._

기댓값을 구하기 위해 적분 대신 Monte-carlo 방법을 사용한다.
$q_\phi(z\|x_i)$으로 출력된 평균과 표준편차를 가지는 Gaussian에서 L개를 sampling한 다음 평균을 구한다.

![](12.jpg)
_Figure 12: Reparameterization trick._

여기서 문제는 sampling이다.
Sampling은 random node이기 때문에 역전파를 계산할 수 없다.
그래서 VAE에서는 reparameterization trick을 사용해 역전파를 한다.

실제 코드에서는 L개도 sampling하지 않고 1개만 sampling한다.
그리고 $p$를 Gaussian으로 가정하면 loss function은 MSE가 되고 Bernouill로 가정하면 corss entropy가 된다.

# Ⅳ. Example

![](13.jpg)
_Figure 13: Example - MNIST._

MNIST를 예시는 위와 같다.
MNIST는 흑백 사진으로 pixel은 0또는 1의 값을 가지므로 generator는 Bernoulli라고 가정한다.
즉, loss function으로 cross entropy를 사용한다.

![](14.jpg)
_Figure 14: Mnifold of AE, VAE._

Autoencoder와 VAE의 수식적인 차이점은 loss function의 KL term이 전부라고 할 수 있다.
이 KL term의 효과를 Fig. 14와 같이 latent vector의 range를 제한한다고 할 수 있다.

Range를 제한하면, generator를 학습한 뒤 normal distribution에서 sampling해서 image를 생성할 수 있다.
그렇지 않으면, latent space가 어떤 distribution인지 알지 못하기 때문에 generate할 수 없다.

# Ⅴ. CVAE

![](15.jpg)
_Figure 15: VAE and CVAE_

Conditional VAE (CVAE)는 label 정보를 encoder와 decoder에 활용한 VAE이다.
그래서 $z$는 label 정보를 제외한 다른 특징들(크기, 기울기, 등)이 학습된다.
ELBO 식도 큰 변화가 없고, label 정보를 활용하므로 빨리 수렴이 된다.

![](16.jpg){: w="500"}
_Figure 16: Fix label $y$ or latent vector $z$_

Label을 고정하면 스타일이 같은 숫자들이 생성되고, latent vector $z$를 고정하고 label을 다르게 주면 스타일이 같은 다른 숫자들이 생성된다.

# Ⅵ. AAE

![](17.jpg)
_Figure 17: AAE structure and loss function._

Regularization term $KL\big(q_\phi(z\|x) \|\| p(z\|x)\big)$은 Gaussian distribution이 아니면 계산하기 어렵다.
이러한 제한점을 없애고자 한 것이 Adversarial Autoencoder(AAE)이다.

KL term의 목적인 두 분포를 유사하도록 만드는 것이다.
같은 목적을 가지는 것이 바로 GAN이다.
따라서 AAE는 KL term대신 GAN을 사용해 임의의 분포 generator $G(z)$가 prior distribution $p(z)$와 유사해지도록 만든다.

Fig. 17과 같이 discriminator는 prior distribution에서 sampling된 것은 true, AAE의 generator에서 sampling된 것은 false로 생각한다.

![](18.jpg)
_Figure 18: AAE training procedure._

훈련은 Fig. 18과 같이 총 3번의 update가 한 batch에서 일어난다.

![](19.jpg)
_Figure 19: AAE with label._

CAE와 같이 label 정보를 AAE의 discriminator에 활용할 수 있다.

AAE의 장점은 manifold를 원하는 분포로 나타낼 수 있다는 것이다.


# References
1. [YouTube - 오토인코더의 모든 것 - 2/3][1]{:target="_blank"}
2. [Slideshare - 오토인코더의 모든 것][2]{:target="_blank"}

[1]: https://www.youtube.com/watch?v=rNh2CrTFpm4
[2]: https://www.slideshare.net/NaverEngineering/ss-96581209
[3]: https://arxiv.org/pdf/1606.05908.pdf