---
title: "Entropy"
categories: [Theory, Math]
tags: [Information Theory, Entropy]
img_path: /assets/img/posts/theory/math/entropy/
author: gshan
math: true
---

먼저 [information theory의 기본적인 내용][3]{:target="_blank"}을 먼저 살펴보는 것을 추천합니다.

# Ⅰ. Entropy

확률 분포(probability distribution)의 entropy는 주어진 분포의 random variable $X$와 관련된 불확실성(uncertainty)으로 해석할 수 있다.

이게 무슨 말인지 처음 접하면 와닿지 않을 것이다.
한번 entropy를 data source의 information content로 해석해보자.
예를 들어, 분포 $p$에서 순차적으로 n개의 sample을 뽑았다고 하자.
만약 $p$가 높은 entropy를 가졌다면 각 observation $x_i$에 대한 불확실성이 크므로 예측하기 힘들 것이다.
따라서 dataset $\mathcal{D} = (x_1, \cdots, x_n)$은 많은 information content를 가지게 되고, 해당 dataset을 참고해 $p$가 update될 수도 있다.
반면, $p$가 entropy 0을 가진다고 해보자.
그러면 불확실성이 없기 때문에, 모든 $x_i$는 무조건 같은 값을 가진다는 확신이 생기고, dataset $\mathcal{D}$는 아무런 information content를 가지지 않게 된다.
이처럼 entropy가 낮을수록 어떤 사건이 발생할지 쉽게 예측할 수 있기 때문에 불확실성은 낮아진다고 볼 수 있다.

$K$개의 상태(state)를 가지는 discrete random variable $X$에 대한 entropy는 다음과 같이 정의된다.

$$
\begin{split}
\mathbb{H}(X) &\triangleq - \sum_{k=1}^Kp(X=k)\text{ log}_2p(X=k)\\
&= - \mathbb{E}_{X}[\text{log }p(x)]
\end{split}
$$

- 보통 log base 2를 사용하고 단위(unit)는 bits이다. Log base가 e이면 단위를 nats라고 한다.

가장 큰 entropy를 가지는 discrete distribution은 uniform distribution이다.
반대로 가장 작은 entropy를 가지는 분포는 모든 mass를 하나의 상태(state)로 두는 delta-function이다.
Uniform distribution일 때 entropy는 아래와 같다.

$$
\mathbb{H}(X) = - \sum_{k=1}^K\frac{1}{K}\text{ log }(\frac{1}{K}) = - \text{ log }(\frac{1}{K}) = \text{ log }(K)
$$

특정 분포: Bernoulli의 entropy를 한번 살펴보자.
Bernoulli distribution의 entropy는 binary entropy function이라 한다.

$$
\begin{split}
\mathbb{H}(X) &= - [p(X=1)\text{ log}_2p(X=1) + p(X=0)\text{ log}_2p(X=0)]\\
&= - [\theta\text{ log}_2p(\theta) + (1-\theta)\text{ log}_2p(1-\theta)]
\end{split}
$$

$\theta$에 따른 entropy는 아래 그림과 같다.

![](1.jpg)
_Entropy of a Bernoulli random variable as a function of $\theta$. The maximum entropy is $\text{log}_22 = 1$._

지금까지 entropy가 어떤 느낌인지 감이 올 것이다.
Uniform distribution이라고 알려진 random variable $X$에선 모든 사건이 불확실하므로 entropy가 클 것이고, 관찰된 dataset은 많은 정보량을 담고 있어 $X$의 분포를 update할 때 유용할 것이다.

그렇다면 communication system에서 entropy는 어떤 의미를 가질까?
Communication system에서 entropy는 dataset $\mathcal{D}$를 표현하기 위해 필요한 최소 평균 bits를 의미한다.

예를 들어, 8마리 말이 경주를 한다고 하자.
1번 부터 순서대로 각 말이 이길 확률은 $(\frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \frac{1}{16}, \frac{1}{64}, \frac{1}{64}, \frac{1}{64}, \frac{1}{64})$이다.
이때 여러 번의 경주를 통해 각 경주마다 어느 말이 우승했는 지 전달하기 위해 필요한 bits는 얼마일까? 
일반적으로 8마리 말이 있으므로 한 경주마다 3 bits가 필요해 보인다.
하지만, 이는 최소 평균 bits가 아니다.
우리는 1번 말이 우승할 확률이 $\frac{1}{2}$이고 5~8번 말이 우승할 확률이 매우 낮은 것을 알고 있다.
그렇다면 bits의 길이 측면에서 000을 여러 번 사용하는 것은 낭비가 아니겠는가?
만약 각 말을 순서대로 0, 10, 110, 1110, 111100, 111101, 111110, 111111 같이 decode가 가능하도록 표현한다면 평균 bits는 2 bits가 된다.

$$
3 \cdot \frac{1}{2} + 3 \cdot \frac{1}{4} + 3 \cdot \frac{1}{8} + 3 \cdot \frac{1}{16} + 3 \cdot 4 \cdot \frac{1}{64} = 3
\Rightarrow 
1 \cdot \frac{1}{2} + 2 \cdot \frac{1}{4} + 3 \cdot \frac{1}{8} + 4 \cdot \frac{1}{16} + 6 \cdot 4 \cdot \frac{1}{64} = 2
$$

이를 entropy로 표현하면 다음과 같다.

$$
-\sum_{i=1}^8 p(i)\text{ log}_2p(i) = \bigg(-\frac{1}{2}\text{ log}_2\big(\frac{1}{2}\big)\bigg) + \cdots + \bigg(-\frac{4}{64}\text{ log}_2\big(\frac{1}{64}\big)\bigg) = 2
$$

만약 $p(i)$가 모두 동일하다면 entropy는 $-\text{log}_2(\frac{1}{8}) = 3$이 된다.

# Ⅱ. Cross Entropy

두 확률 분포 $p$와 $q$ 사이의 cross entropy는 아래와 같이 정의된다.

$$
\begin{split}
\mathbb{H}(p,q) &\triangleq - \sum_{k=1}^Kp(X = k) \text{ log }q(X=k)\\
&=\mathbb{E}_{p(X)}[\text{log }q(x)]
\end{split}
$$

Information theory에서 동일한 events set에 대한 두 확률 분포 $p$와 $q$ 사이의 cross entropy는 true distribution $p$를 통해 sampling된 events를 estimated probability distribution $q$에 최적화된 coding scheme를 사용해 식별(identify)하는데 필요한 평균 bits이다.

좀 더 직관적으로 보기 위해 위의 경주 예시를 살펴보자.
Uniform distribution을 추정된 확률 분포 $q$라 하고 $(\frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \frac{1}{16}, \frac{1}{64}, \frac{1}{64}, \frac{1}{64}, \frac{1}{64})$를 실제 확률 분포 $p$라 하자.
$q$의 coding scheme는 000, 001, 010, 011, 100, 101, 110, 111이 될 것이고, $p$와 $q$의 cross entropy 값은 3이 된다.
Cross entropy 값은 $q$가 $p$와 같을 때, 즉 coding scheme가 0, 10, 110, 1110, 111100, 111101, 111110, 111111일 때 최소가 된다.

Optimal code의 기댓값이 $\mathbb{H}(p,p) = \mathbb{H}(p)$인 $q = p$ setting에 의해 cross entropy가 최소값이 되는데, 이를 Shannon's source coding theorem이라고 한다.

일반적인 machine learning task에서 $p$은 ground truth의 분포, $q$는 model의 분포로 생각할 수 있다.
Cross entropy를 줄이는 방향으로 model을 학습시켜면, ground truth와 유사하도록 model의 분포가 update될 것이다.

# Ⅲ. Joint entropy

두 random variables $X, Y$의 joint entropy는 다음과 같이 정의된다.

$$
\mathbb{H}(X,Y) = - \sum_{x,y}p(x,y)\text{ log}_2p(x,y)
$$

1부터 8까지의 수, $n \in $ { $1, \dots, 8$ } 중에서 하나를 고르는 것을 생각해보자.
만약 $n$이 짝수이면 $X(n) = 1$이고, n이 소수(prime)이면 $Y(n)=1$이다.

![](2.jpg){: w="500"}

Joint distribution은 다음과 같다.

![](3.jpg){: w="500"}

따라서 joint entropy는 1.81 bits가 된다.

$$
\mathbb{H}(X,Y) = - \bigg[ \frac{1}{8}\text{log}_2\frac{1}{8} + \frac{3}{8}\text{log}_2\frac{3}{8} + \frac{3}{8}\text{log}_2\frac{3}{8} + \frac{1}{8}\text{log}_2\frac{1}{8} \bigg] = 1.81 \text{ bits }
$$

$X, Y$의 marginal propabilities는 uniform하다.
- $p(X=1) = p(X=0) = p(Y=0) = p(Y=1) = 0.5$

따라서 $X$와 $Y$의 entropy는 $\mathbb{H}(X) = \mathbb{H}(Y) = 1$이고 $\mathbb{H}(X, Y) = 1.81 < \mathbb{H}(X) + \mathbb{H}(Y) = 2$인 것을 알 수 있다.
Joint entropy의 해당 upper bound는 일반적으로 유지된다.

$$
\mathbb{H}(X, Y) \le \mathbb{H}(X) + \mathbb{H}(Y)
$$

만약 $X, Y$가 independent하면, $\mathbb{H}(X, Y) = \mathbb{H}(X) + \mathbb{H}(Y)$이다.
직관적으로 보자면, random variables이 어떤 방식으로 연관있을 때 system의 'degrees of freedom'가 감소하므로 전체 entropy가 감소되는 것이다.

그렇다면 $\mathbb{H}(X, Y)$의 lower bound는 무엇일까?
만약 $Y$가 $X$의 deterministic function이라면 $\mathbb{H}(X, Y) = \mathbb{H}(Y)$가 된다.
따라서 lower bound는 다음과 같다.

$$
\mathbb{H}(X, Y) \ge \text{max }\{\mathbb{H}(X), \mathbb{H}(Y)\} \ge 0
$$

이는 한 problem에 대해 단순히 미지수를 더 추가한다고 해서 uncertainty를 줄일 수 없다는 것을 의미한다.
Uncertainty를 줄이려면 data를 관찰해야 한다.

# Ⅳ. Conditional entropy

$X$가 주어졌을 때 $Y$의 conditional entropy는 $X$를 보고난 뒤 $Y$가 가지는 uncertainty를 모든 $X$에 대해 평균을 낸 것이다.

$$
\begin{split}
  \mathbb{H}(Y|X) &\triangleq  \mathbb{E}_X[\mathbb{H}(p(Y|X))]\\
  &= \sum_x p(x)\mathbb{H}(p(Y|X=x)) = -\sum_xp(x)\sum_yp(y|x)\text{ log }p(y|x)\\
  &= -\sum_{x,y}p(x,y)\text{ log }p(y|x) = -\sum_{x,y}p(x,y)\text{ log }\frac{p(x,y)}{p(x)}\\
  &= -\sum_{x,y}p(x,y)\text{ log }p(x,y) + \sum_xp(x)\text{ log }p(x)\\
  &= \mathbb{H}(X,Y) - \mathbb{H}(X)
\end{split}
$$

만약 $Y$가 $X$의 deterministic function이라면 $X$가 $Y$를 완전히 결정하므로 $\mathbb{H}(Y\|X) = 0$이 된다.
만약 $X, Y$가 independent하면 $X$는 $Y$에 대해 아무것도 말해주는 게 없기 때문에 $\mathbb{H}(Y\|X) = \mathbb{H}(Y)$이다.

$\mathbb{H}(X, Y) \le \mathbb{H}(X) + \mathbb{H}(Y)$이기 때문에 $\mathbb{H}(Y\|X) \le \mathbb{H}(Y)$이다.
이는 데이터에 대한 conditioning은 '보통' 불확실성을 증가시키지 않는다는 것을 보여준다.
여기서 '보통'이라고 말한 것은 특정한 $x$의 observation은 불확실성을 증가시킬 수 있다. (즉, $\mathbb{H}(Y\|x) > \mathbb{H}(Y)$)

$\mathbb{H}(Y\|X) = \mathbb{H}(X,Y) - \mathbb{H}(X)$는 다음과 같이 entropy에 대한 chain rule이라고 볼 수 있다.

$$
\begin{split}
  &\mathbb{H}(X_1, X_2) = \mathbb{H}(X_1) + \mathbb{H}(X_2|X_1)\\
  &\mathbb{H}(X_1, X_2, \dots, X_n) = \sum_{i=1}^n \mathbb{H}(X_i|X_1, \dots, X_{i-1})
\end{split}
$$

# Ⅴ. Differential entropy for continuous random variables

PDF $p(x)$를 가지는 continuous random variable $X$에 대해 differential entropy는 다음과 같이 정의된다.

$$
h(X) \triangleq - \int_\mathcal{X} p(x)\text{ log }p(x)dx
$$

Discrete한 경우와 다르게 differential entropy는 pdf의 값이 1보다 클 수 있으므로 음수가 될 수 있다.
Differential entropy를 이해하는 한 가지 방법은 실수를 quantization(실수를 n진수로 표현, 여기선 2진수.)하는 상황으로 생각하는 것이다.
실수의 quantization은 오직 유한한(finite) precision으로만 표현이 가능하고, random variable $X$의 $n$ bits quantization은 대략 $h(X) + n$ bits만 필요로 한다.

> 부동소수점 계산을 먼저 이해하고 계속 읽는 것을 추천합니다.

예를 들어, $X \sim U(0, 1/8)$이라고 하자.
$X \le 1/8$이므로, 부동소수점은 $0.000\text{xxx}_2$로 표현된다.
Binary point 이후 3 bits는 항상 0이라고 확신할 수 있고, $X$를 $n$ bits의 accuracy로 quantization할 때 오직 $n-3$만 필요하게 된다.
$h(X) = \text{log}_2(1/8) = -3$이기 때문에 $X$의 $n$ bits quantization의 entropy는 $n-3$이라고 할 수 있다.

# References

1. [Kevin P. Murphy. (2022). Probabilistic Machine Learning: An introduction, MIT Press][1]{:target="_blank"}
2. [Kevin P. Murphy. (2023). Probabilistic Machine Learning: Advanced Topics, MIT Press][2]{:target="_blank"}
3. [Wiki - Cross entropy][4]{:target="_blank"}

[1]: https://probml.github.io/pml-book/book1.html
[2]: https://probml.github.io/pml-book/book2.html
[3]: https://c0natus.github.io/posts/information-theory/
[4]: https://en.wikipedia.org/wiki/Cross_entropy
