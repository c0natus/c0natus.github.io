---
title: "Mutual Information and Information Bottleneck"
categories: [Theory, Math]
tags: [Information Theory, Mutual Information, Information Bottleneck]
img_path: /assets/img/posts/theory/math/mi_ib/
author: gshan
math: true
---

Mutual information과 information bottleneck이 무엇인지 간단하게 알아보자.

Mutual information (MI)이란 두 random variables간 상호 의존도(mutual dependence)를 측정하는 방법이다.
즉, 하나의 random variable를 관찰함으로써 다른 random variable에 대해 얻은 "정보의 양"을 정량화하는 것으로 entropy와 밀접하게 연관되어 있다.

Information bottleneck (IB)은 random variable X와 그것과 관련된 observed random variable Y의 joint probability distribution P(X, Y)가 주어진 상황에서 X를 요약(예: 클러스터링)할 때 정확도와 복잡성(압축의 정도) 간의 최상의 tradeoff를 찾기 위해 설계되었다.

# Ⅰ. Correlation

두 random variables간 관계를 파악하는 방법으로 제일 먼저 (Pearson) correlation coefficient($\rho$)를 떠올릴 수 있다.

$$
\rho \triangleq \text{corr}[X, Y] \triangleq \frac{\text{Cov}[X,Y]}{\sqrt{\mathbb{V}[X]\mathbb{V}[Y]}}
$$

$\text{Cov}[X,Y]$는 $X, Y$간 **linear**하게 연관된 정도를 나타내는 covariance이다.

$$
\text{Cov}[X,Y] \triangleq \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])] = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]
$$

Covariance는 $(-\infty, \infty)$ 사이의 값을 가지고, $\rho$는 covariance의 normalize된 measure로 $[-1, 1]$ 사이의 값을 가진다.

이러한 correlation는 linear relationship의 방향과 noisiness만 반영한다. 그렇기 때문에 <span class="text-color-yellow">몇가지 한계점이 존재</span>한다.

![](1.png)
_Figure 1: Several sets of (x, y) points, with the correlation coefficient of x and y for each set. Note that the correlation reflects the strength and direction of a linear relationship (top row), but not the slope of that relationship (middle), nor many aspects of nonlinear relationships (bottom). (Note: the figure in the center has a slope of 0 but in that case the correlation coefficient is undefined because the variance of Y is zero.)_

위의 그림에서 살펴볼 수 있듯이, relationship의 slope (middle)와 nonlinear aspects(bottom)를 반영할 수 없다. 또한 random variables가 real-value를 가지고 있을 때만 계산할 수 있다. 

이러한 correlation의 한계점을 보완할 수 있는 방법으로 MI를 알아보자.

# Ⅱ. Mutual Information (MI)

KL divergence는 두 distirubtions이 얼마나 유사한지를 측정하는 방법이다. 
그렇다면 두 random variables가 얼마나 dependant한지는 어떻게 측정할까?
이러한 질문을 분포의 유사성에 대한 질문으로 바꿔서 생각하게 되며 등장한 개념이 MI이다.

두 random variables $X, Y$에 대한 MI는 joint probability distribution p(X, Y)가 p(X)p(Y)와 얼마나 비슷한지를 측정하는 것으로, 아래와 같이 정의한다.

$$
\mathbb{I}(X;Y) \triangleq D_{\mathbb{KL}}(p(x,y)||p(x)p(y)) = \sum_{y \in Y}\sum_{x \in X}p(x,y)\text{log }\frac{p(x,y)}{p(x)p(y)}
$$

- Variables의 집합을 나타내기 위해 $\mathbb{I}(X,Y)$ 대신 $\mathbb{I}(X;Y)$를 사용했다. 예를 들어, $X$와 $(Y, Z)$간 MI는 $\mathbb{I}(X;Y,Z)$로 표현한다.
- KL divergence는 0이상의 값을 가지므로 $MI \geq 0$이다. $p(x,y)=p(x)p(y)$일 때, $MI=0$이다.

## ⅰ. Interpretation

MI는 joint ($p(x, y)$) 와 factored marginal distributions ($p(x), p(y)$) 사이의 KL divergence이다. 
즉, MI는 두 variables를 독립적인 $p(x)p(y)$로 취급하는 model에서 실제 joint density $p(x, y)$를 모델링하는 model로 업데이트된 경우의 information gain을 측정한다.

조금 더 직관적으로 MI를 joint and conditional entropies로 표현하여 해석할 수 있다.

$$
\begin{split}
  \mathbb{I}(X;Y) 
    &= \sum_x\sum_y p(x,y)\text{log }\frac{p(x,y)}{p(x)p(y)}\\
    &= \sum_x\sum_y p(x,y)(\text{log } p(x, y) - \text{log }p(x) - \text{log }p(y))\\
    &= \sum_x\sum_y p(x,y)\text{log } p(x, y) - \sum_x\sum_y p(x,y)\text{log }p(x) - \sum_x\sum_y p(x,y)\text{log }p(y)\\
    &= -\mathbb{H}(X,Y) + \mathbb{H}(X) + \mathbb{H}(Y)\\
    &= \mathbb{H}(Y) -\mathbb{H}(Y|X) = \mathbb{H}(X) -\mathbb{H}(X|Y)
\end{split}
$$

위의 식은 Y를 관찰한 후 X의 uncertainty 감소 또는 X를 관찰한 후 Y의 uncertainty 감소를 의미한다.
이는 X, Y가 얼마나 dependent한 지를 나타낸다고 볼 수 있다.
- MI는 0이상의 값을 가지기 때문에, conditioning이 평균적으로 entropy를 감소시킨다는 증거 또한 제공한다.

![](2.jpg){: w="500"}
_Figure 2: The marginal entropy, joint entropy, conditional entropy and mutual information represented as information diagrams. Used with kind permission of Katie Everett._

MI을 또 다른 관점으로 바라볼 수 있다.
Fig. 2는 이러한 관점들을 information diagram으로 요약한 것이다.

$$
\begin{split}
  \mathbb{I}(X;Y) 
    &= \mathbb{H}(X,Y) - \mathbb{H}(X|Y) - \mathbb{H}(Y|X)\\
    &= \mathbb{H}(X) + \mathbb{H}(Y) - \mathbb{H}(X, Y)
\end{split}
$$

# Ⅲ. Information Bottleneck (IB)

Input $\mathbf{x}$와 output $\mathbf{y}$ 사이의 *stochastic bottleneck*을 사용하는 model $p(\mathbf{y}\|\mathbf{x})$에 대한 information bottleneck을 알아보자.

$\mathbf{z}$가 $\mathbf{x}$의 (possibly stochastic) function이라면, $\mathbf{z}$를 $\mathbf{x}$의 representation이라고 한다.
그리고 $\mathbf{y} \perp \mathbf{x} \| \mathbf{z}$이면 task $\mathbf{y}$에 대해 $\mathbf{x}$의 representation $\mathbf{z}$는 **sufficient**하다고 한다.
쉽게 말해, $\mathbf{x}$로 $\mathbf{y}$를 예측하는 성능을 $\mathbf{z}$만으로 낼 수 있다면 $\mathbf{z}$는 sufficient하다.
- $\mathbf{y} \perp \mathbf{x} \| \mathbf{z}$는 $\mathbb{I}(\mathbf{z};\mathbf{y}) = \mathbb{I}(\mathbf{x};\mathbf{y})$ 즉, $\mathbb{H}(\mathbf{y}\|\mathbf{z}) = \mathbb{H}(\mathbf{y}\|\mathbf{x})$와 같은 의미를 가진다.

Sufficient한 $\mathbf{z}$ 중 가장 작은 $\mathbb{I}(\mathbf{z};\mathbf{x})$의 값을 가지는 $\mathbf{z}$를 **minimal sufficient statistic**이라고 한다.
쉽게 말해 $\mathbf{x}$의 정보를 가장 적게 활용하는 sufficient한 $\mathbf{z}$를 의미한다.

이때, information bottleneck은 $\mathbb{I}(\mathbf{z};\mathbf{y})$를 최대화 하면서, $\mathbb{I}(\mathbf{z};\mathbf{x})$를 최소화 하는 representation $\mathbf{z}$를 찾는 것이다.

$$
\text{min } \beta \mathbb{I}(\mathbf{z};\mathbf{x}) - \mathbb{I}(\mathbf{z};\mathbf{y})
\text{ where } \beta \ge 0
$$

- $p(\mathbf{z}\|\mathbf{x}), p(\mathbf{y}\|\mathbf{z})$ 분포에 관한 최적화이다.

![](3.jpg)
_Figure 3: Information diagrams for information bottleneck. (a) $Z$ can contain any amount of information about $X$ (whether it useful for predicting $Y$ or not), but it cannot contain information about $Y$ that is not shared with $X$. (b) The optimal representation for $Z$ maximizes $\mathbb{I}(Z, Y)$ and minimizes $\mathbb{I}(Z, X)$. Used with kind permission of Katie Everett._

Fig. 3은 IB principle을 그림으로 표현한 것이다.
$Z$는 $X$의 함수이지만, $Y$와 독립적이라고 가정한다.
즉, graphical model $Z \leftarrow X \leftrightarrow Y$을 가정하기 때문에, joint distribution은 다음과 같다.

$$
p(\mathbf{x}, \mathbf{y}, \mathbf{z}) = p(\mathbf{z}|\mathbf{x})p(\mathbf{y}|\mathbf{x})p(\mathbf{x})
$$

Fig. 3 (a)는 $Z$가 원하는 만큼 $X$의 정보를 capture할 수 있지만, $Y$만 가지고 있는 정보는 포함할 수 없다는 것을 보여준다.
Fig. 3 (b)을 보면, optimal $Z$는 $Y$에 유용한 $X$의 정보만 capture하여 capacity를 낭비하는 것을 방지하고, 입력 $X$와 관련 없는 정보를 없앤다.
그 결과, $Z$는 $Y$에 유용한 최소한의 $X$ 정보만 가지게 된다.

모든 random variables가 discrete하고 $\mathbf{z}$가 $\mathbf{x}$의 deterministic function이거나 모든 variables가 jointly Gaussian이면 objective는 analytical하게 solve할 수 있다.

하지만 보통 정확하게 solve하는 것은 어렵기 때문에 approximation (Variational IB)을 사용한다.


# References

1. [Wiki - Mutual information][1]{:target="_blank"}
2. [Wiki - Information bottleneck method][2]{:target="_blank"}
3. [Wiki - Pearson correlation coefficient][3]{:target="_blank"}
4. [Kevin P. Murphy. (2022). Probabilistic Machine Learning: An introduction, MIT Press][3]{:target="_blank"}
5. [Kevin P. Murphy. (2023). Probabilistic Machine Learning: Advanced Topics, MIT Press][4]{:target="_blank"}
6. [Tistory - Mutual Information이란? (상호의존정보란?, Normalized Mutual Information, Maximal Information Coefficient), 유니의 공부][6]{:target="_blank"}

[1]: https://en.wikipedia.org/wiki/Mutual_information
[2]: https://en.wikipedia.org/wiki/Information_bottleneck_method
[3]: https://en.wikipedia.org/wiki/Pearson_correlation_coefficient
[4]: https://probml.github.io/pml-book/book1.html
[5]: https://probml.github.io/pml-book/book2.html
[6]: https://process-mining.tistory.com/141