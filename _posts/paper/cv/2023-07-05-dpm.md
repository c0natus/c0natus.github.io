---
title: Deep Unsupervised Learning using Nonequilibrium Thermodynamics, (ICML'15)
categories: [Paper, Computer Vision]
tags: [Generative model, Diffusion]
img_path: /assets/img/posts/paper/cv/dpm/
author: gshan
math: true
---

|[Paper Link][1]{:target="_blank"}|[Official Github][2]{:target="_blank"}|

## Abstract

해당 논문은 [DDPM][3]{:target="_blank"} 이전에 나온 것으로, diffusion을 통해 이미지를 생성을 하는 논문이다.
DDPM 논문을 완벽히 이해했다면, 해당 논문은 안 읽어도 된다.
DDPM은 본 논문에서 제시하는 forward 시간을 단축한 논문이다.

Challenges
: Probabilistic model은 tractability와 flexibility 사이의 trade-off를 가지고 있다.
 
Address
: 저자들은 tractability와 flexibility를 동시에 만족 시킬 수 있는 방법인 forward/reverse diffusion process를 제안한다.


## 1. Introduction

Tractable한 models (e.g. Gaussian or Laplace)는 analytical evaluation이 가능하고 data에 쉽게 fit될 수 있지만, 복잡한 dataset의 구조를 적절하게 표현할 수 없다.
Flexible한 models는 복잡한 dataset의 구조를 표현할 수 있지만, training, evaluating drawing smaples에서 많은 계산이 필요한 Monte Carlo process를 수행해야 한다.

그래서 저자들은 probabilistic models를 제안한다.
1. **(Flexibility)** Extreme flexibility in model structure.
2. **(Tractability)** Exact sampling. 
3. **(Compute a posterior)** Easy multiplication with other distributions.
4. **(Complexity)** The model log likelihood, and the probability of individual states, to be cheaply evaluated.

저자들은 non-equilibrium statistical physics에서 영감을 받아, 점진적으로 하나의 distribution에서 다른 것으로 변화시키는 generative Markov chain을 사용한다.
- Diffusion process를 사용해 simple distribution을 target distribution으로 변환한다.

이처럼 점진적으로 변화시키는 과정을 통해서 generative model은 flexibility와 tractability를 만족하게 된다.
- **(Flexibility)** 각 단계마다 small perturbation을 추정함으로써 복잡한 dataset을 표현할 수 있다.
- **(Tractability)** 각 단계마다 analytical evaluation이 가능한 분포를 사용하므로, 전체 과정도 analytical evaluation이 가능해진다.

## 2. Algorithm

![](1.png)

본 논문은 복잡한 data 분포를 간단한 data 분포로 바꾸는 forward diffusion process $q$를 정의하고, reversal diffusion process $p$로 generative model 분포를 정의하는 것을 목표로 한다.
- Fig. 1의 첫 번째와 두 번째 row를 참고하자.
- 학습 해야 하는 것은 reverse diffusion process $p$이다.

Dirt term이란 stochastic process에서 mean이 변화하는 속도이다.
Fig. 1의 마지막 row를 보면 나선형 모양으로 mean의 변화율이 작은 것을 볼 수 있다.

### 2.1. Forward Trajectory

관찰된 data 분포를 $q(\textbf{x}^{(0)})$로 표기한다.
그리고 해당 분포에 Markov diffusion kernel $T_\pi(\textbf{y}\|\textbf{y}^\prime;\beta)$를 반복적으로 적용시켜 analytically tractable 분포 $\pi(\textbf{y})$로 변환시킨다.
$\beta$는 diffusion rate이다.

$$
\begin{align}
\pi(\textbf{y}) &= \int d\textbf{y}^\prime T_\pi(\textbf{y}|\textbf{y}^\prime;\beta)\pi(\textbf{y}^\prime) \\
q(\textbf{x}^{(t)}|\textbf{x}^{(t-1)}) &= T_\pi(\textbf{x}^{(t)}|\textbf{x}^{(t-1)}; \beta_t)
\end{align}
$$

처음 data 분포를 T번 diffusion한 것은 아래와 같이 나타낼 수 있다.
Foward diffusion을 통해 identity-covariance를 가지는 Gaussian 분포 또는 독립적인 binomial 분포로 변환될 수 있다.

$$
\begin{equation}
q\big(\textbf{x}^{(0 \cdots T)}\big)
 = q\big(\textbf{x}^{(0)}\big) \prod_{t=1}^Tq\big( \textbf{x}^{(t)}|\textbf{x}^{(t-1)} \big)
\end{equation}
$$

### 2.2. Reverse Trajectory

Generative 분포는 방향만 다르게 해서 forward trajectory를 설명하도록 학습된다.

$$
\begin{align}
p\big(\textbf{x}^{(T)}\big) &= \pi\big(\textbf{x}^{(T)}\big) \\
p\big(\textbf{x}^{(0 \cdots T)}\big) &= \pi\big(\textbf{x}^{(T)}\big)\prod_{t=1}^Tp\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}\big)
\end{align}
$$

Gaussian/bonomial diffusion의 경우 step size $\beta$가 작다면 reversal diffusion process는 forward process와 동일한 형태를 띈다. ([Ref][4]. 1)
- $q\big( \textbf{x}^{(t)}\|\textbf{x}^{(t-1)} \big)$가 Gaussian/binomial diffusion이고 $\beta$가 작다면, $p\big( \textbf{x}^{(t-1)}\|\textbf{x}^{(t)} \big)$도 Gaussian/binomial diffusion이 된다.
- Trajectory가 길수록 더 작은 diffusion rate $\beta$를 사용할 수 있다.

![](2.png)

학습 동안 Gaussian diffusion에 대해선 mean $f_\mu(\cdot)$과 covariance $f_\sum(\cdot)$를 추정하고, binomial diffusion에 대해선 bit flip probability $f_b(\cdot)$를 추정해야 한다.

### 2.3. Model Probability

생성 model의 probability를 marginal distribution으로 나타내면 아래와 같다.

$$
\begin{equation}
p\big(\textbf{x}^{(0)}\big) = \int d\textbf{x}^{(1\cdots T)}p\big(\textbf{x}^{(0\cdots T)}\big)
\end{equation}
$$

Integral 계산이 intractable하므로 비교적 계산하기 쉬운 data distribution $q$를 통해 계산한다.
이를 importance sampling이라고 한다.

$$
\begin{align}
p\big(\textbf{x}^{(0)}\big) &= \int d\textbf{x}^{(1\cdots T)}p\big(\textbf{x}^{(0\cdots T)}\big) \\
&= \int d\textbf{x}^{(1\cdots T)} q\big(\textbf{x}^{(1\cdots T)}|\textbf{x}^{(0)}\big) \frac{p\big(\textbf{x}^{(0\cdots T)}\big)}{q\big(\textbf{x}^{(1\cdots T)}|\textbf{x}^{(0)} \big)} \\
&= \int d\textbf{x}^{(1\cdots T)} q\big(\textbf{x}^{(1\cdots T)}|\textbf{x}^{(0)}\big) p\big(\textbf{x}^{(T)}\big) \prod_{t=1}^T \frac{p\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}\big)}{q\big(\textbf{x}^{(t)}|\textbf{x}^{(t-1)} \big)} \\
\nonumber &= \mathbb{E}_{\textbf{x}^{(1\cdots T)} \sim q\big(\textbf{x}^{(1\cdots T)}|\textbf{x}^{(0)}\big)}\bigg[p\big(\textbf{x}^{(T)}\big)\prod_{t=1}^T \frac{p\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}\big)}{q\big(\textbf{x}^{(t)}|\textbf{x}^{(t-1)} \big)}\bigg]
\end{align}
$$

- Eq. 3, 5를 통해 Eq. 8에서 Eq. 9로 식을 변형할 수 있다.

### 2.4. Training

Model의 log likelihood를 최대화하는 방향으로 학습을 진행해야 한다.

$$
\begin{align}
L &= \int d\textbf{x}^{(0)}q\big(\textbf{x}^{(0)}\big)\text{ log }p(\textbf{x}^{(0)})\\
&= \int d\textbf{x}^{(0)}q\big(\textbf{x}^{(0)}\big)\text{ log }\bigg(\mathbb{E}_{\textbf{x}^{(1\cdots T)} \sim q\big(\textbf{x}^{(1\cdots T)}|\textbf{x}^{(0)}\big)}\bigg[p\big(\textbf{x}^{(T)}\big)\prod_{t=1}^T \frac{p\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}\big)}{q\big(\textbf{x}^{(t)}|\textbf{x}^{(t-1)} \big)}\bigg]\bigg)
\end{align}
$$

Logarithm은 concave이기 때문에 Jensen's inequality로 아래의 식을 만족하게 된다.

$$
\begin{equation}
\begin{split}
L &\ge \int d\textbf{x}^{(0)}q\big(\textbf{x}^{(0)}\big)\mathbb{E}_{\textbf{x}^{(1\cdots T)} \sim q\big(\textbf{x}^{(1\cdots T)}|\textbf{x}^{(0)}\big)}\bigg[p\big(\textbf{x}^{(T)}\big)\text{ log }\bigg(\prod_{t=1}^T \frac{p\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}\big)}{q\big(\textbf{x}^{(t)}|\textbf{x}^{(t-1)} \big)}\bigg)\bigg]\\
&= \int d\textbf{x}^{(0 \cdots T)} q\big(\textbf{x}^{(0 \cdots T)}\big)\text{ log }\bigg(p\big(\textbf{x}^{(T)}\big)\prod_{t=1}^T \frac{p\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}\big)}{q\big(\textbf{x}^{(t)}|\textbf{x}^{(t-1)} \big)}\bigg)
\end{split}
\end{equation}
$$

여기서 log likelihood를 최대화하는 문제를 lower bound $K$를 최대화하는 문제로 생각할 수 있다.
논문의 Appendix B를 참고하면 아래와 같이 식을 바꿀 수 있다.

먼저, log 안에 있는 $p\big(\textbf{x}^{(T)}\big)$을 분리하자.

$$
\begin{split}
K &= \int d\textbf{x}^{(0 \cdots T)} q\big(\textbf{x}^{(0 \cdots T)}\big)\text{ log }\bigg(p\big(\textbf{x}^{(T)}\big)\prod_{t=1}^T \frac{p\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}\big)}{q\big(\textbf{x}^{(t)}|\textbf{x}^{(t-1)} \big)}\bigg)\\
&= \int d\textbf{x}^{(0 \cdots T)} q\big(\textbf{x}^{(0 \cdots T)}\big)\text{ log }\bigg(\prod_{t=1}^T \frac{p\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}\big)}{q\big(\textbf{x}^{(t)}|\textbf{x}^{(t-1)} \big)}\bigg) + \int d\textbf{x}^{(0 \cdots T)} q\big(\textbf{x}^{(0 \cdots T)}\big)\text{ log }p\big(\textbf{x}^{(T)}\big)\\
&= \int d\textbf{x}^{(0 \cdots T)} q\big(\textbf{x}^{(0 \cdots T)}\big)\text{ log }\bigg(\prod_{t=1}^T \frac{p\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}\big)}{q\big(\textbf{x}^{(t)}|\textbf{x}^{(t-1)} \big)}\bigg) + \int d\textbf{x}^{(T)} q\big(\textbf{x}^{(T)}\big)\text{ log }p\big(\textbf{x}^{(T)}\big)\\
&= \int d\textbf{x}^{(0 \cdots T)} q\big(\textbf{x}^{(0 \cdots T)}\big)\text{ log }\bigg(\prod_{t=1}^T \frac{p\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}\big)}{q\big(\textbf{x}^{(t)}|\textbf{x}^{(t-1)} \big)}\bigg) + \int d\textbf{x}^{(T)} q\big(\textbf{x}^{(T)}\big)\text{ log }\pi(\textbf{x}^{(T)})
\end{split}
$$

본 논문에서는 설계상 $\pi(\textbf{x}^{(t)})$의 cross entropy는 diffusion kernels에서 상수(constant)이고, $p(\textbf{x}^{(t)})$의 entropy와 동일하다고 말하고 있다.

$$
\begin{split}
H_p(\textbf{X}^{(T)}) &= -\int d\textbf{x}^{(t)}q(\textbf{x}^{(t)}) \text{ log }\pi(\textbf{x}^{(t)}) \text{, for } \forall t\\
\therefore K &= \sum_{t=1}^T \int d\textbf{x}^{(0 \cdots T)} q\big(\textbf{x}^{(0 \cdots T)}\big) \text{ log }\bigg[\frac{p\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}\big)}{q\big(\textbf{x}^{(t)}|\textbf{x}^{(t-1)}\big)}\bigg] - H_p(\textbf{X}^{(T)})
\end{split}
$$

> 무슨 말인지 제대로 이해하지 못했다. 뇌피셜로는 매우 작은 step $\beta$에 대해 결국 $q$와 $\pi$가 Standard Gaussian distribution $p(\textbf{X}^{(T)})$으로 동일해지기 때문에 entropy로 보는 것 같다.

그리고 $t=0$에서 edge effect를 제거하기 위해 reverse trajectory의 마지막 step을 forward diffusion step과 동일하게 설정한다.

$$
p\big(\textbf{x}^{(0)}|\textbf{x}^{(1)}\big) = q\big(\textbf{x}^{(1)}|\textbf{x}^{(0)}\big)\frac{\pi(\textbf{x}^{(0)})}{\pi(\textbf{x}^{(1)})} = T_\pi\big(\textbf{x}^{(0)}|\textbf{x}^{(1)};\beta_1\big)
$$

> Computer vision에서 edge effect는 주로 CNN을 사용할 때, model이 image의 가장자리에 있는 객체(object) 또는 패턴(pattern)을 처리하는 데 어려움을 겪는 현상을 말한다.
> Image의 가장자리에 있는 pixel은 이웃 pixel과의 관계가 덜 정의되어 있으므로 정보가 부족할 수 있다.
> 일반적으로 CNN은 kernel 또는 filter를 사용하여 image에서 feature를 추출하는데, 가장자리에 있는 pixel은 이 커널과 충분한 상호 작용을 할 수 없어 정보의 손실이 발생할 수 있다.  
> Diffusion에서는 생성 관점에서 $\textbf{x}^{(0)}, \textbf{x}^{(1)}$ 사이에서 edge effect가 생긴다고 생각하는 것 같다. 완벽히 이해하진 못했다.

$$
\begin{split}
K &= \sum_{t=2}^T \int d\textbf{x}^{(0 \cdots T)} q\big(\textbf{x}^{(0 \cdots T)}\big) \text{ log }\bigg[\frac{p\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}\big)}{q\big(\textbf{x}^{(t)}|\textbf{x}^{(t-1)}\big)}\bigg] + \int d\textbf{x}^{(0 \cdots T)} q\big(\textbf{x}^{(0 \cdots T)}\big) \text{ log }\bigg[\frac{p\big(\textbf{x}^{(0)}|\textbf{x}^{(1)}\big)}{q\big(\textbf{x}^{(1)}|\textbf{x}^{(0)}\big)}\bigg] - H_p(\textbf{X}^{(T)})\\
&= \sum_{t=2}^T \int d\textbf{x}^{(0 \cdots T)} q\big(\textbf{x}^{(0 \cdots T)}\big) \text{ log }\bigg[\frac{p\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}\big)}{q\big(\textbf{x}^{(t)}|\textbf{x}^{(t-1)}\big)}\bigg] + \int d\textbf{x}^{(0, 1)} q\big(\textbf{x}^{(0, 1)}\big) \text{ log }\bigg[\frac{q\big(\textbf{x}^{(1)}|\textbf{x}^{(0)}\big)\pi(\textbf{x}^{(0)})}{q\big(\textbf{x}^{(1)}|\textbf{x}^{(0)}\big)\pi(\textbf{x}^{(1)})}\bigg] - H_p(\textbf{X}^{(T)})\\
&= \sum_{t=2}^T \int d\textbf{x}^{(0 \cdots T)} q\big(\textbf{x}^{(0 \cdots T)}\big) \text{ log }\bigg[\frac{p\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}\big)}{q\big(\textbf{x}^{(t)}|\textbf{x}^{(t-1)}\big)}\bigg] + \int d\textbf{x}^{(0, 1)} q\big(\textbf{x}^{(0, 1)}\big) \text{ log }\bigg[\frac{\pi(\textbf{x}^{(0)})}{\pi(\textbf{x}^{(1)})}\bigg] - H_p(\textbf{X}^{(T)})\\
&= \sum_{t=2}^T \int d\textbf{x}^{(0 \cdots T)} q\big(\textbf{x}^{(0 \cdots T)}\big) \text{ log }\bigg[\frac{p\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}\big)}{q\big(\textbf{x}^{(t)}|\textbf{x}^{(t-1)}\big)}\bigg] + H_p(\textbf{X}^{(T)}) - H_p(\textbf{X}^{(T)}) - H_p(\textbf{X}^{(T)})\\
&= \sum_{t=2}^T \int d\textbf{x}^{(0 \cdots T)} q\big(\textbf{x}^{(0 \cdots T)}\big) \text{ log }\bigg[\frac{p\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}\big)}{q\big(\textbf{x}^{(t)}|\textbf{x}^{(t-1)}\big)}\bigg] - H_p(\textbf{X}^{(T)})
\end{split}
$$

위의 식을 Bayes' rule을 사용해 $q\big(\textbf{x}^{(t)}\|\textbf{x}^{(t-1)}\big)$를 posterior와 marginals 형태로 변형할 수 있다.

$$
\begin{split}
K &= \sum_{t=2}^T \int d\textbf{x}^{(0 \cdots T)} q\big(\textbf{x}^{(0 \cdots T)}\big) \text{ log }\bigg[\frac{p\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}\big)}{q\big(\textbf{x}^{(t)}|\textbf{x}^{(t-1)}\big)}\bigg] - H_p(\textbf{X}^{(T)})\\
K &= \sum_{t=2}^T \int d\textbf{x}^{(0 \cdots T)} q\big(\textbf{x}^{(0 \cdots T)}\big) \text{ log }\bigg[\frac{p\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}\big)}{q\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}, \textbf{x}^{(0)}\big)}\frac{q\big(\textbf{x}^{(t-1)}|\textbf{x}^{(0)}\big)}{q\big(\textbf{x}^{(t)}|\textbf{x}^{(0)}\big)}\bigg] - H_p(\textbf{X}^{(T)})
\end{split}
$$

여기서 condition entropies를 발견할 수 있다.

$$
\begin{split}
K = &\sum_{t=2}^T \int d\textbf{x}^{(0 \cdots T)} q\big(\textbf{x}^{(0 \cdots T)}\big) \text{ log }\bigg[\frac{p\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}\big)}{q\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}, \textbf{x}^{(0)}\big)}\bigg] + \sum_{t=2}^T \int d\textbf{x}^{(0 \cdots T)} q\big(\textbf{x}^{(0 \cdots T)}\big)\text{ log }\bigg[\frac{q\big(\textbf{x}^{(t-1)}|\textbf{x}^{(0)}\big)}{q\big(\textbf{x}^{(t)}|\textbf{x}^{(0)}\big)}\bigg] - H_p(\textbf{X}^{(T)})\\
= &\sum_{t=2}^T \int d\textbf{x}^{(0 \cdots T)} q\big(\textbf{x}^{(0 \cdots T)}\big) \text{ log }\bigg[\frac{p\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}\big)}{q\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}, \textbf{x}^{(0)}\big)}\bigg] \\
&+ \sum_{t=2}^T \int d\textbf{x}^{(0, 1)} q\big(\textbf{x}^{(0, 1)}\big) \int d\textbf{x}^{(2 \cdots T)} q\big(\textbf{x}^{(2 \cdots T)}\big) \bigg[\text{ log }q\big(\textbf{x}^{(t-1)}|\textbf{x}^{(0)}\big) - \text{ log }q\big(\textbf{x}^{(t)}|\textbf{x}^{(0)}\big)\bigg] - H_p(\textbf{X}^{(T)})\\
= &\sum_{t=2}^T \int d\textbf{x}^{(0 \cdots T)} q\big(\textbf{x}^{(0 \cdots T)}\big) \text{ log }\bigg[\frac{p\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}\big)}{q\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}, \textbf{x}^{(0)}\big)}\bigg] + \sum_{t=2}^T\bigg[ H_q\big(\textbf{X}^{(t)}|\textbf{X}^{(0)}\big) - H_q\big(\textbf{X}^{(t-1)}|\textbf{X}^{(0)}\big) \bigg] - H_p(\textbf{X}^{(T)})\\
\end{split}
$$

위의 수식에서 첫 번째 term을 KL divergence로 표현할 수 있다.

$$
\begin{split}
K = & \sum_{t=2}^T \int d\textbf{x}^{(0 \cdots T)} q\big(\textbf{x}^{(0 \cdots T)}\big) \text{ log }\bigg[\frac{p\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}\big)}{q\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}, \textbf{x}^{(0)}\big)}\bigg] + \sum_{t=2}^T\bigg[ H_q\big(\textbf{X}^{(t)}|\textbf{X}^{(0)}\big) - H_q\big(\textbf{X}^{(t-1)}|\textbf{X}^{(0)}\big) \bigg] - H_p(\textbf{X}^{(T)})\\
= & \sum_{t=2}^T \int d\textbf{x}^{(0 \cdots t-2, t \cdots T)} q\big(\textbf{x}^{(0 \cdots t-2, t \cdots T)}\big) \int d\textbf{x}^{(t-1)} q\big(\textbf{x}^{(t-1)}\big)\text{ log }\bigg[\frac{p\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}\big)}{q\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}, \textbf{x}^{(0)}\big)}\bigg] \\
&+ \sum_{t=2}^T\bigg[ H_q\big(\textbf{X}^{(t)}|\textbf{X}^{(0)}\big) - H_q\big(\textbf{X}^{(t-1)}|\textbf{X}^{(0)}\big) \bigg] - H_p(\textbf{X}^{(T)})\\
= & -\sum_{t=2}^T \int d\textbf{x}^{(0, t)} q\big(\textbf{x}^{(0, t)}\big) D_{KL}\big(q\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}, \textbf{x}^{(0)}\big)||p\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}\big)\big) + \sum_{t=2}^T\bigg[ H_q\big(\textbf{X}^{(t)}|\textbf{X}^{(0)}\big) - H_q\big(\textbf{X}^{(t-1)}|\textbf{X}^{(0)}\big) \bigg] - H_p(\textbf{X}^{(T)})\\
= & -\sum_{t=2}^T \mathbb{E}_{\textbf{x}^{(0, t)}\sim q(\textbf{x}^{(0, t)})}\bigg[D_{KL}\big(q\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}, \textbf{x}^{(0)}\big)||p\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}\big)\big)\bigg] + \sum_{t=2}^T\bigg[ H_q\big(\textbf{X}^{(t)}|\textbf{X}^{(0)}\big) - H_q\big(\textbf{X}^{(t-1)}|\textbf{X}^{(0)}\big) \bigg] - H_p(\textbf{X}^{(T)})\\
\end{split}
$$

Entropies는 analytical하게 계산될 수 있고, KL divergence도 $\textbf{x}^{(0)}$과 $\textbf{x}^{(t)}$가 주어지면 analytical하게 계산할 수 있다.

이제 다시 likelihood를 최대화하는 관점에서 살펴보자.

$$
\begin{align}
L \ge & K \\
K = &-\sum_{t=2}^T \mathbb{E}_{\textbf{x}^{(0, t)}\sim q(\textbf{x}^{(0, t)})}\bigg[D_{KL}\big(q\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}, \textbf{x}^{(0)}\big)||p\big(\textbf{x}^{(t-1)}|\textbf{x}^{(t)}\big)\big)\bigg] \\
\nonumber &+ \sum_{t=2}^T\bigg[ H_q\big(\textbf{X}^{(t)}|\textbf{X}^{(0)}\big) - H_q\big(\textbf{X}^{(t-1)}|\textbf{X}^{(0)}\big) \bigg] - H_p(\textbf{X}^{(T)})\\
\end{align}
$$

여기서 reverse diffusioin $p$를 학습해 lover bound K를 최대화해야 한다.

$$
\hat{p}\big( \textbf{x}^{(t-1)}|\textbf{x}^{(t)} \big) = \underset{p( \textbf{x}^{(t-1)}|\textbf{x}^{(t)})}{\text{ argmax }}K
$$

그리고 forward diffusion $q$는 아주 작은 Gaussian noise를 추가하고 있다.
즉, 학습을 통해서 구한 $\hat{p}$의 평균 $\mu$와 분산 $\sigma$이 $q\big(\textbf{x}^{(t-1)}\|\textbf{x}^{(t)}, \textbf{x}^{(0)}\big)$와 유사해야 한다.

---

이후 section은 완벽히 이해하질 못했다. (Diffusion을 활용하는 데 이해 못해도 괜찮다.)
Energy-based 생성과 score-based 생성을 추가로 공부해서 살펴보면 이해가 될 듯하다.

## References

1. In Proceedings of the 1$^\text{st}$, Berkeley Symposium on Mathematical Statistics and Probability.

[1]: https://arxiv.org/pdf/1503.03585.pdf
[2]: https://github.com/Sohl-Dickstein/Diffusion-Probabilistic-Models
[3]: https://arxiv.org/pdf/2006.11239.pdf
[4]: https://c0natus.github.io/posts/diffusion/#references