---
title: "Distilling the Knowledge in a Neural Network (NIPS'14)"
categories: [Paper, Core]
tags: [Knowledge Distillation]
img_path: /assets/img/posts/paper/core/kd/
author: gshan
math: true
---

|[Paper Link][1]{:target="_blank"}|

# Abstract

Model의 성능을 향상하는 방법 중 하나는 여러 모델을 ensemble하는 것이다. 하지만 ensemble은 시간이 많이 소요된다.
[Caruana et al.][2]{:target="_blank"}는 ensemble의 knowledge가 single model로 압축될 수 있는 것을 보여 주었다.
본 논문의 저자들은 해당 idea를 발전시켜 ensemble의 knowledge를 single model로 distillate한다.
또한, 하나 이상의 models과 specialist models로 구성된 새로운 유형의 ensemble을 소개한다.

# Ⅰ. Introduction

곤충은 필요에 맞춰 최적화된 애벌레(영양소 비축)와 성체(활동 및 번식) 형태를 가진다. 
마찬가지로, model도 training과 deployment stage에서 요구하는 게 다르므로 서로 다른 구조를 가져야 한다.
Traning에서는 data에서 정보를 추출하기 위해 cumbersome model(ensemble, very large model, etc.)을 훈련한다.
하지만, deployment는 real time이고 latency가 중요하므로 training과는 다른 구조의 model을 사용해야 한다.
본 논문에서는 <span class="text-color-yellow">학습된 cumbersome model의 knowledge를 deployment에서 사용될 small model로 transfer하는 "distillation"</span>을 제안한다.
즉, cumbersome model의 불필요한 parameters를 증류(distillation)하여 정말 필요한 parameters만 small model에서 사용한다.
해당 전략은 [Caruana et al.][2]{:target="_blank"}에서 시작되었다.

User의 목표가 model이 새로운 data에 대해 일반화가 잘 되도록 하는 것이어도, model은 일반적으로 training data에 대해 performance를 최적화하도록 훈련된다.
새로운 data에 일반화가 잘 되기 위해선, 알맞은 방향에 대한 정보 같은 것이 필요한데 보통 해당 정보를 얻는 것은 불가능하다.
하지만, large model의 knowledge를 small model로 distilling할 때 large model는 그러한 정보를 제공할 수 있다. 
바로 large model의 softmax 값이다.
이를 활용해 large model의 soft target을 바탕으로 softmax cross entropy loss 값를 계산해 small model이 일반화되도록 훈련시킬 수 있다.

![](1.jpg)
_Overall loss function, [출처][3]{:target="_blank"}_

Soft target이 높은 entropy를 가지면 hard target보다 더 많은 정보를 제공하고, training case간 gradient의 variance를 낮춘다.
따라서 small model은 large model보다 적은 data와 높은 learning rate로 학습이 가능하다.

Cumbersome model이 높은 confidence로 정답을 맞추는 MNIST에서, 정답을 제외하고 나머지의 softmax 확률 값은 매우 작다. 예로 들어, 숫자 2에 대해 3으로 인식될 확률은 $10^{-6}$, 7로 인식될 확률은 $10^{-9}$라고 하자. 분명 해당 값들은 숫자 2가 3 또는 7과 유사하다는 정보를 가지고 있지만, 거의 0에 가깝다. 따라서 knowledge를 transfer하는 과정에서 small model의 cross-entropy loss에 거의 영향을 끼치지 않을 것이다. 이를 방지하기 위해 [Caruana et al.][2]{:target="_blank"}은 softmax를 거치지 않은 logit 값으로 MSE를 계산하고, 본 저자들이 제안하는 'distillation'은 temperature를 높여 cumbersome model의 softmax가 충분히 soft해지도록 한다. Logit 값으로 MSE를 계산하는 것은 'distillation'의 특별한 case로 temperature가 매우 높은 'distillation'이다.

Small model을 학습할 때, cumbersome model의 soft target뿐만 아니라 label이 있는 dataset을 활용해 true target도 사용하는 것이 경험적으로 좋다고 말한다.

# Ⅱ. Distillation

Neural networks는 보통 output layer의 logit($z_i$)과 softmax를 가지고 class probabilities를 생성한다.

$$
q_i = \frac{\textrm{exp}(z_i/T)}{\sum_j \textrm{exp}(z_j/T)}
$$

- $z_i$: i번째 class의 logit
- $T$: temperature, 일반적으로 1
- $q_i$: i번째 class의 probability

높은 $T$를 사용하면 cumbersome model은 더 soft한 probability distribution를 생성한다. Temperature라고 명명하는 것은 꽤 직관적이다. 일반적으로 증류(distillation) 시 적절한 온도(temperature)를 사용하면 목적에 맞게 불순물을 제거할 수 있다.

Distillation을 할 때, correct label도 같이 사용하는 것이 경험적으로 더 나았다. 
Soft targets을 수정하기 위해 사용될 수도 있지만, 두 개의 objective functions의 weighted average하는 것이 더 좋은 결과를 냈다.
- 첫 번째 objective function은 cumbersome model과 같은 temperature를 사용한, soft targets과의 cross-entropy이다.
- 두 번째 objective function은 tempertature를 1로 설정한, correct labels과의 cross entropy이다. 경험적으로 해당 objective에 훨씬 작은 weight를 주는 게 결과가 좋았다.

Soft targets의 gradient 크기는 $\frac{1}{T^2}$로 조정된다. 따라서 <span class="text-color-yellow">hard와 soft targets를 모두 사용할 때 soft에 $T^2$을 곱하는 것이 중요</span>하다.

## ⅰ. Matching logits is a special case of distillation

잠깐 언급을 했듯이, [Caruana et al.][2]{:target="_blank"}이 제안한 방법은 distillation의 특별한 case이다. 왜 그런지 살펴보도록 하자.

Soft targets과의 cross-entropy gradient는 다음과 같이 나타낼 수 있다. 미분하는 과정은 [여기][4]를 참고하자.

$$
\frac{\partial C}{\partial z_i} 
  = \frac{1}{T}(q_i - p_i)
  = \frac{1}{T}\bigg( \frac{e^{z_i/T}}{\sum_j e^{z_j/T}} - \frac{e^{v_i/T}}{\sum_j e^{v_j/T}} \bigg)
$$

- $p_i$: soft target probabilities
- $v_i$: logits of cumbersome model

만약 logits에 비해 temperature가 높다면 아래와 같은 식으로 근사가 된다.

$$
\frac{\partial C}{\partial z_i}
  \approx \frac{1}{T}\bigg( \frac{1+z_i/T}{N+\sum_j z_j/T} - \frac{1+v_i/T}{N+\sum_j v_j/T} \bigg)
$$

만약 각 transfer case마다 logits이 zero-mean($\sum_jz_j=\sum_jv_j=0$)을 가진다면 아래와 같이 된다. 

$$
\frac{\partial C}{\partial z_i}
  \approx \frac{1}{NT^2}(z_i-v_i)
$$

따라서 temperature가 높으면 $\frac{1}{2}(z_i-v_i)^2$을 최소화하는 것과 같아진다.

작은 temperature을 사용하면, distillation은 매우 적은 확률을 가지는 soft targets을 무시한다. 
적은 확률을 가지는 soft targets는 cumbersome model의 loss function에 영향을 거의 끼치지 않는다. 
즉, 매우 nosiy한 값으로 small model의 학습을 방해할 수 있으므로, temperature를 적게 줘 무시할 수 있다. 
하지만, 적은 확률을 가지더라도 유용한 정보를 포함할 수 있어 완전히 무시하는 것도 좋진 않다.
어느 것을 중요시할 지는 경험적으로 선택해야 한다.

- 본 논문에서는 엄청 작은 model에 knowledge를 전달할 때, 중간 크기 정도의 temperature를 줘 매우 적은 확률을 가지는 logits을 무시할 때 성능이 좋다고 한다.

---

MNIST와 speech recognition에 대한 실험, ensemble로 cumbersome model을 학습하는 법, soft targets를 사용하는 이유 등을 추가적으로 알고 싶다면 논문을 참고하자.


[1]: https://arxiv.org/pdf/1503.02531.pdf
[2]: https://dl.acm.org/doi/pdf/10.1145/1150402.1150464
[3]: https://intellabs.github.io/distiller/knowledge_distillation.html
[4]: https://c0natus.github.io/posts/cross-entropy-derivative/