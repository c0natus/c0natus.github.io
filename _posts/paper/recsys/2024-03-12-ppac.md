---
title: "Debiasing Recommendation with Personal Popularity, (WWW'24)"
categories: [Paper, RecSys]
tags: [Popularity Bias]
img_path: /assets/img/posts/paper/recsys/ppac/
author: gshan
math: true
---

|[Paper Link][1]{:target="_blank"}|[Official github][2]{:target="_blank"}|

## Abstract

추천시스템은 Global popularity (GP) bias 현상을 겪는다.
인기있는 item을 대부분의 user에게 제공하기 때문에 GP bias는 '개인화'라는 목표에 방해가 된다.

Challenges
: 1. GP bias를 완화하려는 기존 방법들이 있지만, GP bias의 근본적인 원인을 해결하지 않았다.
2. 기존 방법들은 모든 user 관점에서 global하게 popularity를 설정한다. 이는 개인의 interest를 고려할 수 없다.

Address
: 1. User-aware version popularity bias인 <kbd>personal popularity (PP)</kbd>를 제안한다.
2. 유사한 선호도를 가지는 users를 고려해 각 user마다 서로 다른 popularity items를 정의한다.
3. PP는 개인마다의 선호도를 바탕으로 설정되기 때문에, 자연스럽게 GP가 완화된 개인화 추천을 할 수 있다.


Debiasing할 때, popularity가 score에 직접적으로 끼치는 영향을 완화한다. 
만약 user가 동일하게 좋아하는 item 2개가 있다고 했을 때, popularity가 높을수록 user에게 노출될 확률이 높기 때문에 score가 더 높을 것이다.
따라서 PP와 GP가 끼치는 영향을 casuality를 통해 indirect, direct로 분해하고 direct 효과를 조절해서 GP bias를 완화한다.
- Indirect 효과의 예로는 item이 popular하더라도 인기있는 item을 좋아하는 사람인지 아닌지에 따라 score에 영향을 준는 것을 생각해볼 수 있다.

## 1. Introduction

![](figure1.png)

Global popularity (GP)는 모든 user 중 해당 item을 소비한 user의 비율로 정의한다.
GP bias는 기존의 model(MF, LightGCN 등)이 높은 GP를 가지는 item을 과도하게 제시하는 것을 말한다.
Figure 1은 ML-1M dataset에 대해 모든 user의 top 50 추천 결과에서 head와 tail item의 비율을 표시한 것으로 GP bias를 확인할 수 있다.
GP bias는 서로 다른 user에게 homogeneous (유사한) 추천 결과를 보여줄 위험성이 있기 때문에, personalized 추천을 제공하기 위해선 GP bias가 완화되어야 한다.

GP bias는 'global' 관점에서 정의된다.
즉, 모든 user에게 정의된 popular item이 똑같다.
특정한 user가 관심 없더라도 popular하다는 이유로 그 user에게 추천이 된다.
따라서 user마다 item popularity를 정의해야 한다.
본 논문에서는 이를 personal popularity (PP)라고 부른다.

PP는 유사한 user들 사이에서 item의 populairty를 측정한다.
GP와 비교했을 때, PP는 개인의 선호도를 고려하기 때문에 user마다 서로 다른 popular item set이 정의된다.

PP 값이 높은 items는 users로부터 더 좋은 평가를 받는 경향이 있기 때문에 추천 성능 향상에 도움이 될 것이다.
그리고 PP는 GP와 다르기 때문에 GP 이외의 item을 추천해 GP bias를 줄이는 데 도움이 될 것이다.

> 쉽게 말해서, GP bias를 완화하기 위해 PP가 높은 item에 더 높은 score를 준다.

PP를 추천에 활용하기 위해 저자들은 personal popularity aware counterfactual (PPAC) framework를 제안한다.

- GP bias를 완화한다.
- Model-agnostic하다.

![](figure2.png)

Figure 2에서 casual graph로 표현해 PPAC와 다른 방법들 간의 차이를 보여준다.

- Figure 2(a): MF, LightGCN과 같은 일반적인 model.
- Figure 2(b): GP를 고려해 debiasing하는 model.
- Figure 2(c): 저자들이 주장하는 score가 생성되는 더 fine-grained한 과정.
- Figure 2(d): 저자들이 제안하는 구조로, 2(c)에 기반해 PP와 GP의 영향력을 측정해 debiasing하는 model.

Figure 2(d)에서의 점선표시는 GP bias를 완화하기 위해 저자들은 PPAC에서 <kbd>GP와 PP의 영향력을 추정하고 조정</kbd>한다는 의미이다.
두 점선의 영향력을 추정하기 어렵기 때문에 PPAC는 PP와 GP를 결합하기 위해 counterfactual inference를 사용하는 <kbd>proxy variable을 도입</kbd>한다.
- Counterfactual inference는 특정 variables를 실제와 다른 reference values로 설정한 뒤, target variable에 끼치는 영향력을 추정한다.
- 예를 들어, 남성인 user의 성별을 여성(reference value)으로 바꿔 성별이 추천 성능(target variable)에 끼치는 영향력을 추정하는 것이다.

## 2. Personal Popularity

Definition 1 (Global Popularity)
: Item $$ i $$의 GP $ g_i = \| \mathcal{U}_i \| / \| \mathcal{U} \|$, where $$ \mathcal{U}_i $$는 item $$ i $$를 이전에 소비한 users

GP는 모든 users에 대해 item의 매력도를 측정한 것이기 때문에, 개인의 선호도에 대한 정보를 포함하고 있지 않다.
이는 homogeneous 추천과 GP bias라는 문제를 일으킨다.
이를 해결하기 위해 personal popularity를 제안한다.

PP를 계산하기 전에 user similarity 부터 정의해아 한다.

**Definition 2 (User Similarity)**

$$ 
sim_{u,v} = \frac{| \mathcal{I}_u \cap \mathcal{I}_v |}{| \mathcal{I}_u \cup \mathcal{I}_v|}
$$

User similarity를 기반으로 hyper-parameter인 k users를 similar user set으로 정의한다.

Definition 3 (Similar User set)
: $$ \mathcal{S}_u $$는 user $$ u $$에 대해 $$ sim_{u,v} $$가 높은 k users의 set이다.

**Definition 4 (Personal Popularity)**

$$ 
p_{u,i} = \frac{|\mathcal{S}_u^i|}{|\mathcal{S}_u|} \in [0, 1]
$$

- $$ \mathcal{S}_u^i $$는 item $$ i $$를 소비한 similar user의 수 이다.

본 논문에서는 오직 user/item id만 제공되는 기본적인 collaborative filtering 환경을 고려한다.
만약 side information이 있다면 PP의 정의는 달라질 수 있다.
- 예를 들어 user profile을 사용할 수 있다면 더 정확한 user similarity를 계산할 수 있다. 
- 또는 item category가 주어진다면, personal popuarity가 personal category preference로 확장될 수 있다.

개인마다 similar user set은 다르기 때문에 PP의 값은 서로 다를 가능성이 크다.
그렇기 때문에 PP는 개인의 관심사를 파악해 보다 개인화된 추천을 제공하고 GP bias를 줄일 수 있다.

![](figure3.png)

Figure 3은 MovieLens-1M dataset을 분석한 것이다.
- (a)는 PP가 추천에 도움이 된다는 점을 알려주고, (b)는 GP와 PP가 다르다는 것을 알려준다.

Figure 3(a)
: PP 값의 크기 순으로 정렬하고, 같은 크기를 가지는 5가지 group으로 나누었다.
그리고 각 그룹의 rating 점수를 평균을 내었다.
높은 PP 값을 가지는 item은 상대적으로 높은 rating을 가지는 것을 알 수 있다.
이는 PP가 추천에 유용하다는 점을 알려준다.

Figure 3(b)
: GP가 높은 50개 items set $$ I_{GP} $$를 추출하고 각 user마다 PP가 높은 50개 item set $$ I_{PP, u} $$를 추출한다.
그리고 $$ I_{PP,u} $$ 중 $$ I_{GP} $$에 속하지 않는 item의 수인 $ d_u = \| I_{PP,u} - I_{GP} \| $를 계산한다.
이를 5구간으로 나눠 봤을 때, 20 이상인 $$ d_u $$의 비율이 85%로 PP는 GP와 다르게 user-specific 선호도 정보를 포함한다는 것을 알려준다.

## 3. PPAC Framework

Section 3.1에서 counterfactual key concepts를 알아보고, section 3.2에서 causal 관점에서 debiasing을 위한 PPAC framework를 제안한다.
그리고 section 3.3에서 PPAC를 model에 적용하고, section 3.4에서 어떻게 train과 inference를 하는지 살펴본다.

### 3.1. Preliminaries for Counterfactual Inference

![](figure4.png)

**Causal Graph**은 DAG (Directed Acyclic Graph)로 node는 random variables를 뜻하고 edge는 variables간 causal-effect 관계를 표현한다.
저자들은 random varibale 영어 대문자(e.g, A)로 나타내고 그것의 관찰된 값은 영어 소문자(e.g., a)로 나타낸다.

$$ A \rightarrow L $$은 술이 폐암의 'direct effect'라는 것을 의미한다.
$$ A \rightarrow C \rightarrow L $$는 mediator 담배를 통해 술이 폐암에 'indirect effect'를 가진다는 것을 의미한다.
술을 마시는 사람은 담배를 피울 확률이 높기 때문에 폐암에 걸릴 수 있다.

$$ L $$의 값은 그것의 조상 node로 계산된다.

$$
L_{a,c} = L(A=a,C=c), c=C_a=C(A=a)
$$

- $$ C_a $$는 술 소비량이 $$ a $$인 사람의 담배 소비량을 나타낸다.
- $$ L_{a,c} $$는 술 소비량이 $$ a $$이고 담배 소비량이 $$ c $$인 사람의 폐암에 걸린 여부이다.

$$ L $$에 대한 $$ A $$의 'direct, indirect effect'를 추정하기 위해 counterfactual inference를 사용해야 한다\dots

**Counterfactual Inference**

Counterfactual은 하나의 viariable의 값을 변경한 결과를 상상하는 것이다.
Figure 4(b)는 '술 소비량이 달라지면 어떤 일이 일어날까?'를 고려했다.
회색 nodes는 reference state 즉, reference value로 설정된 variables이다.
이는 사실과 독립적인 intervention으로 causaul effects를 추정할 때 사용한다.

Figure 4(c)는 counterfactual 세계의 causal graph를 보여준다.
$$ C $$는 $$ c^*=C(A=a^*) $$로 설정되고 $$ L $$은 $$ L_{a,c^*} = L(A=a,C=c^*) $$로 설정된다.
이것이 counterfactual 시나리오라고 불리는 이유는 $$ A = a $$와 $$ A = a^* $$가 동시에 존재하기 때문이다.

> 잊지 말아야 할 것은 해당 시나리오는 $$ A $$가 $$ L $$에 미치는 영향을 알아보는 것이다.

**Causal Effect**

$$ A $$가 $$ L $$에 미치는 causal effect는 $$ A $$가 변할 때 target variable $$ L $$의 값이 변화하는 정도이다.
예를 들어, figure 4에서 $$ L $$에 대한 $$ A $$의 total effect (TE)는 다음곽 같다.

$$
TE = L_{a,c} - L_{a^*, c^*}
$$

TE는 natural direct effect (NDE: effect via $$ A \rightarrow L $$)과 total indirect effect (TIE: effect via $$ A \rightarrow C \rightarrow L $$)의 합으로 표현할 수 있다.

$$
\begin{split}
&NDE = L_{a,c^*} - L_{a^*, c^*} \\
&TIE = TE - NDE = L_{a,c} - L_{a,c^*}
\end{split}
$$

### 3.2. Counterfactual Debiased Recommendation

해당 section에서는 causal graph를 통해 GP와 PP가 prediction scores에 끼치는 영향과 debiasing을 위해 PPAC가 왜 좋은지에 대해 말한다.

![](figure5.png)

실제 세상에서는 user가 높은 GP와 PP의 값을 가지는 item을 알 확률이 높다.
따라서 figure 5(a)처럼 GP와 PP는 user-item interaction 확률인 prediction score $$ S $$에 직접적으로 영향을 끼친다.
저자들은 두 direct effect를 조절하는 것이 debasing하는 효과적인 방법이라고 주장한다.

두 direct effect $$ PP \rightarrow S, GP \rightarrow S $$를 각각 추정하는 것은 매우 어렵다.
Proximal casual inference에 영향을 받아, figure 5(b)처럼 proxy variable $$ X $$를 도입한다.
- Proxy variable은 서로 다른 confounders를 single-treatment 방식으로 통합하는 것이다.

저자들은 $$ S $$에 대한 PP와 GP의 casual effect를 계산하기 위해 PP와 GP node를 하나의 X node로 설정한다.
PP가 $$ U $$와 $$ I $$에 모두 영향을 끼치기 때문에 proxy variable $$ X $$도 $$ U, I $$에 영향을 끼친다.

이제 $$ X $$가 $$ S $$에 끼치는 direct effect를 추정해야 한다.
Figure 5(c)의 counterfactual world를 바탕으로 total effect (TE)를 계산한다.

$$
TE = S_{x,u,i} - S_{x^*, u^*, i^*}
$$

$$ U, I $$를 reference sates( $$ I = i^* = I(X=x^*), U = u^* = U(X=x^*) $$ )로 설정하면서, 
indirect path $$ X \rightarrow U \rightarrow S $$와 $$ X \rightarrow I \rightarrow S $$가 block되어 natural direc effect (NDE)를 추정할 수 있다.

$$
NDE = S_{x,u^*,i^*} - S_{x^*, u^*, i^*}
$$

TIE는 다음과 같다.

$$
TIE = TE - NDE = S_{x,u,i} - S_{x,u^*,i^*}
$$


> Populairty의 direct effect(즉, NDE)를 조절함으로써 **GP bias**를 없앤다는 것을 잊지 말아야 한다.
{: .prompt-warning}

저자들은 hyper-parameter $$ \epsilon $$으로 NDE를 조절한다.

$$

\begin{split}
TIE + \epsilon NDE &= S_{x,u,i} - S_{x,u^*,i^*} + \epsilon(S_{x,u^*,i^*} - S_{x^*, u^*, i^*}) \\
&= S_{x,u,i} - \epsilon S_{x^*, u^*, i^*} + (\epsilon - 1)S_{x,u^*,i^*}
\end{split}

$$

$$ x^*, u^*, i^* $$는 reference values로 어떠한 특정 값이 될 수도 있고 아니면 어떤 물리적 중요도를 안 가질 수도 있다.

> 간단히 모든 값들의 평균 값이라고 생각하자. 
> 예를 들어, GP 값들의 평균값 또는 user embedding의 평균값 처럼 특정 의미가 없는 값이다.

### 3.3. Model Designs

이제 추정해야 할 값은 $$ S_{x,u,i}, \epsilon S_{x^*, u^*, i^*}, (\epsilon - 1)S_{x,u^*,i^*}$$ 3 가지이다.

#### Estimate $$ S_{x,u,i} $$

User, item representation만 필요한 기존의 model과 다르게 $$ X $$도 추가로 고려해야 한다.
이를 위해, $$ X $$의 추정 값 $$ \hat{x}_{u,i} $$를 prediction score $$ f_R(U=u, I=i) $$에 곱한다.
- $$ f_R $$은 debiasing이 필요한 base model이다.

$$
S_{x,u,i} = \hat{x}_{u,i} * f_R(U=u, I=i)
$$

$$ X $$는 GP와 PP 정보를 포함하고 있다. 따라서 $$ \hat{x}_{u,i} = \sigma(f_{PP}(U=u,I=i)) * \sigma(f_{GP}I=i) $$로 정의한다.
$$ \sigma $$는 sigmoid 함수로 최종 값이 (0, 1) 사이의 값을 가지도록 해서 0이 안 되도록하고, 다른 model의 결과 값을 무시할 정도로 커지지 않는다.
$$ f_{PP}와 f_{GP} $$는 각각 embedding을 입력 받아 간단한 MLP로 $$ p_{u,i}, g_i $$를 맞추도록 학습한다.

#### Estimate $$ \epsilon S_{x^*, u^*, i^*} $$

$$ S_{x^*, u^*, i^*} $$는 모든 user, item에 대해 고정된 constant 값이다.
따라서 ranking에는 영향력이 없으므로 무시한다.

#### Estimate $$ (\epsilon - 1)S_{x, u^*, i^*} $$

$$ X=x, X=x^* $$인 상태가 동시에 존재하는 counterfactual world이다. 
$$ S_{x, u^*, i^*} = \hat{x}_{u,i} * f_R(U=u^*, I=i^*) $$이고, $$ f_R(U=u^*, I=i^*) $$은 고정된 값이다.
$$ (\epsilon - 1) * f_R(U=u^*, I=i^*) = \tau $$라고 두면, $$ S_{x, u^*, i^*} = \tau \hat{x}_{u,i} $$가 된다.

$$
\begin{split}
S_{x, u^*, i^*} &= \tau \hat{x}_{u,i} \\
& = \gamma p_{u,i} + \beta * g_i
\end{split}
$$

- $$ \gamma, \beta $$는 hyper-parameter이다. 
- $$ p_{u,i}, g_i $$는 training dataset으로 계산된 PP와 GP의 값이다. 

$$ \hat{x}_{u,i} $$에 대한 정의가 $$ S_{x,u,i} $$를 추정할 때와 다르다.

> 설명이 납득이 가지 않는다.
> 단순히 실험을 통해 성능이 더 좋았다고만 말하고 있다.

학습 때는 user/item embedding으로 PP와 GP를 예측하도록 만들기 때문에 embedding을 더 잘 학습할 수 있다.
추론 때는 이런 효과가 필요없고 정확한 PP와 GP 값을 사용하는 것이 더 적합하다.

따라서 $$ S_{x,u,i} $$을 학습할 때는 PP와 GP를 예측하고, counterfactual inference로 추론할 때는 PP와 GP 값을 그대로 사용한다. 

## 3.4. Training and Inference


$$ f_{PP}, f_{GP}, f_R $$을 학습해야 한다.
학습 단계에서는 GP와 PP의 효과를 regularize하지 않는다.
따라서 다음과 같이 prediction score를 구한다.

$$
\begin{split}
\hat{y}_{u,i} &= \hat{x}_{u,i} * f_R(U=u, I=i) \\
&= \sigma(\hat{p}_{u,i}) * \sigma(\hat{g}_i) * \hat{r}_{u, i}
\end{split}
$$

$$ \hat{r}_{u, i} $$는 base model $$ f_R $$에 의해 구해진 값이고, $$ \hat{p}_{u,i}, \hat{g}_i $$는 각각 $$ f_{PP}, f_{GP}  $$로 구해진 값이다. 
BPR Loss를 활용해 학습을 진행한다.

$$ f_{PP}, f_{GP} $$를 학습하기 위해 아래와 같이 MSE loss를 사용해 예측을 한다.

$$
\begin{split}
L_p &= \frac{1}{|\mathcal{R}|} \sum_{(u, i) \in \mathcal{R}} (p_{u,i} - \sigma(\hat{p}_{u,i}))^2
L_G &= \frac{1}{|\mathcal{I}|} \sum_{i \in \mathcal{I}} (g_i - \sigma(\hat{g}_{i}))^2
\end{split}
$$

최종 loss는 이들을 모두 더한 값이다.

$$
L = L_R + \alpha(L_P + L_G) + \lambda ||\Theta||^2_2
$$

- $$ \alpha, \lambda $$는 hyper-parameter 이다.

Inference 단계에서는 NDE를 조절해 GP bias를 완화한다.

$$
\sigma(\hat{p}_{u,i}) * \sigma(\hat{g}_i) * \hat{r}_{u,i} + \gamma * p_{u,i} + \beta * g_{i}
$$

$$ g_i == g_j $$일 때, $$ p_{u,i} > p_{u,j} $$ 이지만 model이 $$ \hat{y}_{u,i} < \hat{y}_{u,j} $$로 예측을 했다고 하자.
직관적으로는 $$ p_{u,i} > p_{u,j} $$이므로 i를 더 선호할 확률이 높아야 한다. 
이를 counterfactual inference를 사용해 \gamma * p_{u,i} 고려해 조정할 수 있다.

저자들은 실험적으로 $$ \gamma $$는 양수, $$ \beta $$ 는 음수일 때 GP bias를 효과적으로 완화했다고 말한다.
GP effect를 감소시키고 PP effect를 강조함으로써 추천 성능은 높이면서 GP bias를 완화할 수 있다.


## 4. Experiments

생략

[1]: https://arxiv.org/abs/2305.05204
[2]: https://github.com/Stevenn9981/PPAC