---
title: "Neural Collaborative Filtering, (WWW'17)"
categories: [Paper, RecSys]
tags: [Neural Network, Collaborative Filtering, Matrix Factorization]
img_path: /assets/img/posts/paper/recsys/neumf/
author: gshan
math: true
---

|[Paper Link][1]{:target="_blank"}|[Official Github][2]{:target="_blank"}|[Implementation][3]{:target="_blank"}|


# Abstract

DNN(Deep Neural Networks)는 음성인식, 컴퓨터비전, 자연어처리에서 엄청난 성공을 거두지만, 추천 시스템에서는 상대적으로 덜 연구되었다. 본 논문에서는 implicit feedback을 기반으로 CF(Collaborative Filtering) **추천시스템**의 핵심 문제를 해결하기 위해 **NN(Nerual Network)을 사용**하는 기술을 연구했다.  
다른 NN을 이용한 연구에서는 item의 보조 정보(글자 설명, 음악의 음향 feature)등을 이용했고, CF의 핵심 요소인 user와 item feature의 상호작용을 모델링 하는 것에는 여전히 MF(Matrix Factorization)와 user와 item의 잠재 요소(latent feature)에 의존한다.  
**내적을 신경망 구조로 바꾼 NCF**(Nerual network based Collaborative Filtering)이라 불리는 일반적인 프레임워크를 제안한다.

# Ⅰ. Introduction

개인 추천 시스템의 핵심은 CF로 알려진 평점이나 클릭과 같은 과거의 상호작용에 근거한 users의 선호도를 모델링하는 것이다. 다양한 CF 기법들 사이에서 MF가 가장 유명한다.  
- MF는 Netflix Prize로 유명해졌고, latent factor 기반 추천시스템의 표본(*defacto* approach)이 되었다.
- MF는 users와 items를 표현하는 latent features vector를 사용해 users와 items를 공유된 잠재 공간(latent space)으로 투영한다. 그후 items에 대한 user의 상호작용은 그들의 latent vector를 내적으로 모델링된다.

많은 연구자들이 MF 모델의 성능을 높이려고 했다. 하지만, 효율적인 CF 기법인 MF는 간단한 상호작용 함수인 내적을 선택했기 때문에 성능이 급격히 좋아지지 않았다.
- 예를 들어 explicit feedback에 대한 평점 예측에서 user와 item의 bias term을 상호작용 함수에 포함하면 성능이 올라간다고 알려져있다. 이것은 내적의 작은 변경처럼 보이지만, users와 items 사이의 latent feature 상호작용을 모델링할 때 더 나은 전용(dedicated) 상호작용 함수를 설계하는데 긍적적인 효과를 준다. 하지만, latent feature를 간단히 선형적으로 곱하고 합치는 내적은 복잡한 user의 상호작용 데이터의 구조를 표현하는 데 충분하지 않다.

본 논문에서는 이전에 많이 사용되었던 handcraft(수작업) 보단 데이터로부터 상호작용 함수를 학습하는 DNN의 사용을 탐색한다.
- NN은 어떤 연속적 함수라도 근사할 수 있다는 것이 증명되었다. [[참고][4]{:target="_blank"}]
- 컴퓨터비전, 자연어처리에서는 DNN이 많이 사용되지만, 추천시스템에서는 최근까지 MF 방법론에 비해 DNN 방법론이 덜 채택되었다.

본 논문에서는 CF를 위한 NN 모델링 접근법을 형식화함으로써, 추천시스템에 DNN을 적용했지만 결국 MF에 의존하고 있는 이전 논문들의 문제점을 다룬다. 본 논문은 <span class="text-color-yellow">implicit feedback에 집중</span>하고 있다.
- Implicit feedback은 자동으로 추적할 수 있고 수집하기 쉽다. 하지만, user의 만족도를 관찰할 수 없고 negative feedback이 자연적으로 부족하기 때문에 활용하기에 어려움이 있다.
- 본 논문에서는 DNN을 활용해 노이즈가 많은 implicit feedback 신호를 모델링하는 방법에 대한 중심 주제를 탐구한다.
 
본 논문의 주요 기여는 다음과 같다.
1. users와 items의 latent features를 모델링하는 NN 구조를 제안하고,  일반적인 NN기반 CF인 NCF 프레임워크를 고안한다.
2. MF를 NCF의 특수화(specialization)로 해석된다는 것을 보였고, NCF 모델링에 높은 수준의 비선형성을 부여하는 MLP를  활용한다.
3. NCF 접근법의 효과 와 CF을 위한 DNN의 가능성을 입증하기 위해 두 가지 실제 dataste에 대해 광범위한 실험을 수행한다.

# Ⅱ. Preliminaries

먼저 문제를 정의하고 이미 존재하는 implicit feedback에 대한 CF의 방법론에 대해 토의할 것이다. 그리고 내적을 사용함으로써 발생하는 제한점을 강조하여 널리 사용되고 있는 MF 모델을 짧게 요약할 것이다.

## ⅰ. Learning from Implicit Data

$M, N$이 각각 users와 itemts의 수를 나타낸다고 하자.  그리고 user-tiem 상호작용 matrix $Y \in \mathbb{R}^{M\times N}$는 users의 implicit feedback을 바탕으로 아래와 같이 정의된다.
    
$$
y_{ui}= \begin{cases}1,&\text{if interaction (user}\ u, \text{item}\  i) \ \text{is observed;}\\0, &\text{otherwise.}\end{cases}
$$
    
$y_{ui}$의 값이 1인 것은 user $u$와 item $i$ 사이의 상호작용이 있다는 것이다. 하지만, 그것이 $u$가 $i$를 좋아한다는 것은 아니다. 유사하게 $y_{ui}$의 값이 0인 것은 $u$와 $i$사이의 상호작용이 없다는 것이지, $u$가 $i$를 싫어한다는 것은 아니다.  
이는 사용자의 선호도에 대한 노이즈 신호만 제공하기 때문에, implicit feedback을 이용해 학습하는 데 어려움이 있다.  
- 관측된 entry들은 적어도 users의 items에 대한 흥미를 반영하지만, 관측되지 않은 entry들은 단순한 데이터 누락일 수 있으며 negative feedback의 자연스럽운 부족함일 수 있다.

따라서, implicit feedback으로 추천시스템을 만드는 것은 관측되지 않은 $Y$의 entry를 추정하는 문제로 귀결된다. 모델 기반 접근법은  $\hat{y}_{ui}$를 학습하는 것으로 요약될 수 있다.
    
$$
\hat{y}_{ui} = f(u, i|\Theta)\\
\text{where} \ f \ \text{is interaction function,}\ \Theta \ \text{is model parameters}
$$
    
모델 기반 접근법은 일반적으로 2가지 유형의 목적 함수를 사용한다.
- **pointwise loss function**
    
	$$
	L = min \ \frac{1}{2}(\hat{y}_{ui} - y_{ui})^2
	$$
	
	- Regression문제에 많이 사용되며 실제값과 예측값의 squared loss를 최소화 한다.
	- Negative 데이터의 부재를 다루기 위해 관찰되지 않은 모든 entry를 negative feedback으로 처리하거나 관찰되지 않은 entry에서 negative instance를 샘플링했다.
- **pairwise loss function**
    
	$$
	L = max(0, \ f(y_{\text{unobs}}) - f(y_{\text{obs}}) + \alpha) \ \ \ \text{s.t rank(}y_{\text{obs}})>\text{rank}(y_{\text{unobs}})
	$$
	
	- 관측값($y_{\text{obs}}$)이 관측되지 않은 값($y_{\text{unobs}}$)보다는 더 큰 값(더 높은 rank)을 가지므로, 두 값의 마진을 최대화하는 $f$를 찾는 것이다.

NCF 프레임워크는 상호작용 함수 $f$를 매개변수화(parameterize)하여 $\hat{y}_{ui}$를 추정한다. 따라서 NCF는 두가지 유형의 목적 함수를 자연스럽게 지원한다.

## ⅱ. Matrix Factorization

행렬 $Y$의 $y_{ui}$를 예측하는 방법 중 하나인 MF는 아래와 같이 $Y$를 보다 저차원($k<n$)의 행렬 2개$(P, Q)$로 분해하여 표현하는 방법이다.

$$
\begin{array}{c}
    Y(user-item)\\
    \left[\begin{array}{ccc}
        y_{1,1} & \cdots & y_{1,n}\\
        \vdots  & \ddots & \vdots\\
        y_{m,1} & \cdots & y_{m,n}\\
    \end{array}\right]\\
    m\times n
\end{array}
=

\begin{array}{c}
    P(user)\\
    \begin{bmatrix}
        p_{11} & \cdots & p_{1k}\\
        \vdots  & \ddots & \vdots\\
        p_{m1} & \cdots & p_{mk}\\
    \end{bmatrix}\\
    m\times k
\end{array}

\begin{array}{c}
    Q(item)\\
    \begin{bmatrix}
        q_{11} & \cdots & q_{1n}\\
        \vdots  & \ddots & \vdots\\
        q_{k1} & \cdots & q_{kn}\\
    \end{bmatrix}\\
    k\times n
\end{array}
$$

$$
\begin{align*}
where\ \  \mathbf{p}_{u}=[p_{u1}, \ldots ,p_{uk}], \ \ \   \mathbf{q}_{i}=[q_{1i}, \ldots ,q_{ki}]
\end{align*}
\\
$$

MF는 latent feature(관측되지 않은 feature)의 실제 값 vector와 각 user와 item을 연관시킨다.  
$\boldsymbol{p}_u$는 user $u$의 latent vector를 나타내고, $\boldsymbol{q}_i$는 item $i$의 latent vector를 나타낸다.
- MF는 하나의 상호작용인 $y_{ui}$를 $\boldsymbol{p}_u$와 $\boldsymbol{q}_i$의 내적으로 추정한다.

$$
\begin{align*}
\hat{y}_{u,i}= f(u,i|\mathbf{p}_{u}, \mathbf{q}_{i}) = \mathbf{p}_{u} \mathbf{q}_{i}^{T}=  \sum_{k=1}^K p_{uk} q_{ki}
\end{align*}
$$

- **MF's limitation**

  ![](1.jpg)
  _Figure 1: An example illustrates MF's limitation. From data matrix (a), u4 is most similar to u1, followed by u3, and lastly u2. However in the latent space (b), placing p4 closest to p1 makes p4 closer to p2 than p3, incurring a large ranking loss._


  Figure 1은 내적이 왜 MF의 표현력의 한계인지를 보여준다. 위의 그림을 이해하기 전에 먼저 명확히 알아야할 사실이 2가지 있다.
  1. MF는 users와 items를 같은 latent space에 매핑하기 때문에 두 명의 users 사이의 유사도 또한 내적으로 아니면 유사하게 그들의 latent vector의 cosine 각으로 측정될 수 있다.
  2. 일반성의 손실 함수가 없기 때문에, Jaccard coefficient를 MF가 복구해야 하는 두 명의 users의 ground truth 유사도로 사용한다.

  Figure 1a에서 $u_1, u_2, u_3$의 Jaccard coefficient로 유사도를 살펴보면 아래와 같다.

  $$
  s_{23}(0.66) > s_{12}(0.5) > s_{13}(0.4)
  $$

  Figure 1b는 위의 관계를 기하학적으로 보여주고 있다.
  Dash line으로 표시된 새로운 user $u_4$가 추가되었을 때 Jaccard coefficient는 아래와 같다.

  $$
  s_{41}(0.6) > S_{43}(0.4) > s_{42}(0.2)
  $$

  $u_4$는 $u_1, u_3, u_2$순서로 유사하다. 하지만, MF 모델이 $\boldsymbol{p}_4$를 latent space에 놓을 때 $\boldsymbol{p}_3$보단 $\boldsymbol{p}_2$에 가깝에 위치하게 되고, 큰 ranking loss를 얻게 된다.  
  위의 예시처럼 MF의 한계는 user와 item 간의 복잡한 관계를 저차원의 latent space에서 단순하고 고정된 내적을 사용하는 데서 기인한다.  
  - linear space 는 고정된(fixed) 특징이 있어 새로운 관계를 표현하는데 유연하지 못하다.

  이러한 문제점을 해결하는 방법 중 하나는 큰 숫자의 latent factor K를 사용하는 것이다. 하지만 이것은 data에 overfitting되어 모델의 일반화 성능을 낮춘다. 따라서 본 논문에서는 DNN을 이용해 상호작용 함수를 학습시켜 이러한 한계를 해결한다.

# Ⅲ. Nerual Collaborative Filtering

먼저 일반적인 NCF 프레임워크를 제시하며, implicit data의 이진 속성을 강조하는 확률론적 모델로 NCF를 학습하는 방법을 자세히 설명한다. 그다음, MF가 NCF에서 표현되고 일반화 될 수 있음을 보여준다.
- CF를 위한 DNN을 탐색하기 위해, MLP를 통해 user-item 상호작용 함수를 학습하는 NCF의 instance를 제안한다.

마지막으로 NCF 프레임워크에서 MF와 MLP를 앙상블하는 새로운 nerual matrix factorization을 제시한다.
- 새로운 nerual matrix factorization은 user-item latent 구조를 모델링하기 위해 MF의 선형성과 MLP의 비선형성의 장점을 통합한다.

## ⅰ. Genral Framework

![](2.jpg)
_Figure 2: Neural collaborativeltering framework_

**Input Layer**
- user $u$와 item $i$를 설명하는 $v_u^U$와 $v_i^I$로 구성되어 있다.
- Context-aware, content-based, neighbor-based 같은 users 및 items의 광범위한 모델링을 지원하도록 $v_u^U$와 $v_i^I$는 사용자가 정의할 수 있다.
- 해당 작업에서는 순수 CF에 중점을 두기 때문에, one-hot encoding으로 binarized sparse vector로 변환해서 user와 items의 ID만 입력 feature로 사용한다.
- 입력에 대한 이러한 일반적인 기능 표현( one-hot encoding)을 사용하면, content features을 사용하여 users와 items을 표시함으로써 cold-start 문제를 해결하도록 방법을 쉽게 조정할 수 있다.
    - 기존에는 새로운 input이 오면 내적을 통해 예측을 할 수 없었지만, NCF에서는 embedding을 통해 user와 item의 latent vector를 구할 수 있으므로 예측이 가능하다.
    - 하지만, 새로운 input이 들어오면 차원이 달라지고 embedding을 다시 학습시켜야 한다.
    
**Embedding Layer**
- sparse 표현을 dense vector로 투영하는 fully connected layer이다.
- User/item을 embedding한 결과는 latent factor model 관점에서 user/item의 latent vector로 볼 수 있다. 그리고 이들은 점수를 예측하기 위해 latent vectors로 매핑하는 multi-layer neural 구조에 feed 된다.
  <details>
    <summary>보충</summary>
    <div markdown="1" class="post-toggle">
    
    ![](3.jpg){:width="80%", height="80%"}
    
    보통 dense 벡터를 얻기 위해 임의로 가중치를 초기화 하지만, 별도의 fully connected neural network를 사용할 수 도 있다. 예를 들어 위 그림은 user dense 벡터를 얻기 위한 모델이다. 아래의 모델을 통해 가중치 행렬 $P$를 얻게 되는데, 이때 $P$는 $m$차원의 sparse 벡터를 $k(<m)$차원의 공간에 projection하는 변환 행렬으로도 볼 수 있다. 따라서 이 $P$행렬의 각 row는 각 user를 표현하는 저차원의 dense 벡터가 되고 이를 user latent vector로 사용하게 된다.

**Netural CF Layer**
- user latent vector 와 item latent vector를 concatenation한 벡터를 input으로 받아 DNN을 통과하는 단계다.
- 각 layer는 user-item 상호작용의 latent 구조를 찾기위해 사용자가 정의할 수 있다.
- 마지막 hidden layer $X$는 모델의 차원은 모델의 capability를 결정한다.

**Output Layer**
- $y_{ui}$의 추정치인 $\hat{y}_{ui}$를 구하는 단계다.  
	- $\hat{y}_{ui}$는 user $u$와 item $i$가 얼마나 관련 있는지를 나타내며 그 값은 0과 1 사이가 된다.
- 훈련 과정은 $\hat{y}_{ui}, y_{ui}$의 pointwise loss를 최소화 하는 방향으로 흐른다. 
	- Bayesian Personalized Ranking처럼 pairwise loss를 최소화 하는 방법론도 있지만, 본 논문에서는 NN 모델링에 집중하고 있으므로 future work로 남겨둔다.

**Formular**
    
$$
\hat{y}_{u,i}= f(P^{T}v_{u}^{U}, Q^{T}v_{i}^{I}|P,Q,\Theta_{f}) = \phi_{out}(\phi_{X}(...\phi_{2}(\phi_{1}(P^{T}v_{u}^{U}, Q^{T}v_{i}^{I}))...)), \\ \ \ 0 \leq \hat{y}_{u,i} \leq 1
\\
$$
	
- $P \in \mathbb{R}^{M \times K}$, $Q \in \mathbb{R}^{N \times K}$는 users와 items의 latent factor matrix이고, $v_u^U$와 $v_i^I$는 user $u$와 item $i$의 one-hot vector이다.
- $\Theta_f$는 상호작용 함수 $f$의 모델 parameter를 나타낸다.
- $f$는 multi-layer NN으로 정의되어 있다.
- 최종 결과가 0과 1 사이의 값이 나와야 한다.

- **Learning NCF**

  모델 parameters를 학습 시키기 위해 loss function으로, 기존에 regression에서 주로 수행되는 squared loss인 pointwise 방법론이 있다.

  $$
  L_{sqr} = \sum\limits_{(u,i) \in \mathcal{Y} \cup \mathcal{Y}^-}w_{ui}(y_{ui}-\hat{y}_{ui})^2
  $$

  $\mathcal{Y}$는 $Y$에서 관측된 상호작용, $\mathcal{Y}^-$는 관측되지 않은 모든(혹은 샘플된) 상호작용이다. 그리고 $w_{ui}$는 훈련 instance $(u, i)$의 weight를 나타내는 hyper parameter이다.
  Squared loss는 Gaussian 분포로 관측이 생성된다고 가정하기 때문에 implicit data에 완전히 부합하지 않다.
  - Implicit data의 target value $y_{ui}$는 0과 1로 이뤄져 있기 때문이다.

  이러한 implicit data의 이진 속성을 반영하기 위해 pointwise NCF를 학습에서는 *logistic*이나 *probit* 함수와 같은 확률론적 함수를 $\phi_{out}$의 활성화 함수로 사용한다.
  이러한 점들을 반영해서 liklihood function은 다음과 같이 정의할 수 있다.

  $$
  p(\mathcal{Y}, \mathcal{Y}^{-}|P,Q,\Theta_{f})= \prod_{(u,i) \in \mathcal{Y} }{\hat{y}_{u,i}}^{y_{u,i}}  \prod_{(u,j) \in \mathcal{Y}^{-}} ({1-\hat{y}_{u,j}})^{1-y_{u,i}}
  $$

  - $(u,i) \in \mathcal{Y}$에 대해 $y_{u,i}$는 항상 1이고, $(u,j) \in \mathcal{Y}^{-}$에 대해 $1-y_{u,i}$는 항상 1이다. 따라서 논문에서는 생략되어 있다.

  Likelihood의 negative logarithm을 적용한 loss function은 아래와 같다.
      
  $$
  \begin{align*}
  L &= -log \ p(\mathcal{Y}, \mathcal{Y}^{-} | P,Q,\Theta_{f}) \\
  \\
    &= -\sum_{(u,i) \in \mathcal{Y}} y_{u,i}\ log \ \hat{y}_{u,i} - (\sum_{(u,j) \ in  \mathcal{Y}^{-}} (1- y_{u,i}) \ log \ (1-\hat{y}_{u,j}) ) \\
    
    \\
    
    &= -(\sum_{(u,i) \in  \mathcal{Y} \cup \mathcal{Y}^{-} } ( y_{u,i}\ log \ \hat{y}_{u,i} + (1- y_{u,i}) \ log \ (1-\hat{y}_{u,i})))
  \end{align*}
  \\
  $$
      
  - 이것은 NCF 방법론에서 최소화해야할 목적식으로, SGD를 수행함으로써 최적화를 수행할 수 있다.
  - 이는 *binary cross entropy loss*(also known *as log loss*)와 동일한 형태로, 확률론적 함수를 채택하여 implicit feedback 추천시스템을 binary classification 문제로 해결했다.
  - 분류 인식 log loss는 추천시스템 논문에서 거의 조사되지 않았기 때문에, 본 논문에서 이를 탐색하고 그 효과를 경험적으로 보인다.

  각 iteration에서 negative instance인 $\mathcal{Y}^-$은 관측되지 않는 상호작용에서 **균등하게** 샘플링했고, 관측된 상호작용의 수를 바탕으로 샘플링 비율을 제어한다.
  - Item의 인기도에 따라 균등하지 않게 샘플링 하는 전략은 성능을 더욱 향상시킬 수 있을 것이다. 이것도 future work로 남겨둔다.

## ⅱ. GMF(Generalized Matrix Factorization)

MF가 NCF의 특별한 케이스임을 보여줄 것이다. MF는 추천시스템에서 가장 유명한 모델이고 광범위하게 조사되었기 때문에, NCF를 통해 MF를 복원할 수 있다는 것은 NCF가 factorization model의 large family를 모방할 수 있다는 것을 의미한다.

**NCF: MF formula**
    
$$
\begin{align*}
\hat{y}_{u,i} = a_{out}(h^{T}\phi_{1}(P^{T}v_{u}^{U}, Q^{T}v_{i}^{I}))

\ \ where \ \ a_{out} &= \text{identity function} \\
h^{T}&=[1,...,1]_{\text{1} \times \text{k}} \\
\phi_{1} &= P^{T}v_{u}^{U} \odot Q^{T}v_{i}^{I}  \\

\end{align*}
$$
    
- $\odot$는 element-wise product를 의미한다.
- $a_{out}$은 활성화 함수, $h$는 edge weights를 나타낸다.  $a_{out}$으로 identity function을 사용하고 $h$를 1의 균등 vector(내적과 같은 효과)로 두면 정확히 MF 모델을 복원할 수 있다.
	- $h^{T}$는 $1 \times k$ vector이고, $\phi_{1}(P^{T}v_{u}^{U}, Q^{T}v_{i}^{I})$는 $k \times 1$ vector이다.

NCF 프레임워크를 토대로, MF는 쉽게 일반화되고 확장된다.
- 예를 들어 $h$를 균등이라는 제약없이 data로 학습되게 하면, latent dimensions의 다양한 중요성을 허용하는 변형된 MF가 된다.
- 그리고 $a_{out}$에 비선형 함수를 사용하면, 선형 MF 모델보다 표현력이 좋은 비선형 설정으로 MF를 일반화할 것이다.

본 논문에서는 $a_{out}$에 시그모이드 함수를 사용하고 $h$를 학습하는 NCF 프레임워크를 기반으로 일반화된 버전의 MF를 구현한다. 그리고 이것을 GMF라고 명명한다.

$$
\begin{align*}
a_{out}=1/(1 + e^{-x}), \ \  h^{T}=[h_{1},...,h_{k}]
\end{align*} 
\\
$$

## ⅲ. MLP(Multi-Layer Perceptron)

NCF는 모델 users와 items에 대한 두 가지 경로를 채택하기 때문에, 경로들을 concatenate해서 두 경로의 특징을 결합하는 것이 직관적이다.
- 이러한 설계는 multimodal deep learning 작업에서 널리 채택되어 왔다.

단순한 vector concatenation은 user와 item latent features 사이의 상호 작용을 설명하지 않는다. 그리고 이는 CF 효과를 모델링하기에 불충분한다.  
따라서 concatenated vector에 hidden layers를 추가하는 것을 제안한다. 이것은 MLP를 활용하는 것으로 user와 item latent features사이의 상호 작용을 학습한다.
- 이를 통해 고정된 element-wise product를 사용하는 GMF보다, $\boldsymbol{p}_u$와  $\boldsymbol{q}_i$사이의 상호 작용을 학습할 수 있는 큰 수준의 확장성과 비선형성을 부여할 수 있다.

**NCF: MLP Formula**
    
$$
\begin{align*}
z_{1} &= \phi_{1}(P^{T}v_{u}^{U}, Q^{T}v_{i}^{I}) = \begin{bmatrix} 
P^{T}v_{u}^{U}  \\
Q^{T}v_{i}^{I}  
\end{bmatrix} \\
z_{2} &= \phi_{2}(z_{1}) = a_{2}(W_{2}^{T}z_{1} + b_{2}) \\
		\cdots \\
z_{L} &= \phi_{L}(z_{L-1}) = a_{L}(W_{L}^{T}z_{L-1} + b_{L}) \\
\hat{y}_{u,i} &= \sigma(h^{T}\phi_{L}(z_{L-1}))

\end{align*}
\\
$$
    
- $x$번째 layer의 perceptron에 대해서 $W_x$는 wieght matrix, $\boldsymbol{b}_x$는 bias vector, $a_x$는 활성 함수를 나타낸다.
- 활성 함수는 sigmoid, tanh, ReLU 등 자유롭게 선택할 수 있는데, 본 논문에서는 sigmoid와 tanh의 saturation단점으로 인해 ReLU를 채택한다.
	- ReLU는 sparse data에 대해 잘 적응하고 모델이 overfitting될 가능성을 낮추면서 sparse activations를 장려한다.

Network 구조를 설계에서는 Figure 2처럼 맨 아래 층이 가장 넓고, 연속되는 각 층마다 뉴런 수가 적은 tower pattern을 따르는 것이 일반적인 해결칙이다.
	- 이와 같은 전제는 상위 계층에 대해 소수의 hidden units을 사용함으로써 데이터의 추상적인 특징을 더 많이 배울 수 있다는 것이다.

## ⅳ. Fusion of GMF and MLP

지금까지 선형 kernel을 적용해 latent feature 상호 작용을 모델링하는 GMF와 데이터로 부터 상호 작용 함수를 학습시키기 위해 비선형 kernel을 사용한 MLP를 살펴보았다.
이 두 모델 GMF와 MLP를 통합하여, 두 모델이 가진 각자의 장점을 살리고 단점을 보완하는 방법은 동일한 embedding layer를 공유하고 그들의 상호 작용 함수의 outputs을 결합하는 것이다.
- 이 방법은 잘 알려진 NTN(Neural Tensor Network)와 유사하다.
- 구체적으로, GMF를 단일 레이어 MLP와 결합하는 모델은 다음과 같이 공식화될 수 있다.
    
	$$
	\hat{y}_{ui}=\sigma(\boldsymbol{h}^Ta(\boldsymbol{p}_u \odot \boldsymbol{q}_i + W\left[ \begin{matrix} \boldsymbol{p}_u \\    \boldsymbol{q}_i \\ \end{matrix} \right]+b))
	$$
    
하지만, GMF와 MLP의 embedding을 공유하는 것은 통합된 모델의 성능을 제한시킨다.
- 공유한다면, GMF와 MLP는 같은 크기의 embedding을 가지게 된다.  이로 인해 두 모델의 최적 embedding 크기가 많이 다른 dataset의 경우, 최적의 앙상블을 얻지 못할 수 있다.
- 통합된 모델에 더 많은 확장성을 제공하기 위해 GMF와 MLP가 별도의 embedding을 학습하고 마지막 숨겨진 계층을 연결하여 두 모델을 결합한다.

![](4.jpg)
_Figure 3: Neural matrix factorization model_

Figure 3은 앞에서 언급한 내용을 설명한다. 그리고 이에 대한 식은 아래와 같다.
    
$$
\begin{align*}
\phi^{GMF} &= \boldsymbol{p}_{u}^{G} \odot \boldsymbol{q}_{i}^{G} \\
\phi^{MLP} &= a_{L}(W_{L}^{T}(a_{L-1}(...a_{2}(W_{2}^{T} \begin{bmatrix} 
p_{u}^{M}  \\
q_{i}^{M} 
\end{bmatrix}+b_{2})...))+b_{L}) \\
\hat{y}_{u,i} &= \sigma(h^{T} \begin{bmatrix} 
\phi^{GMF}  \\
\phi^{MLP}
\end{bmatrix})
\end{align*}
$$
    
- 여기서 $\boldsymbol{p}_u^G$와 $\boldsymbol{p}_u^M$은 각각 GMF와 MLP의 user latent vector이다. 서로 다른 embedding layer를 사용한다는 의미는 두 벡터의 차원이 다를 수 있다는 것이다. 
  마찬가지로 $\boldsymbol{q}_i^G$와 $\boldsymbol{q}_i^M$은 각각 GMF와 MLP의 item latent vector를 나타낸다.

각 모델에서 나온 output인 $\phi^{GMF}$와 $\phi^{MLP}$를 concatenation하여 $h$로 가중치를 줘 최종추정치를 구하게 된다. 이 모델은 user-item간의 상호 관계를 표현하기 위해 MF의 linearity 와 MLP의 non-linearity를 결합한 것이 특징이며 본 논문에서는 NeuMF(Neural Matrix Factorization)라고 부른다.

- **Pre-training**

  NeuMF는 non-convexity하기 때문에 gradient로 locally-optimal 해를 찾아야한다. DL(Deep Learning) 모델에서 수렴성과 성능을 위해 가중치 초기화가 중요하다.
  - NeuMF는 GMF와 MLP의 앙상블이기 때문에, 본 논문에서는 GMF와 MLP의 pre-trained 모델로 NeuMF의 가중치들을 초기화할 것을 제안한다.

  본 논문에서는 먼저 랜덤하게 초기화한 GMF와 MLP를 수렴할 때가지 학습시켰다. 그리고 그들의 parameters를 NeuMF의 초기 값으로 사용했다.
  - 하나의 차이점은 두 모델의 가중치를 연결하는 output layer에 있다.
      
  $$
  \boldsymbol{h} \leftarrow \left[ \begin{matrix} \alpha\boldsymbol{h}^{GMF} \\ (1-\alpha)\boldsymbol{h}^{MLP} \end{matrix} \right]
  $$
      
  - $\boldsymbol{h}^{GMF}$와 $\boldsymbol{h}^{MLP}$는 각각 pre-trained GMF와 MLP의 $\boldsymbol{h}$ vector를 의미한다. 그리고 $\alpha$는 두 pre-trained 모델 사이의 trade-off를 결정하는 hyper-parameter이다.

  GMF와 MLP를 처음 훈련시킬 때, Adam(Adaptive Moment Estimation)을 사용했다.
  - Adam은  자주 업데이트 된 parameter에 대해 작은 업데이트를 수행하고 별로 업데이트 되지 않은 parameter에 대해 큰 업데이트를 수행하도록 learning rate를 조정한다.
  - Adam은 vanilla SGD보다 더 빠르게 수렴하였다.

  NeuMF에서는 pre-trained된 두 모델의 parameter를 feed하고 vanilla SGD를 사용해 최적화를 하였다.
  - ADM은 parameter를 적절히 업데이트하기 위해서는 momentum을 저장해야 한다.
  - Pre-trained 모델 paramter만으로 NeuMF를 초기화하면서 momentum 정보를 저장하는 것을 포기하기 때문에 momentum 기반 방법론으로 NeuMF를 최적화하는 것은 적합하지 않다.

# References

1. [Paper code](https://github.com/hexiangnan/neural_collaborative_filtering){:target="_blank"}
2. [[논문 리뷰] Neural Collaborative Filtering](https://leehyejin91.github.io/post-ncf/){:target="_blank"}
3. [Pairwise / Triplet Loss](https://wwiiiii.tistory.com/entry/Pairwise-Triplet-Loss){:target="_blank"}
4. [Meaning of latent features?](https://datascience.stackexchange.com/questions/749/meaning-of-latent-features){:target="_blank"}
5. [자카드 지수](https://ko.wikipedia.org/wiki/%EC%9E%90%EC%B9%B4%EB%93%9C_%EC%A7%80%EC%88%98){:target="_blank"}
6. [Collaborative Filtering using Deep Neural Networks (in Tensorflow)](https://medium.com/@victorkohler/collaborative-filtering-using-deep-neural-networks-in-tensorflow-96e5d41a39a1){:target="_blank"}

[1]: https://dl.acm.org/doi/abs/10.1145/3038912.3052569
[2]: https://github.com/hexiangnan/neural_collaborative_filtering
[3]: https://github.com/c0natus/Paper-review-implements/tree/main/RecSys/NCF
[4]: https://www.sciencedirect.com/science/article/abs/pii/0893608089900208