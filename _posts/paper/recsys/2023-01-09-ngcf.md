---
title: "Neural Graph Collaborative Filtering, (SIGIR'19)"
categories: [Paper, RecSys]
tags: [GNN, Collaborative Filtering, BPR]
img_path: /assets/img/posts/paper/recsys/ngcf/
author: gshan
math: true
---

|[Paper Link][1]{:target="_blank"}|[Official Github][2]{:target="_blank"}|[Implementation][3]{:target="_blank"}|

# Abstract

추천 시스템의 핵심은 user와 item의 embeddings를 학습하는 것이다. 
기존의 matrix factorization, deep learning 방법론들은 ID, attributes 등의 feature를 이용해 embedding을 학습했다.
본 논문에서는 collaborative filtering 효과를 더 높이기 위해, user-item interactions을 추가적으로 활용한 embedding 학습법을 제시한다.
즉, 기존의 방법들은 embedding 학습에 collaborative signal(latent in user-item interactions)을 간접적으로 사용했지만, 본 논문에서는 직접적으로 사용한다.

# Ⅰ. Introduction

개인화된 추천의 핵심은 user의 과거 이력을 바탕으로 user가 선택할만한 item을 추정하는 것이다.
Collaborative filtering(CF)은 성향이 비슷한 user끼리 items에 대한 유사한 선호도를 보인다는 것을 전제로 추천 시스템을 구축한다.

일반적으로 CF를 학습시킬 때 2가지 주요한 요소(embedding, interaction modeling)가 존재한다. 
Embedding은 users와 items를 vectorized representations로 변환하는 것을 의미하고,
interaction modeling은 embeddings를 기반으로 과거 interactions을 재구성하는 것을 의미한다.
- [MF][4]{:target="_blank"}는 ID로 user/item embedding을 만들고, inner product로 user-item interaction을 modeling한다.
- [NeuMF][5]{:target="_blank"}는 MF의 interaction modeling인 inner product를 nonlinear neural networks로 대체한다.

위와 같이, 기존의 방법들의 embedding function은 users(or items)의 행동적 유사도를 나타내는 collaborative signal을 **직접적으로** 사용하지 않는다.
따라서 CF를 위한 만족스러운 embeddings를 만들기에는 불충분한 요소가 있다.
구체적으로 말하자면, embedding function은 descriptive feature(ID와 attributes)만을 활용하고 user-item interactions는 objective function을 정의할 때만 사용된다.

본 논문에서는 user-item interactions를 embedding function에 통합하기 위해 <span class='text-color-yellow'>high-order connectivity</span>를 활용한다.

![](1.jpg)
_Figure 1: An illustration of the user-item interaction graph and the high-order connectivity. The node $u_1$ is the target user to provide recommendations for._

**Running Example.** Figure 1은 $u_1$에 대한 high-order connectivity를 보여준다.
High-order connectivity는 어떠한 node에서 $u_1$에 도달할 수 있는, 1보다 큰 $\ell$ 길이의 path를 의미한다.
이러한 high-order connectivity는 collaborative signal를 전달하는 풍부한 semantics를 포함한다.
- $u_1 \leftarrow i_2 \leftarrow u_2$는 $u_1$과 $u_2$의 행동이 유사하다는 것을 나타낸다.
- $\ell = 3$을 봤을 때,  <$i_4, u_1$>를 연결하는 path가 <$i_5, u_1$>보다 많기 때문에 user $u_1$는 item $i_5$보다 $i_4$를 더 선호한다.

**Present Work.** High-order-connectivity위해, tree로 interaction graph를 확장하는 것은 구현하기 어렵다. 
그래서 저자들은 embeddings를 recursive하게 propagate하는 neural network method를 설계했다.
정확히 말하자면, 상호작용한 items(or users)를 aggregate해서 a user(or an item)의 embedding을 개선하는 
<span class="text-color-yellow">embedding propagation layer</span>를 고안했다.

# Ⅱ. Methodology

![](2.jpg){: w="500"}
_Figure 2: An illustration of NGCF model architecture (the arrowed lines present the flow of information). The representations of user $u_1$ (left) and item $i_4$ (right) are refined with multiple embedding propagation layers, whose outputs are concatenated to make the final prediction._

NGCF에는 3가지 구성 요소가 있다. 
1. embedding layer: user/item embeddings를 제공 및 초기화한다.
2. multiple embedding propagation layers: high-order connectivity 관계로 embeddings를 개선한다.
3. prediction layer: propagation layers로부터 개선된 embeddings를 aggregate하고 user-item 쌍의 선호도를 계산한다.

## ⅰ. Embedding Layer

User와 item의 embedding vector를 $e_u \in \mathbb{R}^d, e_i \in \mathbb{R}^d$라 하자. 이때 $d$는 embedding size를 의미한다.
이는 아래와 같이 embedding look-up table로 parameter matrix를 구축하는 것으로 볼 수 있다.

$$
\begin{equation}
\mathbb{E} = [\underbrace{e_{u_1}, \cdots, e_{u_N}}_{\text{user embeddings}}, \underbrace{e_{i_1}, \cdots, e_{u_M}}_{\text{item embeddings}}]
\end{equation}
$$

Embedding table은 end-to-end 방식으로 최적화되는 user, item embeddings의 초기 상태에 불과하다.
기존의 MF, NCF같은 model은 해당 ID embeddings를 바로 interaction layer로 넘겼다.
반면, NGCF에서는 user-item interaction graph에 propagatie하여 embeddings를 개선한다.
이는 collaborative signal을 embeddings에 직접적으로 주입시키기 때문에, 더 효과적인 embeddings를 얻을 수 있다.

## ⅱ. Embedding Propagation Layers

Graph structure에서 CF signal로 user/item embeddings를 개선하기 위해, GNNs의 message-passing 구조에 기반한 embedding propagation layers를 구축한다.
Embedding propagation layer의 이점은 관련된 user, item representations의 정보를 직접적으로 활용한다는 것이다.
먼저 first-order propagation을 살펴본 뒤, high-order propagation으로 first-order를 일반화 한다.

### ⅱ.ⅰ. First-order Propagation

직관적으로 user가 상호작용한 items은 user의 선호에 직접적으로 영향을 끼친다.
유사하게, item을 소비한 users은 item의 feature로 생각될 수 있고 두 items의 collaborative 유사성을 측정하는 데 사용될 수 있다.
이 사실에 기반해, 2개의 주된 operations:*message construction*과 *message aggregation*로 embedding propgation을 수식화 한다.

**Message Construction.** 연결된 user-item pair($u, i$)에 대해, $i$에서 $u$로의 message를 다음과 같이 정의한다.

$$
\begin{equation}
m_{u \leftarrow i} = f(e_i, e_u, p_{ui})
\end{equation}
$$

- $m_{u \leftarrow i}$: message embedding
- $f(\cdot)$: message encoding function

Embeddings $e_u, e_i$는 input이고 coefficient $p_{ui}$는 edge($u, i$)의 propagation을 통제하기 위한 decay factor이다.
저자들은 $f(\cdot)$을 아래와 같이 정의한다.

$$
\begin{equation}
m_{u \leftarrow i} = \frac{1}{\sqrt{|\mathcal{N}_u||\mathcal{N}_i|}} \big( W_1e_i + W_2(e_i \odot e_u) \big)
\end{equation}
$$

- $W_1, W_2 \in \mathbb{R}^{d' \times d}$: propagation을 위해 유용한 information을 distill하는 weight matrices.
- $e_i \odot e_u$: element-wise product of $e_i, e_u$

오직 $e_i$만 고려하는 기존의 GCNs와 다르게 여기서는 $e_i \odot e_u$를 통해 $e_i, e_u$ 사이의 interaction을 encode한다.
이는 message가 $e_i, e_u$ 사이의 친밀함(affinity)에 의존하도록 한다. 
즉, user와 유사한 items는 message를 통해 더 많이 propagate된다.

GCN과 같이, $p_ui$는 graph Laplacian norm인 $1/\sqrt{\|\mathcal{N}_u\|\|\mathcal{N}_i\|}$로 정의한다.
- Representation learning 관점으로, $p_{ui}$는 historical item이 user 선호도에 얼마나 기여하는지를 나타낸다.
- Message passing 관점으로, $p_{ui}$는 propagate되는 message가 path의 length에 따라 decay되도록 하는 discount factor로 해석될 수 있다.

**Message Aggregation.** $u$의 representation을 개선하기 위해 $u$의 이웃으로부터 propagate된 message를 aggregate한다.

$$
\begin{equation}
e_u^{(1)} = \text{LeakyReLU}\big( m_{u \leftarrow u} + \sum_{i \in \mathcal{N}_u} m_{u \leftarrow i} \big)
\end{equation}
$$

- $m_{u \leftarrow u} = W_1e_u$: self-connection of $u$
- $e_u^{(1)}$: first embedding propagation layer 이우헤 얻어진 user $u$의 representation을 의미

이웃($\mathcal{N}_u$)으로부터 propagate된 message와 더불어, 원래의 information을 유지하는 self-connection를 고려한다.

### ⅱ.ⅱ. High-order Propagation

High-order connectivity information을 활용하기 위해 embedding propagation layers를 더 쌓을 수 있다.
이는 user, item 사이의 관련성 점수를 추정하는 데 중요하다.
$\ell$ embedding propagation layers를 쌓음으로써, user(item)은 그것의 $\ell$-hop 이웃으로부터 전파된 messages를 받을 수 있다.
User $u$의 representation은 아래와 같이 recursive하게 표현할 수 있다.

$$
\begin{equation}
e_u^{(\ell)} = \text{LeakyReLU}\big( m_{u \leftarrow u}^{(\ell)} + \sum_{i \in \mathcal{N}_u} m_{u \leftarrow i}^{(\ell)} \big)
\end{equation}
$$

propagate된 messages는 아래와 같의 정의된다.

$$
\begin{equation}
\begin{cases}
    m_{u \leftarrow i}^{(\ell)} = p_{ui}\big( W_1^{(\ell)}e_i^{(\ell-1)} + W_2^{(\ell)}(e_i^{(\ell-1)} \odot e_u^{(\ell -1)}) \big)
    \\\\
    m_{u \leftarrow u}^{(\ell)} = W_1^{(\ell)}e_u^{(\ell -1)}
\end{cases}
\end{equation}
$$

**Propagation Rule in Matrix Form** 
저자들은 전체적인 embedding propagation을 살펴보고 batch implementation을 사용하기 위해, layer-wise propagation rule의 matrix form을 제공한다.

$$
\begin{equation}
E^{(\ell)} = \text{LeakyReLU}\big( (\mathcal{L}+I)E^{(\ell - 1)}W_1^{(\ell)} + \mathcal{L}E^{(\ell - 1)} \odot E^{(\ell - 1)}W_2^{(\ell)} \big)
\end{equation}
$$

- $E^{(\ell)} \in \mathbb{R}^{(N+M) \times d_{\ell}}$: $\ell$ step의 embedding propagation 이후 users, items의 representations.
- $\mathcal{L}$: user-item graph의 Laplacian matrix
    - 0이 아닌 entry $\mathcal{L}_{ui} = 1/\sqrt{\|\mathcal{N}_u\|\|\mathcal{N}_i\|}$이다.
    - 이는 equation 3의 $p_{ui}$와 같다.

$$
\begin{equation}
\mathcal{L} = D^{-\frac{1}{2}}AD^{-\frac{1}{2}} \text{ and } A 
  = \begin{bmatrix}
    0 & R \\
    R^{\top} & 0
  \end{bmatrix}
\end{equation}
$$

- $R \in \mathbb{R}^{N \times M}$: user-item interaction matrix
- A: adjacency matrix
- D: diagonal degree matrix
    - $t$ 번째 diagonal element $D_{tt} = \| \mathcal{N}_t \|$이다.

Matrix-form propagation rule를 통해 효과적으로 모든 users, items의 representations을 동시에 update할 수 있다. 
이것은 large-scale graph에서 GCN을 구축할 때 사용되는 node sampling 과정을 없애준다.

## ⅲ. Model Prediction

$L$ layers의 propagate이후 user $u$에 대해 multiple representations {$e_u^{(1)}, \cdots, e_u^{(L)}$}을 얻을 수 있다.
서로 다른 layers에서 얻어진 representations는 user 선호도를 반영하는 데 서로 다른 기여를 한다.
따라서 user $u$에 대한 마지막 embedding을 구성하기 위해 그들을 concatenate한다. 
Item $i$도 마찬가지로 $L$ layers의 propagate이후 얻어진 모든 embedding을 concatenate한다.

$$
\begin{equation}
e_u^* = e_u^{(0)}|| \cdots ||e_u^{(L)}, \ \ \ e_i^* = e_i^{(0)}|| \cdots ||e_i^{(L)}
\end{equation}
$$

Concatenation 외에도 weighted average, max pooling, LSTM 등의 aggregators를 사용할 수 있다. 
각각의 aggregators는 connectivies를 결합하는 데 있어서 서로 다른 가정을 내포하고 있다.
Concatenation의 장점은 simplicity이고 GNNs에서 효과적으로 사용되고 있다.

Target item $i$에 대한 user $u$의 선호도는 inner product로 추정한다.

$$
\begin{equation}
\hat{y}_{\text{NGCF}(u, i)} = e_u^{*^{\top}}e_i^*
\end{equation}
$$

Inner product가 아닌 neural network-based interaction functions도 사용될 수 있다.

## ⅳ. Optimization

Model parameter 학습을 위해 pairwise BPR loss를 사용한다.
Objective function은 다음과 같다.

$$
\begin{equation}
\text{Loss} = \sum_{(u,i,j)\in \mathcal{O}} -\text{ln}\sigma(\hat{y}_{ui}-\hat{y}_{uj}) + \lambda||\Theta||^2_2
\end{equation}
$$

$$
\begin{align*}
\mathcal{O}&: \{(u,i,j)\|(u,i) \in \mathbb{R}^+, (u, j) \in \mathbb{R}^-\} \\
\sigma&: \text{ sigmoid function} \\
\Theta&: \{E,\{ W_1^{(\ell)}, W_2^{(\ell)} \}^L_{\ell=1}\}
\end{align*}
$$

Optimizer는 Adam을 사용한다. 
특히, random하게 smaple된 triplets $(u,i,j) \in \mathcal{O}$의 경우 $L$ step의 propagation 이후 loss function의 gradient를 사용해 model parameter를 update한다.

### ⅳ.ⅰ. Message and Node Dropout

Deep learning이 representation ability가 강하지만, 보통 overfitting에 고통받는다.
이를 완화하는 효과적인 방법으로 dropout이 있다. 기존 GCN을 참고해 저자들은 2개의 dropout technique를 제시한다.

- Message Dropout: outgoing messages를 random하게 drop한다. 구체적으로 말하자면, equation 6에서 propagate된 message를 drop out한다.
- Node Dropout: random하게 특정 node를 block하고 그것의 모든 outgoing messages를 버린다.

---

Space/time complexity, 실험 등을 추가로 살펴보고 싶다면 논문을 참고하자.


[1]: https://dl.acm.org/doi/pdf/10.1145/3331184.3331267
[2]: https://github.com/xiangwang1223/neural_graph_collaborative_filtering
[3]: https://github.com/c0natus/Paper-review-implements/tree/main/RecSys/NGCF
[4]: https://c0natus.github.io/posts/mf/
[5]: https://c0natus.github.io/posts/neumf/