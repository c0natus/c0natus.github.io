---
title: "Diffusion Recommender Model, (SIGIR'23)"
categories: [Paper, RecSys]
tags: [Generative Recommender Model, Diffusion Model, Latent and Temporal Diffusion Recommender Models]
img_path: /assets/img/posts/paper/recsys/diffrec/
author: gshan
math: true
---

|[Paper Link][1]{:target="_blank"}|[Official Github][2]{:target="_blank"}|

## Abstract

Challenges
: 1. 기존의 생성 model: GAN, VAE는 여러 가지 문제를 가지고 있다.
2. 엄청 많은 items에 대해 user가 interaction할 확률을 구해야 하는 것은 cost가 많이 든다.
3. User의 선호도 변화를 고려하기 위해 temporal 정보를 활용해야 한다.

Address
: 1. Diffusion을 사용하겠다.
2. Latent DiffuRec을 제안한다.
3. Temporal DiffuRec을 제안한다.


## 1. Introduction

GANs, VAEs 같은 생성 models이 추천 시스템에 많이 활용된다.
이러한 생성 models은 users의 interactions는 latent factor(e.g., user preference)에 의해 결정된다고 가정하고 있다.
하지만 GANs, VAEs는 각각 장/단점이 있다.

GANs는 학습이 불안정(e.g., mode collapse)하고, VAEs는 tractability 때문에 representation ability를 희생하고 있다.
즉, VAEs에서 복잡한 representation을 학습하는 능력이 GANs보다 떨어진다.
Diffusion models (DMs)는 tractable한 forward process로 image를 점진적으로 noise하게 만들어 tractability와 representation ability의 trade-off 관계를 완화한다.
이러한 forward process는 tractable posterior를 가질 수 있도록, 그리고 복잡한 distributions을 가지는 data를 modeling할 수 있도록 만든다.

> DMs는 VAEs와 같이 likelihood-based 생성 model이다.

![](1.png)

User interactions엔 보통 noisy(e.g., 실수로 클릭, popularity item 클릭 등)를 포함되어 있다.
Noisy interaction으로 부터 추천을 진행해야 하기 때문에, DMs로 denosing을 진행하는 것은 resonable하다.
VAEs에 비해 representation ability가 강하기 때문에 복잡한 user interaction을 더 잘 생성할 수 있고, GANs보다 학습이 안정적이다.

그래서 저자들은 **Diff**usion **Rec**ommender Model을 제안한다.
주의할 점은 personalized information은 보존해야 하기 때문에 image에서의 diffusion과 다르게 pure noise로 corrupt하는 것을 피해야 한다.

또한 저자들은, large-scale에서 모든 item을 추천하기 위한 Latent DiffRec (L-DiffRec)과 user의 변화하는 선호도를 반영하기 위해 Temporal Diffrec (T-DiffRec)을 제안한다.

## 2. Preliminary

Diffusion에 관한 내용으로 생략한다.

![](2.png)

## 3. Diffusion Recommender Model

![](3.png)

Image에서의 diffusion과 같이 Gaussian noises를 step by step으로 준다.
그리고 reverse process에서 noise를 점진적으로 denoise하는 것을 학습하고, interaction probabilities를 예측한다.

### 3.1. Forward and Reverse processes

Item set $I$에 대한 user $u$의 interaction history을 $x_u$라고 한다.
$x_u = \big[x_u^1, \cdots, x_u^{\|I\|}\big], x_u^i = 0 \text{ or } 1$이다.
그리고 forward process의 input인 initial state $x_0$로 $x_u$를 사용한다.
간단한 표기를 위해 $x_0$에 $_u$를 생략했다.

![](4.png)

![](5.png)

Gaussian noise로 corrupt를 하는 수식으로 DDPM과 같다.

$$
\beta_t \in (0,1), \ \ \ \alpha_t = 1 - \beta_t, \ \ \ \bar{\alpha}_t = \prod_{t^\prime=1}^t \alpha_{t^\prime}, \ \ \ t \in [1, T]\\
x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1 - \bar{\alpha}_t}\epsilon, \ \ \ \epsilon \sim \mathcal{N}(0, I)
$$ 

본 논문에서는 $\beta_t$대신 $1 - \bar{\alpha}_t$에 대한 linear noise schedule을 설계한다.

![](6.png)

$s \in [0, 1]$은 noise scales로 hyper-parameter이다.
$\alpha_\text{min} < \alpha_\text{max} \in (0,1)$도 hyper parameter로 더해지는 noise의 lower, upper bounds이다.

Reverse process도 DDPM과 같다.

![](7.png)

$\mu_\theta$와 $\Sigma_\theta$을 학습(예측)해야 한다.

### 3.2. DiffRec Training

![](8.png)

DiffRec은 ELBO를 최대화 해야 한다.
Eq. 1에서 prior matching은 constant이기 때문에 생략한다.


<span class="text-color-blue">**Denoising matching term**</span>은 KL divergence를 통해 $p_\theta(\cdot)$를 tractable 분포인 $q(\cdot)$과 유사해지도록 만든다.
$q(\cdot)$은 eq. 2, 3과 Bayes rules를 사용하면 아래와 같이 closed form으로 쓸 수 있다.

![](9.png)

학습 안정성과 계산을 간단하게 하기 위해 $\Sigma_\theta$를 학습하는 대신 DDPM처럼 $\sigma^2(t)I$를 사용한다.

> 정확히 말하자면, DDPM에서는 $\Sigma_\theta$대신 $\beta_t$를 사용했는데, 본 논문에선 $\tilde{\beta}_t = \sigma^2(t)I$를 사용한다.
> DDPM에서는 둘의 성능이 유사하다고 했다.

그러면 step t에서 denoising matching term은 아래와 같이 계산된다.

![](10.png)

$\mu_\theta$를 $\tilde{\mu}$와 유사하게 만들어야 한다.
Eq. 8에서 $x_0$를 model이 학습한 $\hat{x}_0$로 대체하면, 아래와 같이 표현 가능하다.

![](11.png)

Eq. 8, 9를 eq. 10에 대입하면 아래와 같이 된다.

![](12.png)

요약하자면, denoising matching terms을 추정하기 위해 $\hat{x}_\theta$를 neural network로 구현해서 eq. 11을 계산해야 한다.
저자들은 MultiVAE와 유사하게 AutoEncoder를 사용한다.

<span class="text-color-blue">**Reconstruction term**</span>은 아래와 같다.

![](13.png)

저자들은 MultVAE에 나와있듯이 unweighted $-\|\|\hat{x}_\theta(x_1, 1) - x_0\|\|^2_2$로 Gaussian log-likelihood $\text{log }p(x_0\|x_1)$을 추정한다.

> MultVAE와 같이 multinomial을 사용할 수도 있을 것 같다.

따라서 <span class="text-color-blue">**Optimization**</span>은 ELBO를 최대화하기 위해 eq. 11, 12에서 살펴본 loss를 minimize해서 최적의 $\hat{x}_\theta(x_t, t)$를 학습해야 한다.

![](14.png)

최종적인 학습 algorithm은 아래와 같다.

![](15.png)

본 논문에서는 diffusion의 성능을 올리기 위해 importance sampling을 한다.
<span class="text-color-blue">**Importance sampling**</span>이란, 각 step마다 학습 어려움 정도가 다르기 때문에 loss가 큰 step을 더 자주 sampling해야 한다는 것이다.

![](16.png)

- $p_t \propto \sqrt{\mathbb{E}[L_t^2]} \big/ \sqrt{\sum_{t^\prime}\mathbb{E}[L_{t^\prime}^2]}$

$p_t$는 sampling하는 확률값으로 $\sum_tp_t=1$이다.
저자들은 expectation으로 총 10개에 대한 $L_t$ 평균값을 사용한다.
각 t마다 10개의 값이 모이지 않았다면 uniform sampling을 한다.
직관적으로 말하자면, $L_t$가 크면 sampling을 많이 한다는 뜻이다.
그리고 단순히 sampling을 많이 해서 loss의 값이 커지는 것을 방지하기 위해 eq. 14에서 $p_t$를 나눠주고 있다.

### 3.3. DiffRec Inference

![](17.png)

User interactions을 pure noises로 만드는 것은 추천에서 user 선호도의 개인화를 버리는 결과를 낸다. (논문의 4.3.2. 실험을 참고하자.)
그래서 저자들은 간단한 inference 전략을 제안한다.

먼저, user interaction에는 이미 noise한 값 (false-positive/negative)가 있기 때문에, 적은 수의 step $T^\prime < T$만큼 forward를 진행한 다음, $T$ step 만큼 reverse denoising을 한다.
그리고 MultiVAE와 같이 reverse denoising에서 training과 inference 때 variance를 주지 않는다.
즉, deterministic하게 eq. 10에서 예측한 평균값을 사용한다.


### 3.4. Discussion

Computer vision과 다른 추천 task가 가지는 2가지 point를 살펴보자.

<span class="text-color-blue">**Personalized recommendation.**</span>
Training할 동안, user의 personalized characteristics를 잃지 않기 위해 noise를 적게 줘야 한다.
이는 MultVAE에서 $\beta$로 KL divergence를 적게 고려하는 것과 같은 원리이다.
구현 상으로는 noise schedule에서 noise scale s와 $\alpha_\text{max}$를 작게 주면서 noise를 줄였다.
그리고 inference 시에는 natural noises를 고려해 forward step $T^\prime$을 적게 준다.

<span class="text-color-blue">**$x_0$-ELBO.**</span>
Diffrec에서는 noise $\epsilon$을 예측하는 대신 $x_0$를 예측한다.
추천의 목표는 item ranking을 매기기위해 $\hat{x}_0$를 예측해야 한다.
그렇기 때문에 추천에서는 $x_0$-ELBO를 사용하는 것이 직관적으로 더 적절한다.
그리고 $\epsilon \sim \mathcal{N}(0,I)$는 불안정하며, MLP가 이러한 noise를 추정하도록 하는 것은 어렵다.

> Image도 마찬가지로 결국 $x_0$를 예측하는 건데, 뭔가 와닿지가 않는다.
> 납득할 만한 것은 image는 UNet 구조와 attention을 활용하기 때문에 noise를 더 잘 추정할 수 있기 때문이려나?
> 여기선 간단한 MLP (AutoEncoder)를 사용해서 어려운 것 같다....?
> AutoEncoder는 복구는 잘 하니까...?

### 3.5. Latent Diffusion: L-DiffRec

MultVAE나 DiffRec은 모든 item에 대한 $\hat{x}_0$ 확률을 예측해야 한다.
Large-scale item prediction에서는 intractable하다.
Cost를 줄이기 위해 저자들은 L-DiffRec을 제안한다.
핵심은 clustering을 통해 connect 되는 수, 즉 parameter 수를 줄여서 학습/추론 속도를 빠르게 하는 것이다.

> 산업에서는 Re-rank model을 사용하고 있는데, 굳이 필요한가?

![](18.png)

먼저, LightGCN으로 학습된 item embedding을 활용해 clustering을 진행한다.
그리고 각 cluster 마다 서로 다른 VAE encoder를 적용해 latent vectors를 만든다.
만들어진 latent vectors를 concate해서 $z_0$를 얻어 DiffRec과 유사한 과정으로 $\hat{z}_0$를 예측하는 latent diffusion을 수행한다.
그리고 예측된 $\hat{z}_0$ 각 cluster에 해당하는 VAE decoder를 사용해 user interactions를 예측한다.

Encoder와 decoder의 <span class="text-color-blue">**Training**</span>은 MultVAE와 유사하게 진행된다.

![](19.png)

- $\gamma$는 MultVAE에서 $\beta$ annealing하는 부분이다.

그리고 VAE loss와 diffusion loss를 합쳐서 L-DiffRec을 최적화한다.

$$
\mathcal{L}_v(x_0,\phi,\psi) + \lambda \cdot \mathcal{L}(z_0, \theta)
$$

- $\lambda$는 hyper-parameter로 두 항의 크기를 동일하도록 보장해야 한다.

> 이게 뭔 말이지? 크기를 동일하게 만든다라...

<span class="text-color-blue">**Inference**</span>에서는 이전과 DiffuRec과 마찬가지로 deterministic하게 만들기 위해, reverse process와 VAE decoder에 variance를 주지 않는다.

### 3.6. Temporal Diffusion: T-DiffRec

User의 선호도는 시간에 따라 달라지기 때문에 temporal information을 capture하는 것이 중요하다.
최근 interactions이 user의 현재 선호도를 더 잘 표현한다는 가정 하에, user의 최신 interactions에 더 큰 weight를 주는 time-aware reweighting 전략을 제안한다.

User의 interaction sequence를 $\mathcal{S}$라 하자.

$$
\mathcal{S} = \{i_1, \cdot, i_M\}
$$

저자들은 weight $w = [w_1, \cdots, w_M]$을 다음과 같은 time-aware linear schedule로 정의한다.

$$
w_m = w_\text{min} + \frac{m-1}{M-1}(w_\text{max} - w_\text{min})
$$

- $w_\text{min} < w_\text{max} \in (0, 1]$인 hyper-parameter이다.

구해진 weight를 가지고 interaction $x_0$를 $\bar{x}_0 = x_0 \odot \bar{w}$로 나타낸다.
원소로 0 또는 1의 값만 가지는 $x_0$가 weight를 반영한 실수 값을 가지는 $\bar{x}_0$로 표현된다.

## 4. Experiments

논문 참고.

[1]: https://arxiv.org/pdf/2304.04971.pdf
[2]: https://github.com/YiyanXu/DiffRec
