---
title: "Factorization Machines, (ICDM'10)"
categories: [Paper, RecSys]
tags: [CARS, Factorization Model, SVM]
img_path: /assets/img/posts/paper/recsys/fm/
author: gshan
math: true
---

|[Paper Link][1]{:target="_blank"}

<!-- |[Implementation][2]{:target="_blank"}| -->

# Abstract

Factorization Machines (FM)은 Support Vector Machines (SVM)과 factorization models의 장점을 결합한 것이다.
FM은 처음으로 context-aware recommender system (CARS)를 일반적인 supervised learning으로 해결한 방법론이다.

# Ⅰ. Introduction

Collaborative filtering 환경에서 data는 매우 sparse하기 때문에, SVM은 큰 성능을 내지 못한다.
즉, complex한 kernal sapce에서 신뢰할 수 있는 parameter(hyperplanes)를 학습할 수 없다.
MF, SVD++ 같은 factorization model은 좋은 성능을 내지만, real valued feature vector같은 일반적인 data(ex. [0.2, 0.4, 0, 0.3, 0.1])에 대한 prediction(general prediction)을 할 수 없다. 
즉, [1, 0, 0, 0, 0] 같이 제한된 형태의 vector만 prediction할 수 있다.

본 논문에서는 <span class="text-color-yellow">SVM처럼 general prediction을 할 수 있으면서 MF처럼 매우 sparse한 data에서도 parameter를 잘 학습할 수 있는 Factorization Machine (FM)을 제안</span>한다.
General prediction이 가능하다는 것은 추천 시스템 외에도 사용가능한 model이라는 뜻이다.
FM은 polynomial kernel SVM처럼 한 data 내에서 각 feature간 모든 interactions을 modeling한다.
이때, SVM에서 사용하는 dense parameterization $W$ 대신 factorized parameterization $V$($W = VV^{\top}$)을 사용한다.
FM은 linear time complexity를 가지고, SVM/MF 등의 일반화된 형태이다.

# Ⅱ. Prediction Under Sparsity

$\mathbf{x}^{(d)} \in \mathbb{R}^n$을 feature vector, $m(\mathbf{x})$를 $\mathbf{x}$의 non-zero element 개수, $\overline{m}_D$를 모든 $\mathbf{x} \in D$에 대한 $m(\mathbf{x})$의 평균 값이라고 하자.
본 논문에서는 매우 sparse한 data $\mathbf{x}$($\overline{m}_D \ll n$)를 다룬다.
즉, feature vector $\mathbf{x}$의 element $\mathbf{x}_i$ 대부분이 0이다.
이처럼 data가 huge sparsity를 가지는 이유는 category의 수가 크기 때문이다.

![](1.jpg)
_Fig. 1. Example for sparse real valued feature vectors x that are created from the transactions of example 1. Every row represents a feature vector $\mathbf{x}^{(i)}$ with its corresponding target $y^{(i)}$. The first 4 columns (blue) represent indicator variables for the active user; the next 5 (red) indicator variables for the active item. The next 5 columns (yellow) hold additional implicit indicators (i.e. other movies the user has rated). One feature (green) represents the time in months. The last 5 columns (brown) have indicators for the last movie the user has rated before the active one. The rightmost column is the target here the rating._

영화 review data를 예로 들자. user $u \in U$는 특정 시간 $t \in \mathbb{R}$에 한 영화(item) $i \in I$의 평점 $r \in \{1, 2, 3, 4, 5\}$을 매겼다.
그러면 관찰된 data $S$는 다음과 같다.

$$
S = \{(\mathbf{x}^{(1)}, y^{(1)}), \dots, (\mathbf{x}^{(d)}, y^{(d)}), \dots, (\mathbf{x}^{(D)}, y^{(D)}) \}
$$

이때, $\mathbf{x}^{(d)}$는 Fig 1과 같이 user, item, user가 평점을 매긴 items, time, user가 마지막으로 평점을 매긴 item으로 이뤄져 있다.
- User가 평점을 매긴 items는 normalize되어 있다. 즉 합해서 1이다. rating한 것이 많으면 summation 값이 올라가니 납득이 된다.
- Time은 2009년 1월을 기준으로 표현한 것이다. 즉, 2009년 1월을 1, 2010년 1월을 13으로 표현한다.

# Ⅲ. Factorization Machines

해당 section에선 factorization machines를 소개한다. 

# ⅰ. Model

$$
\begin{equation}
\widehat{y} := w_0 + \sum_{i=1}^n w_ix_i + \sum_{i=1}^n \sum_{j=1}^n <\mathbf{v}_i, \mathbf{v}_j>x_ix_j
\end{equation}
$$

model parameter의 차원은 다음과 같다.

$$
\begin{equation}
w_0 \in \mathbb{R}, \mathbf{w} \in \mathbb{R}^n, V \in \mathbb{R}^{n \times k}
\end{equation}
$$

그리고 <$\cdot, \cdot$>은 dot product를 나타낸다.

$$
\begin{equation}
<\mathbf{v}_i, \mathbf{v}_j> := \sum_{f=1}^k v_{i,f} \cdot v_{j,f}
\end{equation}
$$

$\mathbf{v}_i$는 matrix $V$의 $i$번째 row를 나타낸다. $k \in \mathbb{N}^+_0$은 hyperparameter로 factorization의 dimension을 의미한다.


2-way FM(degree d=2)는 $\mathbf{x}^{(d)}$의 모든 single feature 그리고 feature 간 pairwise interactions을 학습한다.
- $w_0$: global bias
- $w_i$: $i$번째 (single) feature의 strength
- $\hat{w}_{i,j}:=<\mathbf{v}_i, \mathbf{v}_j>$: $i, j$번째 feature간 pairwise interactions

FM은 각 iteraction에 적용된 parameter($w_{i,j}$)를 학습하는 대신 그것의 factorization($\mathbf{v}_i, \mathbf{v}_j$)을 학습한다.
이것은 sparse한 data에서 좋은 higher-order interactions ($d \geq 2$)의 parameter를 추정하기 위한 필수 요소이다.

## ⅱ. Expressiveness

Positive definite matrix $W$에 대해, $k$가 충분히 크다면 $W = V \cdot V^{\top}$인 matrix $V \in \mathbb{R}^{n \times k}$가 존재한다.
즉, $k$가 충분히 크다면 FM이 모든 interaction matrix $W$를 표현할 수 있다는 것을 의미한다.

Data가 sparse할 때 복잡한 interaction $W$를 추정할 data가 충분하지 않아 일반적으로 작은 $k$를 선택한다.
$k$를 제한하여 FM의 표현력을 제한하면 더 나은 일반화가 가능하므로 sparse setting에서 interaction matrices가 개선됩니다.

## ⅲ. Computation

모든 pairwise interactions를 계산해야 하기 때문에 Eq. 1의 time complexity는 $O(kn^2)$이다.
이때, pairwise interactions이 factorize되어 있기 때문에 두 변수가 직접적으로 종속된 parameter는 없다.

> 두 변수에 종속된 parameter인 $w_{ij}$가 $\mathbf{v}_i \cdot \mathbf{v}_j$로 factorize된다.

따라서 Eq. 1가 linear time $O(kn)$으로 계산되도록 식을 변형할 수 있다.

$$
\begin{split}
	&\sum_{i=1}^n\sum_{j=i+1}^n <\mathbf{v}_i, \mathbf{v}_j>x_ix_j\\
	=&\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n<\mathbf{v}_i, \mathbf{v}_j>x_ix_j - \frac{1}{2}\sum_{i=1}^n<\mathbf{v}_i, \mathbf{v}_j>x_ix_j\\
	=&\frac{1}{2}\bigg( 
			\sum_{i=1}^n\sum_{j=1}^n\sum_{f=1}^kv_{i,f}v_{j,f}x_ix_j
			- \sum_{i=1}^n\sum_{f=1}^kv_{i,f}v_{i,f}x_ix_i
		\bigg)\\
	=&\frac{1}{2}\sum_{f=1}^k\bigg(
			\bigg(
				\sum_{i=1}^nv_{i,f}x_i
			\bigg)
			\bigg(
				\sum_{j=1}^nv_{j,f}x_j
			\bigg)
			- \sum_{i=1}^nv_{i,f}^2x_i^2
		\bigg)\\
	=&\frac{1}{2}\sum_{f=1}^k\bigg(
			\bigg(
				\sum_{i=1}^nv_{i,f}x_i
			\bigg)^2
			- \sum_{i=1}^nv_{i,f}^2x_i^2
		\bigg)  
\end{split}
$$

그리고 sparse dataset에서 대부분의 $x_i$가 0이므로 더 적은 시간이 걸린다.
$\overline{m}_D$를 feature vector $\mathbf{x}$의 평균 non-zero element의 수라고 했을 때, time complexity는 $O(k\overline{m}_D)$이 된다.

## ⅳ. $d$-way Factorization Machine

**$d$-way FM**

$$
\begin{equation}
\hat{y}(x) 
	:= w_0 
	+ \sum_{i=1}^nw_ix_i 
	+ \sum_{l=2}^d\sum_{i_1=1}^n\cdots\sum_{i_l=i_{l-1}+1}^n\bigg(
			\prod_{j=1}^lx_{i_j}
		\bigg)
		\bigg(
			\sum_{f=1}^{k_l}\prod_{j=1}^lv_{i_j, f}^{(l)}
		\bigg)
\end{equation}
$$

**2-way FM**

$$
\begin{split}
\hat{y}(x) 
	&:= w_0 
	+ \sum_{i=1}^nw_ix_i\\
	&+ \sum_{i_1=1}^n\sum_{i_2=i_1+1}^nx_{i_1}x_{i_2}
		\sum_{f=1}^{k_2}v_{i_1,f}^{(2)}v_{i_2,f}^{(2)}
\end{split}
$$

**3-way FM**

$$
\begin{split}
\hat{y}(x) 
	&:= w_0 
	+ \sum_{i=1}^nw_ix_i\\
	&+ \sum_{i_1=1}^n\sum_{i_2=i_1+1}^nx_{i_1}x_{i_2}
		\sum_{f=1}^{k_2}v_{i_1,f}^{(2)}v_{i_2,f}^{(2)}\\
	&+ \sum_{i_1=1}^n\sum_{i_2=i_1+1}^n\sum_{i_3=i_2+1}^nx_{i_1}x_{i_2}x_{i_3}
		\sum_{f=1}^{k_3}v_{i_1,f}^{(3)}v_{i_2,f}^{(3)}v_{i_3,f}^{(3)}
\end{split}
$$

[1]: https://ieeexplore.ieee.org/abstract/document/5694074