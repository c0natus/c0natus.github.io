---
title: "Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding, (WSDM'18)"
categories: [Paper, RecSys]
tags: [CNN, Sequential Prediction]
img_path: /assets/img/posts/paper/recsys/caser/
author: gshan
math: true
---

|[Paper Link][1]{:target="_blank"}|[Official Github][2]{:target="_blank"}|[Implementation][3]{:target="_blank"}|

# Abstract

Top-N sequential recommendation은 과거에 interaction이 있었던 item의 sequence로 각 user를 modeling한다.
Interaction sequence에서 최근 item일수록 다음 item에 더 큰 영향을 끼친다.
본 논문에서는 이런 sequence의 문제를 해결하기 위해 *Caser*(Convolutional Sequence Embedding Recommendation Model)을 제안한다.
- 최근 item들의 sequence를 시간과 latent space의 'image'로 embedding한다. 그리고 convolutional filter를 활용해 image의 local feature를 학습하는 것처럼 item들의 순차적 패턴을 학습한다.

이 접근 방법은 일반적인 선호도(general preferences)와 순차적 패턴(sequential patterns) 두 가지 모두를 파악할 수 있는 통일되고(unified) 유연한 network 구조를 제공한다.

# Ⅰ. Introduction

대부분의 top-N 추천 시스템은 items의 최신 상태를 고려하지 않은 채 user의 일반적인 선호도 기반으로 item을 추천한다.
일반적인 선호도는 Apple 제품보단 Samsung 제품을 더 좋아하는 등 user의 장기적(long term)이고 정적(static)인 행동을 나타낸다.

User의 행동 중 또 다른 유형은 순차적 패턴으로 user가 다음에 선택할 item 또는 actions은 user가 최근에 상호 작용한 items또는 actions에 의존한다.
순차적 패턴은 user의 단기적이고 동적인 행동을 나타낸다. 
그리고 이것은 가까운 시간 내 item들 사이의 특정 관계에서 비롯된다.
User가 갤럭시를 구매했을 때, 휴대폰 액세서리를 곧 바로 구매할 것이다. 
하지만 휴대폰 액세서리는 user의 일반적인 선호도가 아니다.

## ⅰ. Top-N Sequential Recommendation

가까운 시일 내에 user가 상호 작용할 N개의 items를 추천하는 top-N sequential recommendation을 고려해 user의 순차적 패턴을 모델링해야 한다.
이 문제를 해결하기 위해 몇 가지 집합을 가정해야 한다.
- a set of users: $\mathcal{U} = {u_1, u_2, \cdots, u_{\|\mathcal{U}\|}}$
- a universe of items: $\mathcal{I} = {i_1, \cdots, i_{\|\mathcal{I}\|}}$
- a sequence of some items from $\mathcal{I}$ for each user $u$: $\mathcal{S}^u = (\mathcal{S}^u_1, \cdots, \mathcal{S}^u_{\|\mathcal{S}^u\|}) \ \ \ \text{where } \mathcal{S}^u_i \in \mathcal{I}$
    - $\mathcal{S}^u_t$에서 index $t$는 절대 시간이 아니라 $\mathcal{S}^u$에서 action이 일어난 순서이다.

모든 user의 $\mathcal{S}^u$가 주어졌을 때, 각 user의 needs를 최대한 만족 시키는 item의 리스트를 추천 해야 한다.
전통적인 top-N 추천은 item의 set을 user의 행동으로 모델링 하고, top-N sequential 추천은 item의 sequence로 user의 행동을 모델링 한다.

## ⅱ. Limitations of Previous Work

Top-N sequential 추천의 초기 방법론은 과거 L개의 action에 기반한  L-order Markov chain이었다.
- First-order Markov chain은 MLE를 사용해 학습한 item간 전이 행렬이다.
- FPMC(Factorized personalized Markov chains)와 그것의 변형은 전이 행렬의 low-rank를 가지는 두 개의 latent sub-matrices로 분해함으로써 Markov chain 방법론을 향상 시켰다.
- Fossil(Factorized Sequential Prediction with Item Similarity Models)은 과거 items의 latent 표현에 weighted sum aggregation을 사용해 FPMC 방법론을 high-order Markov chain으로 일반화하였다.

![](1.jpg)
_Figure 1: An example of point and union level dynamic pattern influences, the order of Markov chain $L$ = 3_

**Fail to model union-Level sequential patterns.**
Fig. 1 (a)에서 볼 수 있듯이 Markov chain은 오직 point-level 순차적 패턴을 모델링한다.
Point-level 이란 과거 actions(파랑)이 target action(노랑)에 총괄적으로 영향을 끼치는 것이 아니라 개인적으로 영향을 끼치는 것이다.

Fossil이 high-order Markov chain을 고려한다. 
하지만, 그것의 전반적인 영향력은 first-order Markov transition matrices에서 분해된 이전 item의 latent 표현력의 weighted sum이다.
Point-level의 weighted sum 영향력은 이전의 actions의 sequence를 고려해 target action에 공동으로 영향력을 주는 Fig. 1 (b)의 union-level 영향력을 모델링하기에는 불충분하다.

**Fail to allow skip behaviors.**
현재 존재하는 모델들은 Fig. 1 (c)에서 보이는 것처럼 순차적 패턴의 skip behaviors를 고려하지 않는다.
조금의 step을 건너뛰어도 과거의 행동들이 여전히 target action에 강력하게 영향을 끼친다.
예를 들어, 비행기와 호텔, 식당, 술집, 관광지 순으로 예약한 여행자가 있다고 하자. 비행기와 호텔을 예약한 것이 바로 관광지를 예약하는 것으로 이어지지 않지만, 그들은 강력하게 연관이 되어있다. 식당과 술집을 예약한 것은 비교석 관광지 예약으로 이어지지 않는다.
L-order Markov chain은 이러한 skip behaviors를 명시적으로 모델링하지 않는다.

**Evidences of union-level influences and skip behaviors.**
Union-level 영향력과 skip behaviors의 증거를 제공하기 위해, 실제 dataset인 MovieLens와 Gowalla에서 아래 형태의 순차석 연관 규칙들을 찾았다.

$$
\begin{equation}
(\mathcal{S}_{t-L}^u, \cdots, \mathcal{S}_{t-1}^u) \rightarrow \mathcal{S}_{t}^u 
\end{equation}
$$

위와 같은 형식의 $X \rightarrow Y$ 규칙에서 support count인 $sup(XY)$는 규칙에서와 같이 $X, Y$ sequence 대로 발생하는 수열의 수이고, 신뢰도인 $\frac{sup(XY)}{sup(X)}$는 $X$ 순으로 발생하는 수열이 존재할 때 $Y$가 $X$ 다음으로 발생하는 수열의 백분율이다.
이 규칙은 X의 모든 항목이 Y에 미치는 공동 영향을 나타낸다.
오른쪽 term을 $\mathcal{S}_{t+1}^u, \cdots$  등으로 바꾸면서 skip behavior에 관한 규칙도 찾을 수 있다.

![](2.jpg)
_Figure 2: The number of association rules vs $L$ and skip steps. The minimum support count = 5 and the minimum confidence = 50%._

Fig. 2는 L-order Markov과 skip behaviors를 비교하며 최소 support count = 5, 최소 신뢰도 = 50%인 규칙들의 수를 나타낸다.
위에서 찾은 대부분의 규칙의 L은 2와 3이며, L이 클수록 규칙의 신뢰도가 높아진다.
또한 Fig. 2는 많은 수의 규칙이 1 또는 2단계를 건너뛴다는 것을 나타낸다. 
이러한 연구 결과는 union-level의 영향(no skip)과 skip behaviors의 존재를 뒷받침한다.

## ⅲ. Contributions

이전에 살펴본 제한들을 해결하기 위해, 본 논문에서는 Caser(*ConvolutionAl Sequence Embedding Recommendation Model*)을 제안한다.
Caser는 이전의 $L$개의 items를 $L \times d$ matrix $\boldsymbol{E}$(각 행은 item의 순서이고, $d$는 latent 차원)로 표현한다. 
그리고 이 embedding matrix를 latent space에서 L items의 'image'로 간주하고 다양한 convolution filter를 사용해 local feature로서 순차적 패턴을 탐색한다.
- 이 '이미지'는 입력으로 주어지지 않으며, filter처럼 학습되어야 한다. 즉, embedding matrix 만드는 것을 학습해야 한다.

**Caser's several distinct advantages.**

1. Caser는 수평 및 수직 convlutional filters를 사용해 point-level, union-level, skip behaviors의 순차적 패턴을 찾는다.
2. Case는 사용자의 일반 선호도와 순차적 패턴을 모두 모델링하고, 단일 통합 프레임워크에서 몇 가지 기존 SOTA 방법론들을 일반화한다.
3. Case는 실제 dataset에서 top-N sequential recommendation에 대한 SOTA를 능가한다. 

# Ⅱ. Further Related Work

CF, MF, top-N 추천과 같은 전통적인 추천 방법론들은  순차적 패턴을 찾기에 적합하지 않다.
- 전통적인 방법론들은 통계적 동시 발생에서 비롯된 명시적 순차 연관 규칙을 찾는다. 이것은 패턴의 명시적  표현에만 의존하기 때문에 관찰되지 않은 패턴을 놓칠 수 있다.

RBM(Restricted Bolzmann Machine), Auto-encoder 프레임워크, CNN 등의 방법론은 DL을 사용해 추천 시스템의 성능을 높혔지만, sequential recommendation에 적합하지 않다.
RNN은 세션 기반 추천에 사용되었다. RNN은 sequence를 모델링하는 데 뛰어난 능력을 가지고 있지만, sequential recommendation 환경에서는 효과적이지 않다.
- Sequential Recommendation에서는 인접한 actions 중 일부는 의존도가 없기 때문이다.
- RNN 기반 방법은 dataset에 상당한 순차적 패턴이 포함되어 있을 때 성능이 더 우수하다.

Caser는 RNN처럼 순차적 패턴을 인접 action으로 모델링하지 않지만, CNN의 convolution filter를 선택함으로써 이전 item embedding의 local feature의 순차적 패턴을 모델링한다.
비슷하지만 다른 문제는 temporal recommendation이다.
- 예로 들어, temporal recommendation은 저녁보단 아침에 커피를 추천한다.
- Caser는 시간과는 독립적으로 휴대폰을 구입한 다음 휴대폰 액세서리를 추천한다.
- 이것은 분명히 다른 문제이므로 다른 해결법이 필요하다.

# Ⅲ. Proposed Methodology

Caser는 sequential features를 학습하기 위해 CNN을, user 별 feature를 학습하기 위해 LFM(Latent Factor Model)을 포함하고 있다.
Caser의 목적은 여러가지(multi-fold)이다.
- Union-level과 point-level 모두에서 user의 일반 선호도와 순차적 패턴을 찾는 것이다.
- 관찰되지 않은 모든 공간에서 skip behaviors를 찾는 것이다.
    

![](3.jpg)
_Figure 3: The network architecture of Caser. The rectangular boxes represent items $S^u$ in user sequence, whereas a rectangular box with circles inside stands for a certain vector e.g., user embedding $P_u$. The dash rectangular boxes are convolutional filters with different sizes. The red circles in convolutional layers stand for the max values in each of the convolution results. Here we are using previous 4 actions ($L$ = 4) to predict which items this user will interact with in next 2 steps ($T$ = 2)._

Fig. 3은 Caser가 3가지 요소로 구성되었다는 것을 보여준다.
- Embedding Look-up, Convolutional Layers, Fully-connected Layers

CNN을 학습하기 위해 figure 3의 왼쪽에 표시된 user의 sequence $\mathcal{S}^u$에서 모든 연속된 $L$ 개의 items를 입력으로 추출하고, 다음 items $T$ 개를 target으로 추출한다.
- 이것은 user의 sequence에 대해 크기가 $L+T$인 sliding window를 적용시키고, 각 window는 triplet($u$, 이전 $L$ 개의 items, 다음 $T$ 개의 items)으로 표시되는 user $u$에 대한 훈련 instance를 생성한다.

## ⅰ. Embedding Look-up

Caser는 이전 $L$개의 items의 embeddings를 NN에 주입함으로써 latent space에 있는 sequence feature를 찾는다.
- Item의 embedding인 $\mathcal{Q}_i \in \mathbb{R}^d$는 해당 item의 latent factors와 유사한 개념이다. 이 때 $d$는 latent 차원의 수이다.

Embedding look-up layer에서는 이전 $L$개의 item embedding을 탐색하고 그것들을 모아서 user $u$와 time step $t$에 대한 matrix $\boldsymbol{E}^{(u,t)} \in \mathbb{R}^{L \times d}$를 생성한다.

$$
\begin{equation}
\boldsymbol{E}^{(u,t)} =
\begin{bmatrix}

\mathcal{Q}_{\mathcal{S^u_{t-L}}} \\
\vdots 
\\
\mathcal{Q}_{\mathcal{S^u_{t-2}}} \\
\mathcal{Q}_{\mathcal{S^u_{t-1}}}
\end{bmatrix}
\end{equation}
$$

Item embeddings와 함께, user $u$의 feature를 latent space로 표현하는 embeding vector $p_u \in \mathbb{R}^d$도 있다.

## ⅱ. Convolution Layers

Caser는 $L \times d$ matrix $\boldsymbol{E}$를 latnet space에 있는 이전 $L$ 개의 items의 'image'로 간주한다. 그리고 순차적 패턴을 해당 'image'의 local feature로 생각한다.

![](4.jpg)
_Figure 4: Darker colors mean larger values. The first filter captures '(Airport, Hotel) → GreatWall' by interacting with the embedding of airport and hotel and skipping that of fast food and restaurant. The second filter captures '(Fast Food, Restaurant) → Bar'._

Figure 4는 두 개의 union-level의 순차적 패턴을 찾는, 두 'horizontal filters'의 예시를 보여준다.
- $h \times d$ matrices로 표현되는 filters는 height가 2이고 width가 $d$이다. 이 filters는 $\boldsymbol{E}$의 행을 sliding하면서 순차적 패턴을 위한 signals를 선택한다.
- 첫 번째 filter는 Airport와 Hotel이 큰 값을 가지는 latent dimensions에서 같은 양상으로 큰 값을 가지는 '(Airport, Hotel) → GreatWall' signal을 선택한다.
    - Figure 3에서 max pooling을 한다.

유사하게 'vertical filter'는 $L \times 1$ matrix이고 $\boldsymbol{E}$의 열을 sliding한다.
Horizontal과 vertical filter에 대한 자세한 내용은 조금 이따 살펴보자.
모든 items $i$의 embedding $\mathcal{Q}_i$는 filters와 동시에 학습되기 때문에 'image' $\boldsymbol{E}$는 모델의 입력이 아니다.

<span class="text-color-bold">**Horizontal Convolution Layer**</span>

이 layer는 Figure 3에서 살펴보았 듯이, $n$ 개의 horizontal filters $F^k \in \mathbb{R}^{h \times d}, \ 1 \le k \le n, \ h \in \{1, \cdots, L\}$를 가진다.
예를 들어, $L=4, T=2$일 때 $h \in \{1,2,3,4\}$인 두 개의 filter, 총 $n = 8$ 개의 filter를 선택할 수 있다.
$F^k$는 $1\le i \le L-h+1$(no padding)인 items $i$에 대한 $\boldsymbol{E}$의 행을 위에서 아래로 sliding하면서 모든 horizontal dimensions와 상호 작용한다. 상호 작용의 결과는 $i$ 번째 convolution 값인데 수식으로 나타내면 아래와 같다.
    
$$
\begin{equation}
c_i^k = \phi_c(\boldsymbol{E}_{i:i+h-1} \cdot F^k)\end{equation}
$$

- $\cdot$ 은 내적(CNN의 convolution 연산)을 뜻하고, $\phi_c$는 convolution layer의 활성 함수를 뜻한다.
- $c^k_i$는 $F^k$와 $\boldsymbol{E}$의 $i$ ~ $i-h+1$ 행으로 구성된 sub matrix($\boldsymbol{E}_{i:i+h-1}$)의 내적이다.

$F^k$의 convolution의 최종 결과 vector $c^k \in \mathbb{R}^{L - h + 1}$는 아래와 같다.
    
$$
\begin{equation}
c^k=\begin{bmatrix}
c^k_1 \ c^k_2 \ \cdots \ c^k_{L-h+1}
\end{bmatrix}\end{equation}
$$
    
최종 결과 $c^k$에 max pooling 연산을 적용해 특정 filter로 생성된 값들 중 최대 값을 추출한다. 이 최대 값을 해당 filter로 추출한 가장 중요한 feature라고 할 수 있다.
그러므로 horizontal convolution layer의 $n$ 개의 filters의 결과 값은 아래와 같다. vector $o \in \mathbb{R}^n$
    
$$
\begin{equation}o = \{max(c^1), \cdots, max(c^n)\}\end{equation}
$$
    
Horizontal filters는 embedding $\boldsymbol{E}$를 통해 모든 연속적인 $h$ 개의 items와 상호 작용한다.
최소화 해야 할 목적식은 Section 3.4에서 살펴보자.
Heights를 다양하게 함으로써 위치와 상관 없이 중요한 signal을 선택할 수 있다. 이것은 horizontal filters가 다양한 union 크기를 가진 union-level 패턴을 찾을 수 있도록 학습 된다는 것을 의미한다.

<span class="text-color-bold">**Vertical Convolutional Layer**</span>

이 layer는 Figure 3에서 살펴보았볼 수 있이며,  horizontal layer와 구분하기 위해 tilde(~) 기호를 사용한다.
$\tilde{n}$ 개의 vertical filters $\tilde{F}^k \in \mathbb{R}^{L \times 1}, \ 1 \le k \le \tilde{n}$이 있다고 가정하자. 각 filter $\tilde{F}^k$는 $\boldsymbol{E}$의 열을 왼쪽에서 오른쪽으로 $d$번 sliding 하면서 상호 작용한다.
$\tilde{F}^k$의 최종 결과 vector $\tilde{c}^k \in \mathbb{R}^d$는 아래와 같다.
    
$$
\begin{equation}\tilde{c}^k=\begin{bmatrix}
\tilde{c}^k_1 \ \tilde{c}^k_2 \ \cdots \ \tilde{c}^k_{d}
\end{bmatrix}\end{equation}
$$
    
내적 상호 작용의 경우, $\tilde{F}^k$를 weight로 하는 $\boldsymbol{E}$의 $L$행에 대한 weighted sum과 같다. 그러므로 vertical filters는 이전 $L$ 개의 items의 embedding을 집계(aggregate)하는 것을 학습한다.
    
$$
\begin{equation}\tilde{c}^k = \sum\limits_{l=1}^L\tilde{F}^k_l \cdot E_l\end{equation}
$$

- 이것은 Fossil의 이전 $L$ 개의 item에 대한 latent 표현을 집계하기 위한 weighted sum과 유사하다. 차이점은 vertical convolutional layer에서는 $\tilde{F}^k$가 서로 다른 aggregator처럼 동작한다는 것이다.
> 의문점  
> 이부분이 잘 이해가 되지 않는다. $\tilde{c}^k$의 결과는 $\mathbb{R}^d$인 vector가 나와야 하는데, 수식의 결과는 scalar가 아닌가???
>
> 이것은 vertical filter를 Fossil에서 사용할 수 있는 일반화된 형태인 것을 말해준다. 즉, vertical filter의 결과인 $\tilde{c}^k \in \mathbb{R}^d$의 모든 원소($d$ 개)를 더하면, Fossil에서 weighted sum을 해준 것과 같은 효과를 낸다.
            
따라서 Fossil과 유사하게 이 vertical filters는 이전 item의 latent 표현에 대한 weighted sums를 통해 point-level 순차적 패턴을 찾을 수 있다.
- Fossil은 하나의 weighted sum을 각 user에게 사용하는 반면, 본 논문에서는  $\tilde{n}$ 개의 global vertical filters를 사용해 모든 user에 대한 $\tilde{n}$ 개의 weightd sums인 vector $\tilde{o} \in \mathbb{R}^{d\tilde{n}}$을 생성한다.
    
  $$
  \begin{equation}\tilde{o} = \begin{bmatrix}
  \tilde{c}^1 \ \tilde{c}^2 \ \cdots \ \tilde{c}^{\tilde{n}}
  \end{bmatrix}\end{equation}
  $$
    
  > 의문점  
  > 위 처럼 표현하면 $\tilde{o} \in \mathbb{R}^{d  \times \tilde{n}}$인 행렬이 아닌가? 표기의 의미가 정확히 뭘까???
  >
  >코드를 살펴보면 $\tilde{c}^k$들을 concatenate해서 $1 \times (\tilde{n} \times \text{latent dimension})$ 크기의 tensor로 만든다.
            
Vertical filters의 용도는 집계(aggregation)이므로 horizontal filter와 몇 가지 차이점이 있다.
- 각 vertical filter의 사이즈는 $L \times 1$로 고정되어 있다. 왜냐하면, $\boldsymbol{E}$의 각 열은 latent이기 때문에 한 번에 연속된 열과 상호 작용 하는 것은 의미가 없다.
> 의문점  
> 그러면 $\tilde{n} = T$ 인가??? 사이즈가 $L \times 1$로 고정된 이유가 와닿지 않는다.
- 모든 latent dimension에 대한 집계를 유지하길 원하므로 vertical convolution 결과들에 대해 max pooling할 필요가 없다.

따라서 이 layer의 output은 $\tilde{o}$이다.

## ⅲ. Fully-connected Layers

본 논문에서는 높은 수준(high-level)이면서 추상적인 features를 얻기 위해, horizontal, vertical convolutional layer의 output을 concatenate하고 fully-connected neural network layer에 feed하였다. 공식을 아래와 같다.
    
$$
\begin{equation}
\begin{split}
z = \phi_a(W\begin{bmatrix}o \\ \tilde{o}\end{bmatrix}+b), \ \ \ where \ W \in& \mathbb{R}^{d \times (n+d\tilde{n})} \\ b \in& \mathbb{R}^d
\end{split}\end{equation}
$$
    
- $W$는 주어진 입력 vector를 $d$ 차원으로 만드는 weight matrix이고, $b$는 bias term, $\phi_a$는 활성함수이다.
- $z \in \mathbb{R}^d$는 *convolutional sequence embedding*이라고 부른다. 즉, 이전 $L$  개의 items의 모든 종류의 sequential features를 encoding한다.

User의 일반 선호도를 찾기위해 user embedding $P_u$를 조회하고, $d$ 차원인 vector $z$와 $P_u$를 concatenate한다. 공식은 아래와 같다.
    
$$
\begin{equation}\begin{split}
y^{(u,t)} = \phi_a(W'\begin{bmatrix}z \\ P_u\end{bmatrix}+b'), \ \ \ where \ W' \in& \mathbb{R}^{|\mathcal{I}| \times 2d} \\ b' \in& \mathbb{R}^{|\mathcal{I}|}
\end{split}\end{equation}
$$

- $W'$은 주어진 입력 vector를 $\|\mathcal{I}\|$차원으로 만드는 weight matrix이고, $b$는 biase term이다.
- $y^{(u,t)}_i$는 time step $t$에 user $u$가 item $i$와 상호 작용할 확률과 관련있다.
- $z$는 단기간의 순차적 패턴을 찾아내고, $p_u$는 user의 일반적 선호도를 찾아낸다.

user  $u$에 대한 embedding vector $P_u$를 마지막 layer에 추가하는 이유는 다음과 같다.
- 다른 모델을 일반화할 수 있게 해준다.
- 모델의 parameter를 다른 일반 모델의 parameter로 pre-train할 수 있게 해준다.

## ⅳ. Network Training

Network를 학습시키기 위해선, $y^{(u,t)}$를 아래와 같이 확률로 변형해야 한다.

$$
\begin{equation}p(\mathcal{S}_t^u|\mathcal{S}_{t-1}^u,\mathcal{S}_{t-2}^u,\cdots,\mathcal{S}_{t-L}^u) = \sigma(y^{(u,t)}_{\mathcal{S}_t^u})\end{equation}
$$

$\sigma$는 sigmoid함수이다.
$\mathcal{C}^u=\{L+1, L+2, \cdots, |\mathcal{S}^u|\}$를 user $u$에 대한 예측인 time steps의 집합이라고 하자.
  - dataset에 있는 모든 sequence의 likelihood는 다음과 같다.
  
  $$
  \begin{equation}p(\mathcal{S}|\Theta)=\prod\limits_u\Big(\prod\limits_{t\in\mathcal{C}^u}\sigma(y_{\mathcal{S}^u_t}^{(u,t)})\prod\limits_{j\ne\mathcal{S}_t^u}(1-\sigma(y_{j}^{(u,t)})) \Big)\end{equation}
  $$
  
Skip behavior를 찾기 위해 $\mathcal{S}^u_{t}$ 을 $\mathcal{D}^u_t = \{\mathcal{S}^u_t, \cdots, \mathcal{S}^u_{t+T} \}$로 대체하여, 한번에  target  $T$ 개의 item을 고려한다. 그리고 negative logarithm of likelihood를 적용해서 *binary cross-entropy* loss로 알려진 목적식을 얻을 수 있다. 공식은 아래와 같다.
    
  $$
  \begin{equation}\mathcal{l} = \sum\limits_u\sum\limits_{t\in\mathcal{C}^u}\Big(\sum\limits_{i\in\mathcal{D}^u_t} -log(\sigma(y_i^{(u,t)})) + \sum\limits_{j\ne i}-log(1-\sigma(y_j^{(u,t)}))\Big)\end{equation}
  $$
    
두 번째 term에 있는 traget item $i$에 대한 negative instance $j$는 랜덤하게 3개를 sampling하였다.
모델의 parameter $\Theta=\{P, \mathcal{Q}, F, \tilde{F}, W, W', b, b'\}$은 식 (13)에서 살펴 본 목적식 $l$을 최소화 하는 방향으로 학습된다.
모델의 hyper parameter $(e.g., \ d, n, \tilde{n}, L, T)$는 validation set를 활용한 grid search로 튜닝된다.
Optimizer로 Adam을 사용하고 batch size는 100이다.
Over fitting을 방지하기 위해 두 가지의 정규화 방법론을 사용했다.
- $L_2$ norm는 모든 모델의 parameter에 적용된다.
- Dropout(50%)는 FC layers에만 적용된다.

## ⅴ. Recommendation

학습이 완료된 모델을 가지고 user $u$에 대해 time step $t$에서의 item을 추천하기 위해, $u$의 latent embedding $P_u$와 식 (2)에서 본 $u$의 과거 $L$ 개 item의 embedding을 NN(Convolution + FC)의 입력으로 준다.
Output layer $y$에서 가장 높은 값을 가지는 item $N$개를 추천한다.
- Traget item의 개수 $\mathcal{T}$는 모델 훈련동안 쓰이는 hyperparameter이고, $N$는 모델 훈련이 완료된 후 추천되는 item의 개수이다.

## ⅵ. Connection to Existing Models
Caser로 기존의 추천 모델 방법론 들을 구현할 수 있다.
- Convolutional layer를 모두 버리면 MF 모델을 표현할 수 있다.
- L을 1로 주면 FPMC 모델을 표현할 수 있다.
- Horizontal convolutional layer를 생략하면 Fossil을 표현할 수 있다.
- $\cdots$ 등

# Ⅳ. Experiments

## ⅰ. Experimental Setup

Sequential recommendation는 data set에 순차적 패턴이 포함되어 있어야 한다.
- 이러한 data set인지 확인하기 위해, sequential association rule mining을 적용했고, 그들의 sequential intensity를 계산했다.
- Sequential intensity는 아래와 같이 정의된다.
    
    $$
    \text{Sequential Intensity (SI)} = \frac{\#\text{ruels}}{\#\text{users}}
    $$
        
- #rules는 식 (1)에서, 예로 들어 support 수가 5이상, confidence가 50% 이상인 $sup(XY)$의 수이고, #users는 총 user의 수이다.
- 특정 data set에 대한 SI 값은 아래와 같다.
    
![](5.jpg)
    
- Amazon data set은 SI 값이 낮아서 사용되지 않았다.

수치로 나타내어진 평점은 전부 implicit feedback인 1로 변경하였다.
cold-start 문제를 해결하기 위해 MovieLens data set에 대해선 한 item에 대해 평가를 한 user의 수가 5미만이면 해당 item을 고려하지 않았다.
전체 dataset 중 70%는 training set, 10%는 validation set으로 정하여 모든 모델의 최적 hyper-parameter를 찾았다. 나머지 20%는 모델의 성능을 측정하기 위해 test set으로 사용되었다.

## ⅱ. Performance Comparison

![](6.jpg)

Dataset 4개에 대한 각 모델의 성능은 위와 같다.  실험한 hyperparameter는 논문을 참고하길 바란다.
Improve를 계산하는 공식은 $\frac{\text{Caser} \ - \ \text{baseline}}{\text{baseline}}$ 이다.

[1]: https://dl.acm.org/doi/abs/10.1145/3159652.3159656
[2]: https://github.com/graytowne/caser
[3]: https://github.com/c0natus/Paper-review-implements/tree/main/RecSys/Caser