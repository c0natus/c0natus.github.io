---
title: "LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation, (SIGIR'20)"
categories: [Paper, RecSys]
tags: [GNN, Collaborative Filtering]
img_path: /assets/img/posts/paper/recsys/lightgcn/
author: gshan
math: true
---

|[Paper Link][1]{:target="_blank"}|[Official Github][2]{:target="_blank"}|

# Abstract

GCN 방법들은 원래 attribute가 풍부한 input에 대한 node/graph classification에 사용되었다.
추천 시스템의 collaborative filtering (CF)는 오직 ID feature만 가지지만, ablation 분석이 충분히 이뤄지지 않았다.
저자들은 GCN에서 사용되는 feature transformation과 nonlinear activation는 CF(collaborative filtering)의 성능에 거의 기여하지 않는다는 것을 경험적으로 발견했다.
심지어, 그것들을 포함시키면 training을 방해해 추천 성능을 떨어뜨린다.

본 논문에서는 CF에 적합한, 오직 GCN의 neighborhood aggregation만 포함하는 LightGCN을 제안한다.
LightGCN은 user-item interaction graph에서 user, item embeddings를 linear하게 propagate하고 모든 layer에서 나온 outputs을 weighted sum한다.

> 논문에서는 간단한 average가 일반적으로 성능이 좋아, simplicity를 위해 average 값을 사용한다.

본 논문에서는 이러한 LightGCN(linearly propagation, weighted sum)의 합리성을 분석적으로, 경험적으로 보여준다.

# Ⅰ. Introduction

추천 시스템의 핵심은 user가 item와 interaction이 있을지 예측하는 것이다.
따라서 과거의 user-item interaction을 활용하는 데 초점을 맞춘 CF는 개인화된 추천을 향한 필수적인 task로 남아 있다.
CF에서 가장 흔한 paradigm은 학습된 user/item의 latent features(embedding)으로 예측을 수행하는 것이다.
Matrix factorization(MF)는 그러한 paradigm으로 동작하는 초기 model이다. MF는 user ID로 embeddings를 mapping한다.
이후 연구(SVD++, NAIS, 등)에서 interaction history를 활용해 augmente된 user ID로 더 좋은 embeddings를 만들 수 있다는 것을 발견했다.
User-item interaction graph 관점에서 SVD++ 등이 interaction history를 이용해 성능을 높인 것은 subgraph(one-hop neighbors)를 활용해 성능을 높였다고 해석할 수 있다.

NGCF는 high-hop neighbors subgraph structure로 CF에서 좋은 성능을 내었다. 
NGCF는 GCN에서 영감을 받아 feature transformation, neighborhood aggregation, nonlinear activation를 포함한 propagation rule을 사용한다.
대부분의 GNN의 방법론을 살펴보면, aggregation function에 feature transformation이나 nonlinear activation을 사용한다.
그리고 semantic input, 즉 attribute가 풍부한 input에 대해 node 또는 graph classification task에서 좋은 성능을 낸다.
GCN도 원래 각 node가 input feature로 풍부한 attribute를 가지는 graph에 사용되었다.
하지만, CF의 user-item interaction graph에서 각 node는 오직 one-hot ID로 묘사된다.
즉, NGCF는 좋은 성능을 내었지만, CF에 적합하지 않은 GCN에서 operations을 justification 없이 가져왔다.
- ID로만 만들어진 embedding으로 많은 layer를 거쳐 nonlinear feature transformation을 하는 것은 model 훈련을 어렵게 만든다. 즉, 수렴을 방해한다.

이를 정당화하기 위해, NGCF에서 많은 ablation studies를 진행했고 <span class="text-color-yellow">GCN의 2개 operation(feature transformation, nonlinear activation)이 NGCF의 효율성에 기여하지 않는다</span>고 결론지었다.
놀랍게도 이들을 제거했을 때 높은 성능 향상을 볼 수 있었다.
이러한 경험적 발견으로 저자들은 필수적인 요소(neighborhood aggregation)만 포함하는 LightGCN을 제안한다.

# Ⅱ. Preliminaries

이번 section에서는 GCN의 operation의 유용성을 파악하기 위해, NGCF에 관한 ablation studies를 진행한다.
[NGCF의 동작 방식][3]{:target="_blank"}은 생략한다.

Semi-supervised node classification에서 각 node는 input으로 title, abstract words 등 풍부한 semantic feature를 가진다.
그래서 multiple layers of nonlinear transformation이 feature learning에 도움을 준다.
하지만, 저자들은 경험적 증거로 node가 ID 정보만 가지는 CF에서는 그렇지 않음을 보인다.

## ⅰ. Empirical Explorations on NGCF

본 논문에서는 nonlinear activation과 feature transformation에 대한 ablation study를 진행했다.
그리고 embedding quality를 더 직설적으로 보여주기 위해 final embedding을 concatenation에서 summation으로 바꿨다.
이는 NGCF에 거의 영향을 주지 않는다고 한다.
- Origin NGCF: $e_u^* = e_u^{(0)}\|\| \cdots \|\|e_u^{(L)}, \ \ \ e_i^* = e_i^{(0)}\|\| \cdots \|\|e_i^{(L)}$
- LightGCN: $e_u^* = e_u^{(0)} +  \cdots + e_u^{(L)}, \ \ \ e_i^* = e_i^{(0)} +  \cdots + e_i^{(L)}$

> 솔직히 summation이 왜 embedding quality를 더 직설적으로 나타내는 지 모르겠다.

![](1.jpg)
_Table 1: Performance of NGCF and its three variants._

- NGCF-f: feature transformation matrices $\mathbf{W}_1, \mathbf{W}_2$ 제거
- NGCF-n: nonlinear activation function $\sigma$ 제거
- NGCF-fn: 둘 다 제거

Table 1은 2-layer setting에 대한 성능이다.
위의 결과로 저자들은 다음을 주장한다.
1. Feature transformation은 NGCF에 안 좋은 영향을 준다.
2. Feature transformation이 있으면 nonlinear activation가 성능을 약간 올리지만, 없으면 NGCF에 안 좋은 영향을 끼친다.
3. 둘 다 동시에 없앴을 때 성능이 올랐기 때문에, feature transformation과 nonlinear activation는 NGCF에 안 좋은 영향을 준다.

![](2.jpg)
_Figure 1: Training curves (training loss and testing recall) of NGCF and its three simplified variants._

Fig. 1를 통해 NGCF-fn의 loss가 NGCF보다 작고, recall@20 성능은 높은 것을 볼 수 있다.
즉, NGCF은 overfitting보단 학습에 어려움을 겪는 것이다.
만약 overfitting이었다면, loss가 작아져도 성능이 오르지 않았을 것이다.

이를 통해, 저자들은 추천 시스템에서 ablation study가 부족하면 불필요한 operation으로 학습이 어려워 질 수 있으니 철저한 ablation study가 중요하다고 주장한다.

# Ⅲ. Method

![](3.jpg)
_Figure 2: An illustration of LightGCN model architecture. In LGC, only the normalized sum of neighbor embeddings is performed towards next layer; other operations like self-connection, feature transformation, and nonlinear activation are all removed, which largely simplifies GCNs. In Layer Combination, we sum over the embeddings at each layer to obtain the final representations._

## ⅰ. LightGCN

<span class="text-color-bold">**Light Graph Convolution (LGC).**</span>
본 논문에서는 간단한 weighted sum aggregator를 사용한다.
LightGCN에서 graph convolution operation(or propagation rule)은 다음과 같이 정의된다.

$$
\begin{split}
\textbf{e}_u^{(k+1)} &= \sum_{i \in \mathcal{N}_u}\frac{1}{\sqrt{\mathcal{N}_u}\sqrt{\mathcal{N}_i}}\textbf{e}_i^{(k+1)} \\
\textbf{e}_i^{(k+1)} &= \sum_{u \in \mathcal{N}_i}\frac{1}{\sqrt{\mathcal{N}_i}\sqrt{\mathcal{N}_u}}\textbf{e}_u^{(k+1)} 
\end{split}
$$

표준 GCN에서 사용한 symmetric normalization term $1/\sqrt{\mathcal{N}_u}\sqrt{\mathcal{N}_i}$을 사용했다.
다양한 normalization term에 대한 실험은 나중에 살펴보자.
위의 aggregation에서 주목할 점은 다른 대부분의 graph convolution과 다르게 연결된 neighbors만 aggregate하고 target node 자신은 하지 않는다는 것이다. 
즉, self-connection이 없다.
사실 LightGCN의 layer combination operation가 self-connections과 같은 효과를 가지기 때문에 self-connection을 포함할 필요가 없다.

<span class="text-color-bold">**Layer Combination and Model Prediction.**</span>
K layers LGC 이후 final representation을 얻기 위해 각 layer에서의 embeddings를 combine해야 한다.

$$
\begin{split}
\textbf{e}_u &= \sum_{k = 0}^K\alpha_k\textbf{e}_u^{(k)}\\
\textbf{e}_i &= \sum_{k = 0}^K\alpha_k\textbf{e}_i^{(k)}\\
\end{split}
$$

$\alpha_k \ge 0$이다. $\alpha_k$는 hyper-parameter로 둘 수 있고, model parameter로 둘 수 있다.
본 논문에서는 불필요하게 model이 복잡해지는 것을 방지하기 위해, 일반적으로 좋은 성능을 내는 간단한 average를 사용한다. 
$\alpha_k = \frac{1}{(K+1)}$

저자들은 마지막 layer의 representation을 사용하지 않고 combine하는 이유를 다음과 같이 말하고 있다.
1. Over-smoothing 문제로 인해 단순히 마지막 layer만 사용하면 문제가 발생 할 수 있다.
2. 다른 layer의 embeddings는 다른 semantics를 capture한다. 그래서 layers를 combine하면 더 포괄적인(comprehensive) representation을 만들 것이다.
3. 앞서 말했듯이, self-connection 효과를 가진다. 왜 그런지는 뒤에서 살펴보자.

Prediction은 final representations의 inner product로 진행된다.

$$
\hat{y}_{ui} = \textbf{e}_u^{\top}\textbf{e}_i
$$

<span class="text-color-bold">**Matrix Form.**</span>
Uer-item interaction matrix는 $\textbf{R} \in \mathbb{R}^{M \times N}$이고, M과 N은 각각 user, item의 수를 나타낸다.
Diagonal matrix는 $\textbf{D} \in \mathbb{R}^{(M+N)\times(M+N)}$이다.
Adjacency matrix와 embedding matrix는 다음과 같다.

$$
\textbf{A} =
\begin{bmatrix}
    0 & R \\
    R^{\top} & 0
\end{bmatrix}, \ \ \ \
\tilde{\textbf{A}} = \textbf{D}^{-1/2}\textbf{A}\textbf{D}^{-1/2}\\
\textbf{E}^{(k+1)} = \tilde{\textbf{A}}\textbf{E}^{(k)}
$$

Final embedding matrix는 다음과 같다.

$$
\begin{split}
\textbf{E} &= \alpha_0\textbf{E}^{(0)} + \alpha_1\textbf{E}^{(1)} + \alpha_2\textbf{E}^{(2)} + \cdots + \alpha_K\textbf{E}^{(K)}\\
&= \alpha_0\textbf{E}^{(0)} + \alpha_1\tilde{\textbf{A}}\textbf{E}^{(0)} + \alpha_2\tilde{\textbf{A}}^2\textbf{E}^{(0)} + \cdots + \alpha_K\tilde{\textbf{A}}^K\textbf{E}^{(0)}\\
\end{split}
$$

## ⅱ. Model Analysis

Simplified GCN(SGCN)을 통해 layer combination의 <span class="text-color-yellow">self-connection 효과</span>를 살펴본다.
그리고 Personalized PageRank로 <span class="text-color-yellow">over-smoothing을 완화</span>한 Approximate Personalized Propagation of Neural Predictions (APPNP)와의 비교를 통해 두 model이 근본적인 동일하다는 것을 살펴본다.
마지막으로 second-layer LGC를 분석해 LightGCN mechanism에 대한 insights를 제공하는 <span class="text-color-yellow">smoothness</span>에 대해 살펴본다.

<span class="text-color-bold">**Relation with SGCN.**</span>
SGCN은 GCN에서 nonlinearity를 없애고 weight matrices를 축소해 하나의 weight matrix만 사용한다.
SGCN의 graph convolution은 다음과 같다.

$$
\textbf{E}^{(k+1)} = (\textbf{D}+\textbf{I})^{-1/2}(\textbf{A}+\textbf{I})(\textbf{D}+\textbf{I})^{-1/2}\textbf{E}^{(k)}
$$

$(\textbf{A}+\textbf{I})$가 self-connections을 의미한다.
Last layer의 embeddings는 다음과 같다.
$(\textbf{D}+\textbf{I})$는 단순 scaling하는 것이므로 간단하게 보여주기 위해 생략한다.

$$
\begin{split}
\textbf{E}^{(K)} &= (\textbf{A}+\textbf{I})\textbf{E}^{(K-1)} = (\textbf{A}+\textbf{I})^K\textbf{E}^{(0)}\\
&= {K \choose 0}\textbf{E}^{(0)} + {K \choose 1}\textbf{A}\textbf{E}^{(0)} + {K \choose 2}\textbf{A}^2\textbf{E}^{(0)} + \cdots + {K \choose K}\textbf{A}^K\textbf{E}^{(0)}\\
\end{split}
$$

${K \choose i} = \alpha_i$로 생각하면 LightGCN과 형태가 같다.

<span class="text-color-bold">**Relation with APPNP.**</span>
APPNP는 GCN와 Personalized PageRank를 결합해 over-smoothing risk없이 high-order neighbor information을 propagate할 수 있다.
Personalized PageRank의 teleport design을 참고해 locality 유지와 high-order neighborhood information을 활용하는 정도의 균현을 맞춘다.

$$
\begin{split}
\textbf{E}^{(k+1)} &= \beta\textbf{E}^{(0)} + (1-\beta)\tilde{\textbf{A}}\textbf{E}^{(k)} \\\\
\textbf{E}^{(K)} &= \beta\textbf{E}^{(0)} + (1-\beta)\tilde{\textbf{A}}\textbf{E}^{(K-1)}\\
&= \beta\textbf{E}^{(0)} 
  + \beta(1-\beta)\tilde{\textbf{A}}\textbf{E}^{(0)} 
  + (1-\beta)^2\tilde{\textbf{A}}^2\textbf{E}^{(K-1)}\\
&= \beta\textbf{E}^{(0)} 
  + \beta(1-\beta)\tilde{\textbf{A}}\textbf{E}^{(0)} 
  + \beta(1-\beta)^2\tilde{\textbf{A}}^2\textbf{E}^{(0)} 
  + \cdots 
  + (1-\beta)^K\tilde{\textbf{A}}^K\textbf{E}^{(K-1)}\\
\end{split}
$$

$\alpha_k$를 적절히 setting하면, APPNP를 사용한 효과를 얻을 수 있다.
즉, LightGCN는 APPNP의 강점을 가진다고 볼 수 있다.

<span class="text-color-bold">**Second-Order Embedding Smoothness.**</span>
LightGCN의 linearity와 simplicity 덕분에 embedding이 어떻게 smooth되는 지에 대한 insight를 얻을 수 있다.
2-layer LightGCN에서 user가 어떻게 smooth되는 지 살펴보자.

$$
\textbf{e}_u^{(2)} = \sum_{i \in \mathcal{N}_u}\frac{1}{\sqrt{|\mathcal{N}_u|}\sqrt{|\mathcal{N}_i|}}\textbf{e}_i^{(1)} = \sum_{i\in\mathcal{N}_u}\frac{1}{|\mathcal{N}_i|}\sum_{v\in\mathcal{N}_i}\frac{1}{\sqrt{|\mathcal{N}_u|}\sqrt{|\mathcal{N}_v|}}\textbf{e}_v^{(0)}
$$

User $u$와 같은 item을 소비한 적이 있는 user를 $v$라고 했을 때, $v$가 $u$에게 영향을 주는 smoothness를 측정한 coefficient는 다음과 같다. 만약 같은 item을 전혀 소비하지 않았다면 coefficient는 0이다.

$$
c_{u \rightarrow u} = \frac{1}{\sqrt{|\mathcal{N}_u|}\sqrt{|\mathcal{N}_v|}}\sum_{i\in\mathcal{N}_u\cap\mathcal{N}_v}\frac{1}{|\mathcal{N}_i|}
$$

식이 복잡하지만 결론만 말하면 두 user가 동시에 소비한 item의 개수가 많을수록 $\sum_{i\in\mathcal{N}_u\cap\mathcal{N}_v}\frac{1}{\|\mathcal{N}_i\|}$ 값이 커지므로 더 많은 영향을 끼친다는 의미다.
좀더 자세히 해석하면 다음과 같다.
1. 동시에 소비한 item이 많을수록 coefficient가 커진다.
2. 동시에 소비한 item의 popularity가 낮을수록 $\|\mathcal{N}_i\|$의 값이 작아지므로 coefficient가 커진다.
3. $v$가 덜 active할수록 $\sqrt{\|\mathcal{N}_v\|}$의 값이 작아지므로 coefficient가 커진다.

위의 해석은 user similarity 측정 관점에서 CF의 assumption을 잘 충족시키고 LightGCN의 합리성(reasonability)을 입증한다. 

Item도 user와 유사한 방식으로 분석할 수 있다.

## ⅲ. Model Training

BPR loss를 활용한다.
자세한 것은 논문을 참고하자.

# Ⅳ. Experiments

Hyper-parameter setting, 다른 model과 비교 등 자세한 것은 논문을 참고하자.

## ⅰ. Ablation and Effectiveness Analyses

Layer combination과 symmetric sqrt normalization에 대해 ablation study를 실행했다.

<span class="text-color-bold">**Impact of Layer Combination.**</span>
LightGCN-single은 layer combination을 사용하지 않고 마지막 layer의 embedding $\mathbf{E}^{(K)}$를 prediction에 사용한 것이다.

![](4.jpg)
_Figure 3: Results of LightGCN and the variant that does not use layer combination (i.e., LightGCN-single) at different layers on Gowalla and Amazon-Book (results on Yelp2018 shows the same trend with Amazon-Book which are omitted for space)._

Fig. 3을 통해 다음을 알 수 있다.
1. LightGCN-single은 layer가 쌓이면 성능이 감소한다. Over-smoothing issue가 있다.
2. LightGCN은 layer가 증가하여도 성능이 감소하지 않는다. APPNP와의 연관성에서 살펴봤듯, layer combination은 over-smoothing을 완화하는데 효과적이다.
3. LightGCN은 Gowalla dataset에서는 LightGCN-single보다 성능이 뛰어났지만, Amazon-Book, Yelp2018 dataset에서는 그렇지 않았다. LightGCN-single은 LightGCN의 특수한 case ($\alpha_K=1$)이므로, $\alpha_i$를 tuning하면 더 좋은 성능을 얻을 수 있을 것이다.

<span class="text-color-bold">**Impact of Symmetric Sqrt Normalization.**</span>
본 논문에서는 symmetric sqrt normalization $\frac{1}{\sqrt{\|\mathcal{N}_u\|}\sqrt{\|\mathcal{N}_i\|}}$를 사용했다. 해당 normalization term의 합리성을 위해 저자들은 left side normalization $\frac{1}{\sqrt{\|\mathcal{N}_u\|}}$과 right side normalization $\frac{1}{\sqrt{\|\mathcal{N}_i\|}}$, $L_1$ normalization $\frac{1}{\|\mathcal{N}_u\|\|\mathcal{N}_i\|}$에 대한 실험을 진행했다. 이때, normalization을 없애면 training이 수치적으로(numerically) 불안정하고, not-a-value (NAN) issue를 가지게 된다.

![](5.jpg)
_Table 2: Performance of the 3-layer LightGCN with different choices of normalization schemes in graph convolution. Method notation: -$L$ means only the left-side norm is used, -$R$ means only the right-side norm is used, and -$L_1$ means the $L_1$ norm is used._

Table 5는 3-layer LightGCN에 대한 성능으로, 논문에서 사용한 both side sqrt normalization이 가장 좋은 성능을 가진다.

<span class="text-color-bold">**Analysis of Embedding Smoothness.**</span>
저자들은 smoothing of embeddings가 LightGCN effectiveness의 주요 요소라고 추측한다.
이를 보여주기 위해 저자들은 다음과 같이 user embedding의 smoothing loss를 정의했다.
유사하게 item embedding의 smoothness도 얻을 수 있다.

$$
S_U = \sum_{u=1}^M\sum_{v=1}^M c_{v\rightarrow u}\bigg( \frac{\textbf{e}_u}{||\textbf{e}_u||^2}-\frac{\textbf{e}_v}{||\textbf{e}_v||^2} \bigg)
$$

- User $u, v$가 동시에 소비한 item이 없으면 $c_{v\rightarrow u} = 0$이다.

![](6.jpg)
_Table 3: Smoothness loss of the embeddings learned by LightGCN and MF (the lower the smoother)._

Table 3은 2-layer LightGCN-single과 matrix factorization의 smoothing을 보여준다.

> Smoothing loss이 높으면 유사한 user/item끼리 가깝다는 것을 의미하므로 추천에 더 적합하다고 말할 수 있다.

[1]: https://dl.acm.org/doi/pdf/10.1145/3397271.3401063
[2]: https://github.com/gusye1234/LightGCN-PyTorch
[3]: https://c0natus.github.io/posts/ngcf/#%E2%85%B1-methodology