---
title: "[CS234] 1. Introduction"
categories: [Course, CS234]
tags: [Introduction, Reinforcement Learning]
img_path: /assets/img/posts/course/cs234/rl-intro/
author: gshan
math: true
---

# Ⅰ. Overview of RL

Reinforcement learning(RL)은 다음의 4가지 요소로 구성된다.

- Optimization
: 결정을 내리는 최적의 방법(최적이 아니라도 좋은 전략)을 찾는다.

- Delayed consequences
: 현재의 결정에 대한 즉각적인 feedback을 얻을 수 없어 그 결정이 좋은지 나쁜지 당장 모르고, 훨씬 이후에 알게 된다. 
이는 credit assignment problem을 야기한다.

- Exploration
: Agent는 경험으로부터 이 세계가 어떻게 동작하는지 학습한다.
여기서 문제는 data가 검열된다는 것이다.
검열된다는 뜻은 오직 시도해본 경험으로부터만 학습이 진행된다는 의미이다.
예를 들어, Stanford에 입학하지 않고 MIT에 입학한 결정에 대한 결과는 학습하지 못한다.


![](1.jpg){: w="500"}
_Figure 1: DeepMind Atari breakout game, 2015_

- Generalization
: Policy는 과거의 경험을 action(decision)으로 mapping한다.
이때, policy를 직접 설정하는 것은 불가능하다.
예를 들어, Fig. 1처럼 100*200 pixel로 이뤄진 벽돌 깨기 게임을 학습한다고 하자. 그러면 $(256^{100 \times 200})^3$ 가지의 상태를 action으로 mapping해야 한다.
이것이 학습과 generalization이 필요한 이유이다.

RL은 세계를 탐색하고(explore) 그 탐색(exploration)을 미래의 결정에 활용한다.

## ⅰ. Different than some other types of AI

|                           |RL |AI Planning|(Un)Supervised Learning|IL |
|---------------------------|:-:|:---------:|:---------------------:|:-:|
|**Optimization**           |o  |o          |                       |o  |
|**Learns from experience** |o  |           |o                      |o  |
|**Generalization**         |o  |o          |o                      |o  |
|**Delayed Consequences**   |o  |o          |                       |o  |
|**Exploration**            |o  |           |                       |   |

<span class="text-color-bold">AI Planning</span>
: **Optimization, Generalization, Delayed consequences -**
바둑을 예로 들어보자. 
바둑은 규칙이 존재한다.
즉, 세계가 어떻게 동작하는지 주어지기 때문에 exploration이 포함되지 않는다.

<span class="text-color-bold">(Un)Supervised Machine Learning</span>
: **Generalization -**
(Un)Supervised ML에서는 model이 무엇을 해야 하는지(classification, regression, etc.) 분명하다. 
즉, optimization과 exploration이 포함되지 않는다. 
또한, feedback도 즉각적으로 받을 수 있어 delayed consequences가 포함되지 않는다.

<span class="text-color-bold">Imitation Learning</span>
: **Optimization, Generalization, Delayed consequences -**
Imitation learning(IL)는 다른 agent(인간, 등)로부터 경험(결정)을 학습하는 것으로 RL을 supervised learning 방식으로 해결하는 것이다.
참고하는 agent의 policy가 주어지기 때문에 exploration이 포함되지 않는다.


## ⅱ. Class Goals

1. AI 그리고 non-interactive ML과 RL을 구분짓는 key feature를 정의한다.
2. 문제가 주어졌을 때 RL 문제로 formulate할 수 있는지 확인한다.
3. (Deep) RL 알고리즘을 구현한다.
4. RL 알고리즘 분석하고 평가하기 위한 다양한 criteria를 나열하고 정의할 수 있어야 한다. (regret, sample/computational complexity, etc.)
5. Exloration과 exploitation을 설명하고 이 두가지 channelge를 해결하기 위한 최소 두 가지 방식을 비교/대조한다.

# Ⅱ. Intro to sequential decision making under uncertainty.

![](2.jpg)
_Figure 2: Sequential Decision Making_

Goal
: 미래의 reward 기댓값을 최대화 하는 actions 선택해야 한다.
이를 위해 immediate and long term rewards의 균형을 잘 맞춰야 할 것이다.
그리고 높은 rewards를 얻기 위해 전략적인 행동이 필요할 것이다.

예를 들어, agent(학생들)에게 덧셈과 뺄셈을 가르치고, 대부분의 학생들은 뺄셈보다 덧셈을 더 쉽게 느낀다고 하자.
당연히 학생들이 맞추면 reward +1을 얻고, 틀리면 -1을 얻는다.
이 scenario 상에선 agent는 매우 쉬운 덧셈 문제만 풀어 reward 기댓값을 최대화할 것이다.
이는 reward hacking이라는 문제로 알려져 있다.
Reward function은 조심해서 design해야 한다.

Discrete time $t$에 agent는 action $a_t$로 world에게 영향을 끼치고, world는 새로운 observation $o_t$ 와 reward $r_t$를 agent에게 준다.
Agent는 전달 받은 이들과 과거의 정보: history $h_t=(a_1, o_1, r_1, \cdots, a_t, o_t, r_t)$를 활용해 또 다른 결정 $a_{t+1}$을 내린다.

State space라는 또 다른 중요한 개념이 있다.
State는 agent가 결정을 내릴 때 참고하는 다른 유형의 정보로, 해당 course에서는 history의 function($s_t = f(h_t)$)으로 정의한다.
State에는 world state, agent state가 있다.
World state는 real world에 대한 state이고 agent state는 agent가 인식하는 state이다.
당연 agent는 agent state를 바탕으로 결정을 내린다.

예를 들어, 사람들(agent)은 눈을 통해 정면(agent state)을 관찰하고 결정을 내린다.
정면뿐만 아니라 측면, 후면을 포함한 것이 world state이다.

Observation은 state의 부분집합이다.
State는 현재의 observation와 같을 수 있고, 과거의 observation까지 포함한 것일 수 있다.
Domain 따라 state가 정의된다.

예를 들어, breakout 게임에서 하나의 pixel만으로는 공이 올라가는지 내려가는지 알 수 없는데, 이전 pixel의 관찰을 추가적으로 활용하면 현재 공의 state를 파악할 수 있다.


## ⅰ. Markov Assumption

Markow assumption
: Agent에 의해 사용된 state가 history의 sufficient statistic이기 때문에 현재 environment의 state만 가지고 미래 예측을 한다.

$$
p(s_{t+1}|s_t, a_t) = p(s_{t+1}|h_t, a_t)
$$

간단히 말해 미래 state는 현재 state가 주어졌을 때, 과거의 state와 independent하다는 것을 의미한다.

Example
: 현재 혈압의 상태(observation)을 state로 정의하고 혈압 측정을 할 때, 현재의 혈압 상태로 고혈압인지 알 수 있을까? Markov system인가? 아니다. 운동 후 측정, 비행기에서 측정 등 다양한 요소가 존재한다. 이땐 치료가 필요한지를 결정하기 위해, 이전 또는 이후의 state가 추가로 필요하다. 

일반적인 상황에서 Markov assumption은 만족되지 않을 것이다.
그런데 유명한 이유는 무엇일까? 
State $s_t$를 어떻게 정의하냐에 따라 Markov assumption은 항상 성립한다.

만약 $s_t = h_t$라면 Markov assumption은 항상 성립한다.
하지만, 불피요한 정보까지 포함될 수 있다.
그래서 실제로는 가장 최근 몇 개의 observation을 history의 sufficient statistic이라고 종종 가정한다.
이를 위해선 과거의 경험의 generalization, clustering, aggregation 등 을 잘 해야 한다.

Full observability: Markov Decision Process (MDP)
: Observation이 state와 같은 environment($s_t = o_t$)를 뜻한다. 
즉, agent가 모든 상황을 다 관찰할 수 있는 경우로 agent state와 world state가 같다.

Partial observability: Partially Observable Markov Decision Process (POMDP)
: Agent가 부분적인 world state만 관찰할 수 있는 environment를 뜻한다.
즉, agent가 관찰할 수 없는 부분이 존재하므로 agent state와 world state가 다르다.
따라서 history($s_t = h_t$), beliefs of world state, RNN 등 을 활용해야 한다.

> agent state와 observation이 같은 의미인 것 같다...?

## ⅱ. Types of Sequential Decision Processes

Bandits
: Sequential decision processes의 간단한 version으로 action이 다음 observation에 전혀 영향을 주지 않는다.
예를 들어, user들이 website에 방문을 했고 각 user에게 하나의 광고를 보여줬다고 하자. 
그들이 광고를 클릭을 하든 말든 또다른 user는 webiste를 방문할 것이다.

MDPs and POMDPs
: Action이 world state에 영향을 줘서 다음 observation와 reward도 영향을 받는다.

How the World Changes
: World가 변해가는 과정으로 2가지가 있다.   
- Deterministic은 특정 state에서 action을 취했을 때 그에 따른 결과가 오직 하나의 state인 것을 의미한다.
이를 위해선 model이 world에 대해 sufficient해야 하는데 이러한 model을 만들기 어렵다.
그래서 그들을 stochastic으로 근사한다.(approximate)
- Stochastic은 action을 취했을 때 여러 가능한 state가 제안되는 것을 의미한다.

# Ⅲ. RL Algorithm Components

![](3.jpg)
_Figure 3: Mars rover image - NASA/JPL-Caltech_

Mars Rover는 화성에서 로봇이 탐사를 진행하는 문제를 통해 RL의 구성요소인 **model**, **policy**, **value function**을 알아보자.
간단히 총 7개의 state가 있고, action은 좌우 이동만 있다고 하자.
States 중 $s_1$은 과학적으로 의미있는 정보가 발견된 상태, $s_7$은 과학적으로 중요한 정보(물 등)가 발견된 상태를 의미한다.
따라서 $s_1$의 reward는 +1, $s_7$의 reward는 +10, 나머지는 reward는 0이다.

Model
: Agent의 action으로 world가 어떻게 바뀌는지에 대한 agent의 representation이다.
Model은 2가지 유형을 가진다.

- Transition/dynamics model: 특정 state와 agent의 action을 입력받았을 때, 다음의 state 또는 reward에 대한 distribution을 의미한다.

$$
p(s_{t+1} = s^\prime|s_t=s, a_t=a)
$$

- Reward model: 특정 state와 agent의 action을 입력받았을 때, reward의 기댓값을 예측하는 것이다.

$$
r(s_t=s,a_t=a) = \mathbb{E}[r_t|s_t=s,a_t=a]
$$

Policy $\pi$
: Agent가 state를 입력받아 다음의 action을 출력하는 function이다.
즉, policy $\pi$는 agent가 action을 선택하는 방법을 결정한다.
MDP에서 policy는 state를 action으로 mapping한다. $\pi: S \rightarrow A$

- Deterministic policy: 하나의 action만 출력된다.

$$
\pi(s) = a
$$

- Stochastic policy: action의 distribution을 얻는다.

$$
\pi(a|s) = Pr(a_t=a|s_t=s)
$$

Value function $V^{\pi}$
: 특정한 policy $\pi$를 따랐을 때 특정 state 또는 action으로부터 기인한 미래 rewards의 기댓값(expected discounted sum)을 value라고 한다.
즉, 현재 reward와 미래 rewards를 모두 고려한다.

$$
V^{\pi}(s_t=s) = \mathbb{E}_{\pi}[r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots|s_t=s]
$$

$\gamma \in $ {0,1}는 discount factor로 즉각적인 rewards와 장기적인 rewards에 얼마나 관심이 있는지를 나타낸다.

## ⅰ. Types of RL Agents

![](4.jpg){: w="500"}
_Figure 4:: RL Agents from David Silver’s RL course_

RL에서 흔히 사용되는 agent 유형으로 model-based, model-free가 있다.

Model-based
: Transition/reward model처럼 world가 어떻게 작동하는지에 대한 직접적인 model을 representation으로 유지합니다.
그리고 policy나 value function을 가질 수도 있고 아닐 수도 있다.

> Model에 포함되어 있다면 policy나 value function 가지지 않을 것 같다.

Model-free(Actor-Critic)
: Policy function과 value function을 가지고 model을 포함하지 않는다.

# Ⅳ. Key Challenges in Learning to Make Sequences of Good Decisions

AI planning
: World가 동작하는 방법을 아는 model이 주어진다.
이 말은 state $s$에서 action $a$를 취했을 때 potential score(또는 next state의 확률 분포)를 계산할 수 있음을 의미한다.
그 다음, real world와 interaction 없이 높은 reward를 얻기 위한 optimal action을 학습한다.
마지막으로 real world에서 해당 optimal action을 시행한다.
예를 들어, 모든 규칙을 숙지한 game(perfect model)이 있다.

Reinforcement learning
: Agent가 world가 동작하는 방식을 모른다.
따라서 탐색(exploration)을 통해 world를 이해하고 좋은 결정을 내려도록 policy를 향상시킨다.
예를 들어, 규칙을 모르는 game이 있다. 
취한 action과 어떤 일이 일어났는지를 봄으로써 직접적으로 policy를 학습한다.

# Ⅴ. Exploration and Exploitation

Agent는 오직 자신이 시행한 actions만 경험할 수 있다.
이는 sub-optimal solutioin으로 이끌 수 있다.
이러한 문제를 완화하기 위해 agent가 action의 균형을 맞춰야 할 2가지 있다.

Exploration
: Agent가 미래에 더 나은 결정을 내릴 수 있도록 새로운 것을 시도한다.

Exploitation
: 과거 경험을 바탕으로 좋은 rewards을 얻을 것으로 예상되는 행동 선택한다.

# Ⅵ. Evaluation and Control

2개의 fundamental problems에 관해 살펴보자.

Evaluation
: Policy가 주어졌을 때 rewards의 기댓값을 추정/예측하는 것이다. 
새로운 policy를 시도하지 않는다.

Control
: Optimization에 관한 문제로 가장 좋은 policy를 찾는 것이다.
이는 evaluation의 몇몇 요소를 포함하고 있다.

# References
1. [YouTube, CS234 \| Winter 2019 \| Lecture 1 \| Emma Brunskill][1]{:target="_blank"}
2. [CS234: Reinforcement Learning Winter 2019][2]{:target="_blank"}

[1]: https://www.youtube.com/watch?v=FgzM3zpZ55o&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u&index=1
[2]: https://web.stanford.edu/class/cs234/CS234Win2019/index.html