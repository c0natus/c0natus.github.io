---
title: "[CS234] 6. CNNs and Deep Q Learning"
categories: [Course, CS234]
tags: [Reinforcement Learning, DQN]
img_path: /assets/img/posts/course/cs234/dqn/
author: gshan
math: true
---

임시노트 나중에 정리 예정.

5강에서는 linear VFA를 살펴보았다.
Linear VFA의 성능은 feature의 quality에 크게 의존한다.
좋은 feature를 얻기 위한 feature engineering에는 많은 시간이 필요하다.
그래서 본 강의에서는 큰 domain에서 의사 결정을 하고, feature를 자동으로 추출하기 위해 DNN을 사용한 function approximator를 소개한다.

# Deep Q-Network (DQN)

![](1.png)
_Figure 1: Illustration of the Deep Q-network: the input to the network consists of an 84 × 84 × 4 preprocessed image, followed by three convolutional layers and two fully connected layers with a single output for each valid action. Each hidden layer is followed by a rectifier nonlinearity (ReLU)_

Atari game environment에서 image를 전처리해서 얻은 state s를 DNN의 input으로 주면 DNN는 해당 state s에서 각 action을 취했을 때의 Q-values를 output으로 가진다.

DQN은 Atari game에서 4개의 frame을 전처리한 ($84\times 84\times 4$) image를 input으로 가진다.
- ($210 \times 160 \times 3$) size인 한 frame을 전처리를 통해 ($84\times 84\times 1$) size로 만든다.

## Training Algorithm for DQN

![](2.png)
_Algorithm 1: deep Q-learning_

Algo. 1은 DQN의 전체적인 algorithm이다.
Large nonlinear VFA을 사용하고 online Q-learning에서 안정적으로 학습되기 위해 2개(experience replay, target network)의 다른 점이 있다.

> Online learning considers single observations of data during training, whereas offline learning considers all the data at one time during training. 

Q-network는 아래의 MSE를 최소화하는 방향으로 학습된다.

$$
\begin{split}
J(w) &= \mathbb{E}_{(s_t, a_t, r_t, s_{t+1})}[(y_t^{\text{DQN}} - \hat{q}(s_t, a_t, w))^2]\\
& \text{where } y_t^{\text{DQN}} = r_t + \gamma\underset{a^\prime}{\text{ max }}\hat{q}(s_{t+1},a^\prime, w^-)
\end{split}
$$

- $w^-$는 target network의 parameters를 나타낸다.

Online network parameter $w$는 과거 transition tuples ($s_t, a_t, r_t, s_{t+1}$)의 minibatch에서 sampling된 gradient로 update된다.

Experience replay
: 각 time step $e_t$에서의 agent의 experiences는 reply buffer $D_t$에 저장된다.
Replay buffer에는 가장 최근의 k 개의 experiences가 저장된다.
Q-network는 minibatch data(replay buffer)에서 sampling(uniform)한 후, 그것의 SGD로 update된다.
Sampling된 data가 생성된 시점의 parameter가 현재 parameter와 다르므로 off-policy 방법을 사용해야 한다.
이는 online Q-learning에서 다음과 같은 장점을 가진다.
- Data efficiency, remove sample correlations, avoid oscillations or divergence

Replay buffer는 각 sample의 중요도를 구분하지 못하고, 고정된 buffer size로 과거의 transitions는 없어진다는 단점이 있다.
Transition의 중요도를 구분하면 더 효율적으로 agent의 학습이 진행된다는 연구([Prioritized Experience Replay][1]{:target="_blank"})가 있다.

Target network
: 학습의 stability를 향상시키고 non-stationary learning targets을 다루기 위해, target network가 target $y_j$를 생성할 때 사용된다.
C step마다 behavior network의 parameter $w$를 copy($w^- = w$)하고 그 외에는 고정된 채로 target $y$를 생성한다.

DQN은 각 game에서 같은 model architecture와 learning algorithm, hyper-parameter를 사용하면서 agent를 update한다.
Positive reward는 +1, negative reward는 -1로 설정해 모든 game에서 동일한 learning rate를 사용할 수 있도록 한다.
또한 frame-skipping technique (or action repeat)을 사용한다.

Action repeat
: Agent는 모든 frame에서 action을 선택하는 대신 4번째 frame마다 선택한다.
그리고 그것의 action을 skip된 frames에서도 반복된다.
이는 성능에 큰 영향을 주지 않고 의사 결정 frequency를 줄인다.
다른 말로, agent가 training 동안 4배 더 많은 game을 진행할 수 있다.

# Double Deep Q-Network (DDQN): Reducing Bias

4강에서 Q-learning에는 maximization bias가 있다고 했다.
DQN에서도 action 선택과 action-state value 평가가 같은 network values를 사용하면, overestimated values를 선택하도록 만들어 overoptimistic(지나치게 낙관적) target value 추정값을 얻을 가능성이 높아진다.
DQN에서도 이를 방지하기 위해 이들을 decouple한 Double learning을 한다.

DDQN에서는 greedy action은 online network parameter $w$로 생성되고, target network parameter $w^-$로 action-state value가 추정된다.

DQN과 다른 점은 딱 한줄로, Algo. 1에서 12 line이다.

$$
y_t^{\text{DDQN}} = r_t + \gamma\hat{q}(s_{t+1}, \underset{a^\prime}{\text{argmax }}\hat{q}(s_{t+1}, a^\prime, w), w^-)
$$

- DQN: $y_t^{\text{DQN}} = r_t + \gamma\underset{a^\prime}{\text{ max }}\hat{q}(s_{t+1},a^\prime, w^-)$

DQN과 마찬가지로 C step마다 $w^- = w$로 periodic copy를 한다.

# Dueling Network: Decoupling Value and Advantage

![](3.png)
_Figure 3: Single stream Deep Q-network (top) and the dueling Q-network (bottom). The dueling network has two streams to separately estimate (scalar) state-value V (s) and the advantages A(s, a) for each action; the green output module implements equation to combine the two streams. Both networks output Q-values for each action._

기존 DQN에서는 computer vision에서 사용되던 구조 그대로 사용했다. 본 논문은 기존의 구조 대신 강화학습에 더 특화된 구조를 제안한다.

Fig. 3처럼 Dueling Network에서는 state value와 advantage로 나누어 추정을 한 뒤, 이들을 합쳐 action-state value를 구한다.
- State-value function $V(s)$
: 현재 state의 value를 나타낸다. 앞으로 선택될 action과 상관없이 state 자체의 좋고 나쁨을 나타낸다.
- Advantage function $A(s,a)$
: 현재 state에서 해당 action이 다른 action에 비해 가지는 상대적인 중요도를 나타낸다.

$$
A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)
$$

- $V^\pi(s) = \mathbb{E}_{a \sim \pi(s)}[Q^\pi(s,a)]$
- 따라서 $\mathbb{E}_{a \sim \pi(s)}[A^\pi(s,a)] = 0$

이를 통해 2가지 강점을 가진다.

- State-value의 효과적인 학습
: 각 action에 대응하는 action-state value를 추정할 필요 없이 해당 state의 value를 알 수 있다. 
몇 state에서는 action 선택이 reward에 중요한 영향을 끼치지만, 대부분의 state에서는 거의 영향이 없다.
반면에 state value 추정은 Q-learning 같은 bootstrapping 기반 algorithm의 모든 state에서 매우 중요하다.
기존 구조에서는 하나의 action에 대한 Q-value를 얻으면 해당 $Q(s,a)$의 값만 update되지만, dueling 구조에서는 어느 action에 대해 $Q(s,a)$가 update되면 $V(s)$가 update되기 때문에 다른 action에 대한 Q-value도 간접적으로 학습할 수 있다.
이는 특히 action이 환경에 영향을 주지 않는 state들을 학습하는데 유용하다.
> MC는 마지막 reward로 policy를 update하므로 action의 영향이 bootstrapping보단 클 것 같다. Bootstrapping은 단순히 앞,뒤 state와 action-state value를 보니까 action의 영향이 비교적 덜 할 것 같다.

![](4.png){: w="500"}
_Figure 4: See, attend and drive: Value and advantage saliency maps (red-tinted overlay) on the Atari game Enduro, for a trained dueling architecture. The value stream learns to pay attention to the road. The advantage stream learns to pay attention only when there are cars immediately in front, so as to avoid collisions._

Fig. 4의 위쪽 부분에서는 어떤 action을 취해도 reward가 같기 때문에 advantage 부분이 비활성화되어 현재의 state-value에만 집중한다. 
하지만, 하단은 장애물이 있어 action에 다라 얻게 되는 reward가 달라져 advantage 부분이 추가적으로 활성화된다.

- Noise robustness
: 특정한 state에서 advantage 차이는 매우 작다.
즉, Q-value의 절대적 크기에 비해 Q-value간의 차이가 작다.
그렇기 때문에 약간의 noise는 action에 따른 Q-value의 순서를 쉽게 바꾸기 때문에 학습 성능을 저하시킨다.
따라서 dueling architecture처럼 Q-value의 절대적 크기에 해당하는 state-value와 상대적 차이인 advantage를 분히해 학습하면 noise에 보다 robust할 수 있다.

## Q-value Estimation

Fig. 3에서 state-value와 advantage를 합쳐서 Q-value를 추정하는 aggregation module은 주의깊게 만들어야 한다.

위에서 살펴봤듯이 $Q^\pi(s,a) = A^\pi(s,a) + V^\pi(s,a)$이고, $\mathbb{E}_{a \sim \pi(s)}[A^\pi(s,a)] = 0$이다.
그리고 deterministic policy에 대해 아래의 식을 만족한다.

$$Q(s,a^*) = V(s) \therefore A(s,a^*) = 0 \text{ for } a^* = \underset{a^\prime \in A}{\text{argmax }}Q(s,a^\prime)$$

> Deterministic이면 하나의 action 이외에 다른 action의 확률값은 0이다. 
> 따라서 해당 action의 Q-value가 state value가 되고 advantage 값은 0이된다. 
> 다른 action에 대해서 advantage 값은 $-V(s)$가 된다.

이 사실을 기억한 채 deuling network를 살펴보자.
Q-value를 추정하는 아주 간단한 방법은 state value와 advantage를 더하는 것 $Q(s,a) = V(s) + A(s,a)$ 이다.
하지만 해당 방법은 Q-function이 unidentifiable하다는 문제를 가진다.
즉, Q-value가 주어졌을 때, unique한 state value 값을 구할 수 없다.

이를 방지하기 위해 아래와 같이 식을 약간 변경한다.

$$
Q(s,a) = V(s) + (A(s,a) - \underset{a^\prime \in A}{\text{max }}A(s,a^\prime))
$$

Deterministic policy를 생각해보자.

$$
\begin{split}
&a^* = \underset{a^\prime \in A}{\text{argmax }}Q(s,a^\prime) = \underset{a^\prime \in A}{\text{argmax }}A(s,a^\prime)\\
&\therefore Q(s,a^*) = V(s)
\end{split}
$$

이를 통해 action $a^*$에 대해선 state value 값을 추정하게 되고, 다른 action에 대해선 advantage를 추정하게 만들어 Q-function을 identifiable하게 만든다.

> Stochastic policy에서는 다른 action에 대해서도 $Q(s,a)$가 값을 가지므로, $Q(s,a^*) \neq V(s)$이다.

하지만 이 방법은 실제로 학습 시 pratical하지 않기 때문에 또 다른 방법을 제안한다.

$$
Q(s,a) = V(s) + (A(s,a) - \frac{1}{|A|}\sum_{a^\prime}A(s,a^\prime))
$$

이 방법은 본래 $V(s), A(s,a)$의 의미와 약간 동떨어진 것으로 정확하게 해당 값들을 학습하도록 하지 않는다.
하지만, 최대값 대신 평균을 이용하면 advantage의 변화량이 적어 학습이 안정하게 진행된다고 주장한다.

Dueling netowrk는 state value function을 효율적으로 approximation하는 능력이 있다.
그리고 action의 수가 많을 때 더욱 효율적이다. 

# References
1. [YouTube, CS234 \| Winter 2019 \| Lecture 6 \| Emma Brunskill][2]{:target="_blank"}
2. [CS234: Reinforcement Learning Winter 2019][3]{:target="_blank"}
3. [T-story, Dueling DQN, 이것저것 테크블로그][4]{:target="_blank"}

[1]: https://arxiv.org/pdf/1511.05952.pdf
[2]: https://www.youtube.com/watch?v=gOV8-bC1_KU&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u&index=6
[3]: https://web.stanford.edu/class/cs234/CS234Win2019/index.html
[4]: https://ai-com.tistory.com/entry/RL-%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-3-Dueling-DQN